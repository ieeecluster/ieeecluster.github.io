<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
    <base href="/2025/">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>IEEE Cluster 2025 Program</title>
    <link  rel="stylesheet" type="text/css" href="program/includes/css/jquery-ui.css"/>
    <link href="program/includes/css/shared_styles.css" rel="stylesheet" type="text/css"/>
    <link href="program/includes/css/block_styles.css?v=1" rel="stylesheet" type="text/css"/>
    <link href="program/includes/css/jquery.qtip.min.css" rel="stylesheet" type="text/css"/>
    <link href="program/includes/css/font-awesome-4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"/>
    <link href="program/includes/css/user_generated.css" rel="stylesheet" type="text/css"/>
    <link href="program/archive_at_a_glance_styles.css" rel="stylesheet" type="text/css"/>
    <style>

        .tabs {
            background: #e8ca9b;
        }

        .tabs .divider,
        .tabs .bg_tab {
            background-color: #e8ca9b;
            border-top-color: #e8ca9b;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }

        .tabs .fg_tab {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            color: #CC1A1A;
            text-transform: none;
        }

        .tab_menu_label, .tab_no_menu_label {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .qtip.qtip-rm-tab-menu {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }


        .filter_bar,
        .filter_bar_w_legend {
            background-color: #C9DDF9;
        }

        .filter_bar_w_legend .instr,
        .filter_bar .instr {
            background-color: #A2C2FC;
        }


        .role_stype_bar {
            background-color: #E0AB76;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }


        #footer {
            background-color: #EAEAEA;
            color: #999999;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }

        #footer a {
            color: #777777;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }


        div.banner_top,
        div.banner_top .site_title,
        div.banner_top .no_logo_banner_right,
        div.logo_banner,
        div.logo_banner .user_name,
        #header {
            background-color: #0066CC;
            color: #FFFFFF;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }


        a:link,
        a:visited,
        a:active,
        .clickable,
        a.clickable,
        a.clickable:link,
        a.clickable:visited,
        a.clickable:active,
        .ttip_object_info_blue,
        .ttip_object_info_blue_no_clone,
        .ttip_object_info_blue_wide,
        .ttip_object_info_blue_wide_no_clone,
        .ttip_object_info_blue_very_wide,
        .ttip_object_info_blue_very_wide_no_clone,
        .ttip_object_info_blue_extra_wide,
        .ttip_object_info_blue_extra_wide_no_clone,
        .ttip_object_info_blue_modal,
        .ttip_object_info_blue_modal_no_clone,
        .colorbox_object_info,
        span.menu_item_label,
        .page_box_print .contents A,
        .page_box_print #footer a {
            color: #0000EE;
        }

        /* Light Links */
        .light_link a,
        .light_arrow,
        .light_link a:link,
        .light_link a:active,
        .light_link a:visited,
        .light_clickable,
        a.light_clickable,
        a.light_clickable:link,
        a.light_clickable:active,
        a.light_clickable:visited {
            color: #5088F0;
        }

        /* user hovers     */
        a:hover,
        .light_link a:hover,
        .light_arrow:hover,
        .light_clickable:hover,
        a.light_clickable:hover,
        .hover_link:hover,
        .ttip_object_info_blue:hover,
        .ttip_object_info_blue_no_clone:hover,
        .ttip_object_info_blue_wide:hover,
        .ttip_object_info_blue_wide_no_clone:hover,
        .ttip_object_info_blue_very_wide:hover,
        .ttip_object_info_blue_very_wide_no_clone:hover,
        .ttip_object_info_blue_extra_wide:hover,
        .ttip_object_info_blue_extra_wide_no_clone:hover,
        .ttip_object_info_blue_modal:hover,
        .ttip_object_info_blue_modal_no_clone:hover,
        .ttip_object_info:hover,
        .ttip_object_info_no_clone:hover,
        .ttip_object_info_wide:hover,
        .ttip_object_info_wide_no_clone:hover,
        .ttip_object_info_very_wide:hover,
        .ttip_object_info_very_wide_no_clone:hover,
        .ttip_object_info_extra_wide:hover,
        .ttip_object_info_extra_wide_no_clone:hover,
        .ttip_object_info_modal:hover,
        .ttip_object_info_modal_no_clone:hover,
        .colorbox_object_info:hover,
        .subtabs .fg_tab:hover div,
        .subtabs .fg_tab:hover A,
        .subtabs .bg_tab:hover,
        .subtabs .bg_tab:hover A {
            color: #0000EE;
        }

        ul.rm_mega_menu li.mega > div,
        ul.rm_mega_menu > li.mega-link > a:hover,
        .disp_details_header,
        .disp_details_sub_header,
        .disp_details I, /* This is deprecated, since it clashes with font awesome using I tags. */
        .disp_red,
        .disp_label {
            color: #B32626;
        }


        #related_col .block-title {
            background-color: #BBBBBB;
            color: #000000;
        }

        #related_col .block-title a {
            color: #0000FF;
        }

        #related_col .block-content {
            background-color: #E5E5E5;
        }

        #related_col .block-content .instr {
            background-color: #D0D0D0;
        }

        #related_col .block-content .odd {
            background-color: #DEDEDE;
        }

        #related_col .block-content .even {
            background: #D0D0D0;
        }


        .contents .output_box .title {
            background-color: #D89655;
        }

        .block-content,
        .block2-content,
        .contents .output_box table tr th,
        .contents .output_box {
            background-color: #F9F4E6;
        }

        .output_box_instr,
        .contents .output_box .instr,
        .block .instr,
        .block-content .instr {
            background-color: #F2E0C5;
        }

        .odd,
        .contents .output_box .odd,
        .block-content .odd {
            background-color: #F6EAD5;
        }

        .even,
        .contents .output_box .even,
        .block-content .even {
            background-color: #F2E0C5;
        }


        .tabs .fg_tab {
            background-color: #F0DBBC;
            color: #CC1A1A;
            border-bottom-color: #F0DBBC;
        }

        .tab_menu_label:hover, .active .tab_menu_label,
        .tab_no_menu_label:hover,
        .tab_no_menu_label:hover a {
            color: #CC1A1A;
        }

        .subtabs {
            background-color: #F0DBBC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }

        .subtabs .divider,
        .subtabs .bg_tab,
        .subtabs .bg_tab a {
            background-color: #F0DBBC;
            border-top-color: #F0DBBC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }

        .subtabs .fg_tab {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            color: #CC1A1A;
            text-transform: none;
        }

        /*
        uncomment this to make the subtabs follow the selected tab color instead
        of the link color

        .subtabs .bg_tab:hover a {
            color: #CC1A1A;
        }
        .subtabs .fg_tab:hover a, {
            color: #CC1A1A;
        }
        */

        .subtabs .fg_tab a {
            color: #CC1A1A;
        }


        .documentation_box {
            background: #F0E0BC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }


        body.in_iframe {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .pagedoc {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .page_box {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .page_box_in_iframe {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .contents {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .contents_options {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        #top-links {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .fullscreen {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .subtabs .fg_tab {
            border-bottom-color: #FFFFF7;
        }

        .subtabs .fg_tab div {
            background-color: #FFFFF7;
            border-bottom-color: #FFFFF7;
        }

        .fullscreen_schedule {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .contents input,
        .contents input_box,
        .contents textarea {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .qtip.rm-qtip {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        #cboxContent {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        /* For now, use the main site background color for tool tips. */
        .qtip.qtip-rm,
        .qtip.qtip-rm .qtip-titlebar {
            background-color: #FFFFF7;
        }

        /* Not sure where this should live. */
        #actions_col .block-title-text {
            font-size: 15px;
        }

        #related_col .block-title-text {
            font-size: 15px;
        }

        /* For jquery-ui. */
        .ui-widget {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .arrow-slidedown {
            background-color: #FFFFF7;
            color: #000000;
        }


        .contents .input .title,
        .contents .input_box .title {
            background-color: #247bf4;
        }

        .contents .input,
        .contents .input_box,
        .contents .input table tr th,
        .contents .input_box table tr th,
        .form .block-content {
            background-color: #f6f9fe;
        }

        .contents .input .instr,
        .contents .input_box .instr,
        .multi_block_button,
        .form .block .instr {
            background-color: #dbe8fa;
        }

        .contents .input .odd,
        .contents .input_box .odd,
        .form .block-content .odd {
            background-color: #edf3fc;
        }

        .contents .input .even,
        .contents .input_box .even,
        .form .block-content .even {
            background-color: #dbe8fa
        }


        div.rm_mega_menus_container,
        ul.rm_mega_menu.darker,
        ul.rm_mega_menu > li.mega > a,
        ul.rm_mega_menu.darker > li.mega > a,
        ul.rm_mega_menu > li.mega-link > a,
        ul.rm_mega_menu.darker > li.mega-link > a,
        ul.rm_mega_menu > li.mega-label > span {
            background-color: #777777;
            border-color: #777777;
            color: #FFFFFF;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }

        ul.rm_mega_menu > li.mega.selected > a {
            background-color: #777777;
            border-color: #CC1A1A;
            color: #FFFFFF;
        }

        ul.rm_mega_menu > li.mega:hover > a {
            color: #CC1A1A;
        }

        div.rm_mega_menus_container ul.rm_mega_menu > li.mega > div.menu_dropdown {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .contents input,
        .contents input_box,
        .contents textarea {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }


        #actions_col .block-title {
            background-color: #244A84;
            color: #FFFFFF;
        }

        #actions_col .block-title a {
            color: #FFFFFF;
        }

        #actions_col .block-content {
            background-color: #BDD2F8;
        }

        #actions_col .block-content .instr {
            background-color: #B0CBFC;
        }

        #actions_col .block-content .odd {
            background-color: #C2D6FB;
        }

        #actions_col .block-content .even {
            background: #B0CBFC;
        }
    </style>
    <script src="program/includes/jquery/jquery-1.8.3.min.js" type="text/javascript"></script>
    <script src="program/includes/jquery/jquery-ui.min.js" type="text/javascript"></script>
    <script src="program/includes/jquery/jquery.hoverIntent.minified.js" type="text/javascript"></script>
    <script src="program/includes/jquery/basic.js" type="text/javascript"></script>
    <script src="program/includes/jquery/jquery.qtip.min.js" type="text/javascript"></script>
    <script src="program/includes/jquery/jquery.highlight-4.js"></script>
    <script src="program/includes/jquery/jquery.instaFilter.js"></script>
    <script type="text/javascript">$(function () {
        setup_info_links();
    });</script>
    <script src="program/includes/jquery/user_generated.js"></script>
    <script type="text/javascript">
        $(document).ready(function () {
            $program_table = $("#program-table");
            $("#program_filter").InstaFilter($program_table, {
                typing_pause: 500,
                search_unit_selector: "div.slot-wrapper",
                search_unit_ancestor_fields_selector: ".session-title, .session-chair, .room-name, .session-type"
            });
            $('.author-names').prepend('<b>Presenter(s):</b>'); /* TODO -- remove */
        });
    </script>
</head>
<body>
<a name="top"></a>
<div class="centered">
    <div>
        <div style="float: left;"></div>
        <div style="float: left; margin-top: 15px; height: 80px;"><span
                class="page-title">IEEE Cluster 2025 Program</span></div>
        <div style="clear: both;"></div>
    </div>
</div>
<div>
  <table class="cellpadding5px" id="dummy-program-table" width="100%">
    </table>
</div>
<table class="cellpadding5px" id="program-table" width="100%">
    <tr>
        <td style="min-width: 225px; max-width: 225px;" valign="top">
            <h2 class="section-title">Tuesday September 2nd</h2>

            <div class="timeslot"><h3 class="session-time">8:30-9:30</h3>
                <div class="program-session">
                    <div class="session-title">Registration</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">9:30-11:00</h3>
                <div class="program-session">
                    <div class="session-title">[Tutorial] Smart Networking </div>
                    <div class="room-name">Pentland East</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
  		    <div class="session-title">[Workshop] LLMxHPC: 2025 International Workshop on Large Language Models (LLMs) and HPC</div>
                    <div class="room-name">Pentland West</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
                    <div class="session-title">[Tutorial] NVidia GH200/HPE Ex</div>
                    <div class="room-name">Prestonfield</div>
                </div>
                <div style="clear: both;"></div>
		<hr>
                <div class="program-session">
                    <div class="session-title">[Workshop] REX-IO 2025: 5th Workshop on Re-envisioning Extreme-Scale I/O for Emerging Hybrid HPC Workloads</div>
                    <div class="room-name">Salisbury</div>
                </div>
                <div style="clear: both;"></div>
		<hr>
                <div class="program-session">
                    <div class="session-title">[Tutorial] Inefficiencies at Scale</div>
                    <div class="room-name">Holyrood</div>
                </div>
                <div style="clear: both;"></div>
		<hr>
                <div class="program-session">
                    <div class="session-title">[Tutorial] C++/Sycl</div>
                    <div class="room-name">Duddingston</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">11:00-11:30</h3>
                <div class="program-session">
                    <div class="session-title">Break</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">11:30-13:00</h3>
                <div class="program-session">
                    <div class="session-title">[Tutorial] Smart Networking </div>
                    <div class="room-name">Pentland East</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
                    <div class="session-title">[Workshop] LLMxHPC: 2025 International Workshop on Large Language Models (LLMs) and HPC</div>
                    <div class="room-name">Pentland West</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
                    <div class="session-title">[Tutorial] NVidia GH200/HPE Ex</div>
                    <div class="room-name">Prestonfield</div>
                </div>
                <div style="clear: both;"></div>
                <div class="program-session">
                    <div class="session-title">[Workshop] REX-IO 2025: 5th Workshop on Re-envisioning Extreme-Scale I/O for Emerging Hybrid HPC Workloads</div>
                    <div class="room-name">Salisbury</div>
                </div>
                <div style="clear: both;"></div>
		<hr>
                <div class="program-session">
                    <div class="session-title">[Tutorial] Inefficiencies at Scale</div>
                    <div class="room-name">Holyrood</div>
                </div>
                <div style="clear: both;"></div>
		<hr>
                <div class="program-session">
                    <div class="session-title">[Tutorial] C++/Sycl</div>
                    <div class="room-name">Duddingston</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">13:00-14:00</h3>
                <div class="program-session">
                    <div class="session-title">Lunch Break</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">14:00-15:30</h3>
              <div class="program-session">
  		  <div class="session-title">[Tutorial] Smart Networking </div>
                  <div class="room-name">Pentland East</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
		    <div class="session-title">[Workshop] LLMxHPC: 2025 International Workshop on Large Language Models (LLMs) and HPC</div>
                    <div class="room-name">Pentland West</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
		  <div class="session-title">[Tutorial] NVidia GH200/HPE Ex</div>
                  <div class="room-name">Prestonfield</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
		    <div class="session-title">[Workshop] REX-IO 2025: 5th Workshop on Re-envisioning Extreme-Scale I/O for Emerging Hybrid HPC Workloads</div>
                    <div class="room-name">Salisbury</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
                  <div class="session-title">[Tutorial] Tenstorrent Tensix</div>
                    <div class="room-name">Holyrood</div>
                </div>
		<div style="clear: both;"></div>
		<hr>
                <div class="program-session">
                    <div class="session-title">[Tutorial] C++/Sycl</div>
                    <div class="room-name">Duddingston</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">15:30-16:00</h3>
                <div class="program-session">
                    <div class="session-title">Break</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">16:00-17:30</h3>
              <div class="program-session">
  		  <div class="session-title">[Tutorial] Smart Networking </div>
                  <div class="room-name">Pentland East</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
		    <div class="session-title">[Workshop] LLMxHPC: 2025 International Workshop on Large Language Models (LLMs) and HPC</div>
                    <div class="room-name">Pentland West</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
		  <div class="session-title">[Tutorial] NVidia GH200/HPE Ex</div>
                  <div class="room-name">Prestonfield</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
		    <div class="session-title">[Workshop] REX-IO 2025: 5th Workshop on Re-envisioning Extreme-Scale I/O for Emerging Hybrid HPC Workloads</div>
                    <div class="room-name">Salisbury</div>
                </div>
                <div style="clear: both;"></div>
                <hr>
                <div class="program-session">
                  <div class="session-title">[Tutorial] Tenstorrent Tensix</div>
                    <div class="room-name">Holyrood</div>
                </div>
		<div style="clear: both;"></div>
		<hr>
                <div class="program-session">
                    <div class="session-title">[Tutorial] C++/Sycl</div>
                    <div class="room-name">Duddingston</div>
                </div>
                <div style="clear: both;"></div>
            </div>
        </td>
        
        
        <td style="min-width: 225px; max-width: 225px;" valign="top">
            <h2 class="section-title">Wednesday September 3rd</h2>

            <div class="timeslot"><h3 class="session-time">8:30-9:30</h3>
                <div class="program-session">
                    <div class="session-title">Registration</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">9:30-11:00</h3>
                <div class="program-session">
                    <div class="session-title">Opening Session</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair: Taisuke Boku (U. Tsukuba)</div>
                    <div class="slot-title">
                         <div class="slot-wrapper">
                        <span>Welcome address (Michèle Weiland, General Co-Chair, EPCC)
                        </span>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                        <span>TPC report (Toni Pena, TPC Co-Chair, BSC)
                        </span>
                        </div>
                    </div>
                </div>
                <div style="clear: both;"></div>
                <div class="program-session">
                    <div class="session-title">Keynote (1): Natalia Vassilieva</div>
                    <div style="clear: both;"></div>
                    <div class="room-name">Pentland</div>
                    <div style="clear: both;"></div>
                    <div class="session-chair">Chair: </div>
                    <div style="clear: both;"></div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <a href="keynotes/index.html#keynote1" target="_parent"><div style="font-size:15px"><b>Wafer-Scale Computing: AI and HPC with Fewer, Stronger Machines</b></div></a>
                        </div>
                    </div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">11:00-11:30</h3>
                <div class="program-session">
                    <div class="session-title">Break</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">11:30-13:00</h3>
                <div class="program-session">
                    <div class="session-title">Best Paper Finalists</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair: Adrian Jackson</div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#bpfta1_info" title="#bpfta1_title">
			    Scaling Deep Learning Molecular Dynamics to 500M Atoms on 4096-Node ARMv8 Clusters
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="bpfta1_title">
				    Scaling Deep Learning Molecular Dynamics to 500M Atoms on 4096-Node ARMv8 Clusters
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="bpfta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Molecular dynamics (MD) simulations are essential tools for investigating large-scale molecular systems, yet achieving high performance and scalability on CPU-based architectures remains challenging. In this study, we present a highly optimized framework based on DeepMD-kit for conducting 500 million-atom MD simulations on an ARMv8 SVE high-performance computing (HPC) system. Key optimizations include leveraging OpenMP for multi-threaded acceleration of DeepMD-kit and utilizing the ARMv8 SVE instruction set to optimize double-precision matrix multiplication in PyTorch. These enhancements enable single ARMv8 SVE 64-core processors to achieve 1.3x the training performance of NVIDIA V100 GPU, and two ARMv8 SVE 64-core processors to achieve 1.05x the inference performance of NVIDIA V100 GPU. Leveraging this optimized framework, we achieve large-scale MD simulations across 4,096 computing nodes.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#bpfta2_info" title="#bpfta2_title">
			    PRT: An Efficient Pipeline Reuse Technology for Large Models Training
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="bpfta2_title">
				    PRT: An Efficient Pipeline Reuse Technology for Large Models Training
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="bpfta2_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
In this paper, we present ROCK, a novel system for efficiently serving thousands of LoRA adapters for multimodal models in cloud environments. Through extensive analysis of production workloads, we identify key challenges in current cloud-based image generation services: extreme request burstiness (up to 90× normal rates), heterogeneous task characteristics, and inefficient adapter management that wastes 40% of GPU memory and increases delays by 3x during peak times. ROCK addresses these challenges through a three-layer architecture that decouples hardware, adapters, and requests. Our system features dynamic heterogeneous queues that match tasks to appropriate resources based on multidimensional feature vectors, and a multilevel orchestration framework that intelligently manages adapter placement across heterogeneous storage. Experiments on a 64-GPU testbed demonstrate that ROCK reduces average response latency by 16-26%, and achieves an 84.1% cache hit rate for LoRA adapters—outperforming traditional approaches while reducing adapter update frequency by up to 77%.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#bpfta3_info" title="#bpfta3_title">
			    Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="bpfta3_title">
				    Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="bpfta3_info">
                                            <div class="author-names">
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
        Converged HPC-Cloud computing is an emerging computing paradigm that aims to support increasingly complex and multi-tenant scientific workflows. These systems require reconciliation of the isolation requirements of native cloud workloads and the performance demands of HPC applications. In this context, networking hardware is a critical boundary component: it is the conduit for high-throughput, low-latency communication and enables isolation across tenants. HPE Slingshot is a high-speed network interconnect that provides up to 200~Gbps of throughput per port and targets high-performance computing~(HPC) systems. The Slingshot host software, including hardware drivers and network middleware libraries, is designed to meet HPC deployments, which predominantly use single-tenant access modes. Hence, the Slingshot stack is not suited for secure use in multi-tenant deployments, such as converged HPC-Cloud deployments. In this paper, we design and implement an extension to the Slingshot stack targeting converged deployments on the basis of Kubernetes. Our integration provides secure, container-granular, and multi-tenant access to Slingshot RDMA networking capabilities at minimal overhead.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">13:00-14:00</h3>
                <div class="program-session">
                    <div class="session-title">Lunch</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">14:00-15:30</h3>
                <div class="program-session">
                    <div class="session-title">Session (1) AI Models and Approaches</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair:</div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s1ta1_info" title="#s1ta1_title">
			    ROCK: Serving Multimodal Model in Cloud with Heterogeneous-Aware Resource Orchestration for Thousands of LoRA Adapters
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s1ta1_title">
				    ROCK: Serving Multimodal Model in Cloud with Heterogeneous-Aware Resource Orchestration for Thousands of LoRA Adapters
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s1ta1_info">
                                            <div class="author-names">
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                      <p>
							In this paper, we present ROCK, a novel system for efficiently serving thousands of LoRA adapters for multimodal models in cloud environments. Through extensive analysis of production workloads, we identify key challenges in current cloud-based image generation services: extreme request burstiness (up to 90× normal rates), heterogeneous task characteristics, and inefficient adapter management that wastes 40% of GPU memory and increases delays by 3x during peak times. ROCK addresses these challenges through a three-layer architecture that decouples hardware, adapters, and requests. Our system features dynamic heterogeneous queues that match tasks to appropriate resources based on multidimensional feature vectors, and a multilevel orchestration framework that intelligently manages adapter placement across heterogeneous storage. Experiments on a 64-GPU testbed demonstrate that ROCK reduces average response latency by 16-26%, and achieves an 84.1% cache hit rate for LoRA adapters—outperforming traditional approaches while reducing adapter update frequency by up to 77%.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s1ta2_info" title="#s1ta2_title">
			    SplitQuant: Resource-Efficient LLM Offline Serving on Heterogeneous GPUs via Phase-Aware Model Partition and Adaptive Quantization
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s1ta2_title">
				    SplitQuant: Resource-Efficient LLM Offline Serving on Heterogeneous GPUs via Phase-Aware Model Partition and Adaptive Quantization
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s1ta2_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Modern large language models (LLMs) serving systems address distributed deployment challenges through two key techniques: distributed model partitioning for parallel computation across accelerators and quantization for reducing parameter size. While existing systems assume homogeneous GPU environments, we reveal significant untapped potential in heterogeneous systems with mixed-capacity accelerators where two critical limitations persist: (1) uniform partitioning and quantization strategies fail to adapt to hardware heterogeneity, exacerbating resource imbalance, and (2) decoupled optimization of partitioning and quantization overlooks critical performance synergies between these techniques. We present SplitQuant, a phase-aware distributed serving system that co-optimizes mixed-precision quantization, phase-aware model partitioning, and micro-batch sizing for heterogeneous environments. Our approach combines analytical modeling of quality-runtime tradeoffs with a lightweight planning algorithm to maximize throughput while preserving user-specified model quality targets. Evaluations across 10 production clusters show SplitQuant achieves up to 2.34x (1.61x mean) higher throughput than state-of-the-art approaches without violating accuracy targets. Our results underscore the value of hardware-conscious co-design between quantization and model partition strategies in heterogeneous environments.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s1ta3_info" title="#s1ta3_title">
			    DaCe AD: Unifying High-Performance Automatic Differentiation for Machine Learning and Scientific Computing	  
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s1ta3_title">
				    DaCe AD: Unifying High-Performance Automatic Differentiation for Machine Learning and Scientific Computing
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s1ta3_info">
                                            <div class="author-names">
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Automatic differentiation (AD) is a set of techniques that systematically applies the chain rule to compute the gradients of functions without requiring human intervention. Although the fundamentals of this technology were established decades ago, it is experiencing a renaissance as it plays a key role in efficiently computing gradients for backpropagation in machine learning algorithms. AD is also crucial for many applications in scientific computing domains, particularly emerging techniques that integrate machine learning models within scientific simulations and schemes. Existing AD frameworks have four main limitations: limited support of programming languages, requiring code modifications for AD compatibility, limited performance on scientific computing codes, and a naive store-all solution for forward-pass data required for gradient calculations. These limitations force domain scientists to manually compute the gradients for large problems. This work presents DaCe AD, a general, efficient automatic differentiation engine that requires no code modifications. DaCe AD uses a novel ILP-based automatic checkpointing algorithm to optimize the trade-off between storing and recomputing to achieve maximum performance within a given memory constraint. We showcase the generality of our method by applying it to NPBench, a suite of HPC benchmarks with diverse scientific computing patterns, where we outperform JAX, a Python framework with state-of-the-art general AD capabilities, by more than $92$ times on average without requiring any code changes.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="hr">
                    <hr/>
                </div>
                <div class="program-session">
                    <div class="session-title">Session (2) Job Scheduling and Orchestration</div>
                    <div class="room-name">Prestonfield</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s2ta1_info" title="#s2ta1_title">
			      GreenK8s: Green-aware Scheduling for Sustainable Kubernetes Cluster Management
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s2ta1_title">
				      GreenK8s: Green-aware Scheduling for Sustainable Kubernetes Cluster Management
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s2ta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
With the rise of large-scale data centers and increasing demand for energy-efficient operations, there is a growing need to optimize the use of green energy in cloud computing environments. However, current schedulers focus solely on performance, lacking awareness of energy types and opportunities to promote green, low-carbon operations. This paper presents a Green-Aware Scheduling Framework for Kubernetes, named GreenK8s, aimed at minimizing the use of brown energy and maximizing the utilization of renewable energy sources, specifically solar power. Our framework integrates real-time power consumption monitoring with predictive solar energy models to intelligently schedule workloads based on energy availability. The proposed solution incorporates an AI-based solar power prediction model, Pod oversubscription strategies, and a novel scheduler, enabling Kubernetes to dynamically adapt to both the type and availability of green energy. Extensive experiments using the real-world Google Borg dataset and a realistic Kubernetes testbed demonstrate that GreenK8s reduces total energy consumption by up to 39% and increases the average share of green energy in total consumption to 50.65%, compared to state-of-the-arts baselines. This work provides a promising approach to improve operational efficiency and sustainability in data centers.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s2ta2_info" title="#s2ta2_title">
			    DDRM: An SLO-aware Deep Dynamic Resource Management Framework for Microservices
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s2ta2_title">
				    DDRM: An SLO-aware Deep Dynamic Resource Management Framework for Microservices
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s2ta2_info">
                                          <div class="author-names">
					    
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Loosely coupled microservice architectures have been widely adopted in cloud-native applications due to their inherent advantages in modularity, development agility, and scalability. However, the resulting complex and dynamic service topologies introduce intricate inter-service dependencies, which often lead to backpressure effects and queuing delays. These phenomena significantly challenge traditional monolithic and rule-based resource management approaches, which struggle to capture the non-linear performance characteristics and long-term effects of resource allocation decisions in such environments.
							</p>
							<p>
To address these challenges, we propose DDRM, a two-stage predictor-decider collaborative framework for dynamic resource management in microservice systems. DDRM integrates deep learning to model inter-service interactions and predict the probability of Service Level Objective (SLO) violations, and employs reinforcement learning to optimize resource allocation decisions by maximizing long-term cumulative rewards while meeting SLO targets. Extensive evaluations demonstrate that DDRM outperforms state-of-the-art baselines by up to 29.8%, while exhibiting strong stability and adaptability under highly varying workloads.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s2ta3_info" title="#s2ta3_title">
			    Are We There Yet? Predicting the Queue Wait Times for HPC Jobs
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s2ta3_title">
				    Are We There Yet? Predicting the Queue Wait Times for HPC Jobs
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s2ta3_info">
                                          <div class="author-names">
					    
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Large high-performance computing systems are commonly shared among users that submit their workflows to a resource manager and scheduling framework such as SLURM. Most commonly available job schedulers provide built-in algorithms for performing job backfill and placement, where candidate jobs can be run out of order on currently free resources, provided that they do not negatively impact other jobs already waiting in the queue.
							</p>
							<p>
Backfilling relies on two key requirements: 1) the user’s own estimate of the runtime of their job and 2) the ability for the scheduler to create and maintain a future schedule of possibly all jobs in the queue at any one moment. Unfortunately, user-provided estimates are often erroneous, a well-known problem in parallel job scheduling. These estimates cause the scheduler to plan jobs out based on wildly inaccurate data, which in turn causes the scheduler-provided estimates of estimated user waiting time to also be quite inaccurate. As such, in this work, we leverage several machine learning (ML) techniques to provide a more accurate estimate of user waiting time and contrast them across a variety of different metrics including wait time and bounded per-processor slowdown using simulated data based on real job workload traces. The presented machine learning models improve overall wait time estimation by a factor of 4.1X over traditional scheduler-provided wait times.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">15:30-16:00</h3>
                <div class="program-session">
                    <div class="session-title">Break</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">16:00-17:00</h3>
                <div class="program-session">
                    <div class="session-title">Poster Presentations</div>
                    <div class="room-name">Pentland</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">17:00-18:30</h3>
                <div class="program-session">
                    <div class="session-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#poster_info" title="#poster_title">
                                Poster Session and Conference Reception <i class="fa fa-caret-right"></i>
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div id="poster_popup">
                                    <b id="poster_title">
                                        Posters
                                    </b>
                                    <hr/>
<!--                                    <div id="poster_info">
                                        <div>
                                            <ol>
                                                <li>
                                                    <b>Design, Implementation and Deployment of Sunrise Integrated Health Care Cluster</b>
                                                    <br>Authors:
                                                    Tao Yu (Tongji University),
                                                    Zhifeng Gu (Sunrise medical tech co. ltd),
                                                    Yizheng Sun (Sunrise medical tech co. ltd),
                                                    Xiaofei Wang (Institution: Sunrise medical tech co. ltd / Tsinghua University)
                                                </li>
                                                <li>
                                                    <b>Asynchronous I/O Optimization for X-ray Imaging via GPUDirect Storage</b>
                                                    <br>Authors:
                                                    Du Wu (Tokyo Institute of Technology / RIKEN-CCS),
                                                    Peng Chen (National Institute of Advanced Industrial Science and Technology / RIKEN-CCS),
                                                    Yiyu Tan (Iwate University),
                                                    Yusuke Tanimura (National Institute of Advanced Industrial Science and Technology),
                                                    Toshio Endo (Tokyo Institute of Technology),
                                                    Satoshi Matsuoka (RIKEN-CCS),
                                                    Mohamed Wahib (RIKEN-CCS)
                                                </li>
                                                <li>
                                                    <b>vBoost: A Lock-free Distributed Index based on vEB Tree for Disaggregated Memory</b>
                                                    <br>Authors:
                                                    Yuting Li (University of Science and Technology of China),
                                                    Yun Xu (University of Science and Technology of China),
                                                    Pengcheng Wang (Huawei),
                                                    Yonghui Xu (Huawei),
                                                    Weiguang Wang (Huawei)
                                                </li>
                                                <li>
                                                    <b>Communication Optimization for Distributed GCN Training on ABCI Supercomputer</b>
                                                    <br>Authors:
                                                    Chen Zhuang (Tokyo Institute of Technology, RIKEN Center for Computational Science),
                                                    Peng Chen (National Institute of Advanced Industrial Science and Technology),
                                                    Xin Liu (National Institute of Advanced Industrial Science and Technology),
                                                    Toshio Endo (Tokyo Institute of Technology),
                                                    Satoshi Matsuoka (RIKEN Center for Computational Science / Tokyo Institute of Technology),
                                                    Mohamed Wahib (RIKEN Center for Computational Science)
                                                </li>
                                                <li>
                                                    <b>Optimizing STAR Aligner for High Throughput Computing in the Cloud</b>
                                                    <br>Authors:
                                                    Piotr Kica (Sano Centre for Computational Medicine / AGH University of Kraków),
                                                    Sabina Lichołai (Sano Centre for Computational Medicine / Academic Computer Centre Cyfronet AGH),
                                                    Michał Orzechowski (Sano Centre for Computational Medicine / Academic Computer Centre Cyfronet AGH),
                                                    Maciej Malawski (Sano Centre for Computational Medicine / AGH University of Kraków)
                                                </li>
                                                <li>
                                                    <b>FDPVirt: Flexible Data Placement SSD Emulator</b>
                                                    <br>Authors:
                                                    Joonyeop Park (Seoul National University / Samsung Electronics Co., Ltd.),
                                                    Haeram Kim (Seoul National University),
                                                    Jiwon Ha (Seoul National University),
                                                    Hyeonsang Eom (Seoul National University)
                                                </li>
                                                <li>
                                                    <b>Enhanced Simulation and Analysis of Air Pollutants Using Multi-Platform HPC and In-Situ Visualization</b>
                                                    <br>Authors:
                                                    Chongke Bi (Tianjin University),
                                                    Fumiyoshi Shoji (RIKEN R-CCS),
                                                    Kenji Ono (Kyushu University),
                                                    Naohisa Sakamoto (Kobe University),
                                                    Jorji Nonaka (RIKEN R-CCS),
                                                    Honggang Yin (China Meteorological Administration),
                                                    Wenjuan Cui (Chinese Academy of Sciences)
                                                </li>
                                                <li>
                                                    <b>An optimization pass for training speed-up and strategy search in 3D parallelism</b>
                                                    <br>Authors:
                                                    Ryubu Hosoki (Tokyo Institute of Technology),
                                                    Kento Sato (RIKEN Center for Coputational Science),
                                                    Toshio Endo (Tokyo Institute of Technology),
                                                    Julien Bigot (CEA),
                                                    Edouard Audit (CEA)
                                                </li>
                                                <li>
                                                    <b>Beyond Training: A Zero-Shot Framework to Neural Architecture and Accelerator Co-Exploration</b>
                                                    <br>Authors:
                                                    Wei Fu (University of Science and Technology of China),
                                                    Wenqi Lou (University of Science and Technology of China),
                                                    Lei Gong (University of Science and Technology of China),
                                                    Chao Wang (University of Science and Technology of China),
                                                    Xuehai Zhou (University of Science and Technology of China)
                                                </li>
                                                <li>
                                                    <b>Evaluation of Vectorization Methods on Arm SVE Using the Exo Language</b>
                                                    <br>Authors:
                                                    Rin Iwai (Toyohashi University of Technology),
                                                    Emil Vatai (R-CCS, Riken),
                                                    Jens Domke (R-CCS, Riken),
                                                    Yukinori Sato (Toyohashi University of Technology)
                                                </li>
                                                <li>
                                                    <b>On the Building of a Common In-Situ Visualization Environment for Arm A64FX Supercomputers</b>
                                                    <br>Authors:
                                                    Jorji Nonaka (RIKEN R-CCS),
                                                    Daichi Obinata (Fujitsu),
                                                    Hiroyuki Ito (Ryoyu Systems),
                                                    Atsushi Toyoda (Intelligent Light APAC Office),
                                                    Naohisa Sakamoto (Kobe University),
                                                    Masahiro Nakao (RIKEN R-CCS),
                                                    Hitoshi Murai (RIKEN R-CCS),
                                                    Keiji Yamamoto (RIKEN R-CCS),
                                                    Masaaki Terai (RIKEN R-CCS),
                                                    Tomohiro Kawanabe (RIKEN R-CCS),
                                                    Shunji Uno (JAXA),
                                                    Naoyuki Fujita (JAXA),
                                                    Toshihiko Kai (RIKEN R-CCS),
                                                    Fumiyoshi Shoji (RIKEN R-CCS),
                                                    Takanori Haga (JAXA),
                                                    Seiji Tsutsumi (JAXA),
                                                    Manabu Motokawa (JAXA),
                                                    Atsuhi Fujino (JAXA)
                                                </li>
                                                <li>
                                                    <b>Introduction of WHEEL: An analysis workflow tool for industrial users and its use case on supercomputer Fugaku</b>
                                                    <br>Authors:
                                                    Tomohiro Kawanabe (RIKEN R-CCS),
                                                    Naoyuki Sogo (Longtail software LLC),
                                                    Kenji Ono (Research Institute for Information Technology, Kyushu University)
                                                </li>
                                                <li>
                                                    <b>Implementing Fast Modal Filtering of SCALE-DG</b>
                                                    <br>Authors:
                                                    Xuanzhengbo Ren (Graduate School of Informatics, Nagoya University),
                                                    Yuta Kawai (RIKEN Center for Computational Science),
                                                    Hirofumi Tomita (RIKEN Center for Computational Science),
                                                    Seiya Nishizawa (RIKEN Center for Computational Science),
                                                    Takahiro Katagiri (Information Technology Center, Nagoya University),
                                                    Masatoshi Kawai (Information Technology Center, Nagoya University),
                                                    Tetsuya Hoshino (Information Technology Center, Nagoya University),
                                                    Toru Nagai (Information Technology Center, Nagoya University)
                                                </li>
                                                <li>
                                                    <b>[Best Student Poster Award]</b><br>
                                                    <b>Enhancing Large Scale Brain Simulation with Optimized Parallel Algorithms on Fugaku Supercomputer</b>
                                                    <br>Authors:
                                                    Tianxiang Lyu (Juntendo University),
                                                    Mitsuhisa Sato (Juntendo University),
                                                    Shigeki Aoki (Juntendo University),
                                                    Ryutaro Himeno (Juntendo University),
                                                    Zhe Sun (Juntendo University)
                                                </li>
                                                <li>
                                                    <b>Heterogeneous Application Coupling Library for Center-Wide QC-HPC Hybrid Computing</b>
                                                    <br>Authors:
                                                    Shinji Sumimoto (The University of Tokyo),
                                                    Kazuya Yamazaki (The University of Tokyo),
                                                    Yao Hu (The University of Tokyo),
                                                    Kengo Nakajima (The University of Tokyo / RIKEN CCS)
                                                </li>
                                                <li>
                                                    <b>Innovative Computational Science by Integration of Simulation/Data/Learning on Heterogeneous Supercomputers</b>
                                                    <br>Authors:
                                                    Kengo Nakajima (The University of Tokyo),
                                                    Takashi Furumura (The University of Tokyo),
                                                    France Boillod-Cerneux (CEA),
                                                    Edoardo Di Napoli (Jülich Supercomputing Centre),
                                                    Estela Suarez (Jülich Supercomputing Centre),
                                                    Takshi Arakawa (CliMTech),
                                                    Shinji Sumimoto (The University of Tokyo),
                                                    Hisashi Yashiro (NIES)
                                                </li>
                                                <li>
                                                    <b>A Lossless-Ethernet-based interconnect for FPGA clusters toward FTQC</b>
                                                    <br>Authors:
                                                    Yoshito Higa (Kumamoto University),
                                                    Yasunori Osana (Kumamoto University)
                                                </li>
                                                <li>
                                                    <b>Post-Route Power Estimation: a Case Study of RIKEN-CGRA</b>
                                                    <br>Authors:
                                                    Chenlin Shi (The University of Electro-Communications / RIKEN Center for Computational Science),
                                                    Boma Adhi (RIKEN Center for Computational Science),
                                                    Shinobu Miwa (The University of Electro-Communications / RIKEN Center for Computational Science),
                                                    Kentaro Sano (RIKEN Center for Computational Science)
                                                </li>
                                                <li>
                                                    <b>Scalable Connection of Qubits to Quantum Error Correction Systems using Ethernet</b>
                                                    <br>Authors:
                                                    Jan-Erik R. Wichmann (RIKEN Center for Computational Science),
                                                    Kentaro Sano (RIKEN Center for Computational Science)
                                                </li>
                                                <li>
                                                    <b>Workload Analytics of LLMs Training on ABCI</b>
                                                    <br>Authors:
                                                    Yusuke Tanimura (National Institute of Advanced Industrial Science and Technology),
                                                    Naoki Onishi (National Institute of Advanced Industrial Science and Technology),
                                                    Shinichiro Takizawa (National Institute of Advanced Industrial Science and Technology)
                                                </li>
                                                <li>
                                                    <b>Preliminary Performance Evaluation of Grace-Hopper GH200</b>
                                                    <br>Authors:
                                                    Toshihiro Hanawa (Information Technology Center, The University of Tokyo / JCAHPC),
                                                    Kengo Nakajima (Information Technology Center, The University of Tokyo / JCAHPC),
                                                    Yohei Miki (Information Technology Center, The University of Tokyo / JCAHPC),
                                                    Takashi Shimokawabe (Information Technology Center, The University of Tokyo / JCAHPC),
                                                    Kazuya Yamazaki (Information Technology Center, The University of Tokyo / JCAHPC),
                                                    Shinji Sumimoto (Information Technology Center, The University of Tokyo / JCAHPC),
                                                    Osamu Tatebe (Center for Computational Sciences, University of Tsukuba / JCAHPC),
                                                    Taisuke Boku (Center for Computational Sciences, University of Tsukuba / JCAHPC),
                                                    Daisuke Takahashi (Center for Computational Sciences, University of Tsukuba / JCAHPC),
                                                    Akira Nukada (Center for Computational Sciences, University of Tsukuba / JCAHPC),
                                                    Norihisa Fujita (Center for Computational Sciences, University of Tsukuba / JCAHPC),
                                                    Ryohei Kobayashi (Center for Computational Sciences, University of Tsukuba / JCAHPC),
                                                    Hiroto Tadano (Center for Computational Sciences, University of Tsukuba / JCAHPC),
                                                    Akira Naruse (NVIDIA)
                                                </li>
                                                <li>
                                                    <b>Evaluating MPI Performance on SGX and Gramine</b>
                                                    <br>Authors:
                                                    Kota Shimojima (University of Electro-Communication),
                                                    Shinobu Miwa (University of Electro-Communication),
                                                    Hayato Yamaki (University of Electro-Communication),
                                                    Hiroki Honda (University of Electro-Communication)
                                                </li>
                                                <li>
                                                    <b>Investigating Nvidia GPU Architecture Trends via Microbenchmarks</b>
                                                    <br>Authors:
                                                    Lingqi Zhang (RIKEN Center for Computational Science / Tokyo Institute of Technology),
                                                    Ryan Barton (Tokyo Institute of Technology),
                                                    Peng Chen (National Institute of Advanced Industrial Science and Technology),
                                                    Wang Xiao (Oak Ridge National Laboratory),
                                                    Endo Toshio (Tokyo Institute of Technology),
                                                    Satoshi Matsuoka (RIKEN Center for Computational Science / Tokyo Institute of Technology),
                                                    Mohamed Wahib (RIKEN Center for Computational Science)
                                                </li>
                                                <li>
                                                    <b>Performance Insights into Supporting Kokkos Views in the Kokkos Comm MPI Library</b>
                                                    <br>Authors:
                                                    C. Nicole Avans (Sandia National Laboratories / Tennessee Technological University),
                                                    Jan Ciesko (Sandia National Laboratories),
                                                    Carl Pearson (Sandia National Laboratories),
                                                    Evan Drake Suggs (Tennessee Technological University),
                                                    Stephen L. Olivier (Sandia National Laboratories),
                                                    Anthony Skjellum (Tennessee Technological University)
                                                </li>
                                                <li>
                                                    <b>Toward providing root privilege to flagship HPC users with thin-hypervisor</b>
                                                    <br>Authors:
                                                    Takaaki Fukai (National Institute of Advanced Industrial Science and Technology),
                                                    Manami Mori (Tokyo Metropolitan University / National Institute of Advanced Industrial Science and Technology),
                                                    Keiji Yamamoto (RIKEN Center for Computational Science),
                                                    Takahiro Hirofuchi (National Institute of Advanced Industrial Science and Technology),
                                                    Takuya Asaka (Tokyo Metropolitan University)
                                                </li>
                                                <li>
                                                    <b>[Best Poster Award]</b><br>
                                                    <b>Cheetah: An Efficient Deterministic Concurrency Control Scheme with Non-visible Write Elimination and Re-designed Garbage Collection</b>
                                                    <br>Authors:
                                                    Haowen Li (Keio University),
                                                    Rina Onishi (Keio University),
                                                    Hideyuki Kawashima (Keio University)
                                                </li>
                                                <li>
                                                    <b>Neko: A Modern, Portable, and Scalable Framework for Extreme-Scale Computational Fluid Dynamics</b>
                                                    <br>Authors:
                                                    Niclas Jansson (KTH Royal Institute of Technology),
                                                    Martin Karp (KTH Royal Institute of Technology),
                                                    Stefano Markidis (KTH Royal Institute of Technology),
                                                    Philipp Schlatter (Friedrich-Alexander University, Erlangen-Nuremberg)
                                                </li>
                                                <li>
                                                    <b>Using SYCLomatic to migrate CUDA code to oneAPI adapting NVIDIA GPU</b>
                                                    <br>Authors:
                                                    Wentao Liang (University of Tsukuba),
                                                    Norihisa Fujita (University of Tsukuba),
                                                    Ryohei Kobayashi (University of Tsukuba),
                                                    Taisuke Boku (University of Tsukuba)
                                                </li>
                                                <li>
                                                    <b>Preliminary Evaluation of Kyokko for Inter-FPGA Communication Framework CIRCUS</b>
                                                    <br>Authors:
                                                    Kaito Kitazume (Graduate School of Science and Technology, University of Tsukuba),
                                                    Norihisa Fujita (Center for Computational Sciences, University of Tsukuba),
                                                    Ryohei Kobayashi (Center for Computational Sciences, University of Tsukuba),
                                                    Taisuke Boku (Center for Computational Sciences, University of Tsukuba)
                                                </li>
                                                <li>
                                                    <b>Leveraging Portals4 Microbenchmarks to Enhance GASPI Performance on BXI Networks</b>
                                                    <br>Authors:
                                                    Niklas Bartelheimer (Johannes Gutenberg University Mainz),
                                                    Sarah Neuwirth (Johannes Gutenberg University Mainz)
                                                </li>
                                                <li>
                                                    <b>Refining Compaction Offloading I/O Stack for LSM-based Key-Value Stores with SPDK</b>
                                                    <br>Authors:
                                                    Honghyeon Yoo (Sogang University),
                                                    Hongsu Byun (Sogang University),
                                                    Sungyong Park (Sogang University)
                                                </li>
                                            </ol>
                                        </div>
                                    </div>-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="room-name">JMCC</div>
                </div>
                <div style="clear: both;"></div>
            </div>
        </td>

        <td style="min-width: 225px; max-width: 225px;" valign="top">
            <h2 class="section-title">Thursday September 4th</h2>

            <div class="timeslot"><h3 class="session-time">8:30-9:30</h3>
                <div class="program-session">
                    <div class="session-title">Registration</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">9:30-10:30</h3>
                <div class="program-session">
                    <div class="session-title">Keynote (2): Rosa Badia</div>
                    <div style="clear: both;"></div>
                    <div class="room-name">Pentland</div>
                    <div style="clear: both;"></div>
                    <div class="session-chair">Chair:</div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <a href="keynotes/index.html#keynote2" target="_parent"><div style="font-size:15px"><b>Lifecycle-Aware Workflow Design for Scalable Digital Twins in HPC Ecosystems</b></div></a>
                        </div>
                    </div>
                    <div style="clear: both;"></div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">10:30-11:00</h3>
                <div class="program-session">
                    <div class="session-title">Break</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">11:00-12:30</h3>
                <div class="program-session">
                    <div class="session-title">Session (3) Storage and IO</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s3ta1_info" title="#s3ta1_title">
			    Proactive SSD Failure Prediction with A Gradient-Guided LSTM-xLSTM Hybrid Model
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s3ta1_title">
				    Proactive SSD Failure Prediction with A Gradient-Guided LSTM-xLSTM Hybrid Model
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s3ta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Proactive SSD failure prediction can help maintenance personnel in addressing failing drives in advance and has long been a major research direction in the field of dependable systems. However, the improvement of existing modeling methods' accuracy has been severely hindered by issues such as inconsistent data distributions, extreme data imbalance, and dynamic changes in the correlation between attributes and failures. Herein, we propose an innovative framework called GMPpredictor, which significantly improves the accuracy of SSD failure prediction. Specifically, GMPpredictor first partitions the data based on drive models to handle the distribution differences among different models. Secondly, in addressing the issue of extreme data imbalance, we optimize both the data and model levels, employing a dynamically adjusted loss function to balance the class weights. Then, by leveraging gradient information, we assign higher weights to features that are strongly correlated with failures, further enhancing the model's focus on critical features. Finally, we combine LSTM and xLSTM in a hybrid structure for failure prediction, fully utilizing the advantages of both networks to handle complex patterns in failure data across different drive models. GMPpredictor achieves a precision of 93.77% and an F0.5 score of 85.44%, with a FAR of only 0.05%. Notably, we have evaluated the effectiveness of GMPpredictor using real-world data collected from large-scale solid-state drives in data centers, achieving successful application from laboratory-scale to industry-scale.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s3ta2_info" title="#s3ta2_title">
			    EquilibrIO: Taming the I/O Tides in High-Performance Computing
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s3ta2_title">
				    EquilibrIO: Taming the I/O Tides in High-Performance Computing
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s3ta2_info">
                                            <div class="author-names">
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
In high-performance computing systems, jobs typically have exclusive compute access but share storage resources, such as the parallel file system, often becoming a point of contention. Concurrent execution of data-intensive jobs can exacerbate this phenomenon, as jobs compete for shared resources, impeding each other's progress while suffering from limited I/O bandwidth. As a result, the increasing I/O intensity of workloads places greater demands on resource management systems to optimize the scheduling of data-intensive jobs. Although scheduling decisions significantly impact shared storage systems, scheduling algorithms on production systems generally ignore the I/O intensity of individual jobs. In this work, we present EquilibrIO, a novel job scheduling algorithm that minimizes resource contention and maintains fairness by balancing computation and I/O over time, while requiring minimal information collected by tools commonly used in high-performance computing systems. We show that, depending on the desired level of fairness, our algorithm can reduce the I/O slowdown caused by contention from 64% to 4%. The results further demonstrate that 25% of the jobs augmented with additional I/O information are sufficient to minimize file system congestion, cutting the effect of I/O slowdown by half.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s3ta3_info" title="#s3ta3_title">
			    CFseq: A Framework for Constructing Compression-Friendly Field Sequences for Network Logs
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s3ta3_title">
				    CFseq: A Framework for Constructing Compression-Friendly Field Sequences for Network Logs
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s3ta3_info">
                                            <div class="author-names">
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                      <p>
							The rapid growth of network traffic has resulted in a substantial increase in log data, creating significant challenges for storage and processing. Although general-purpose compression algorithms are widely used, they often underperform on network logs due to their inability to exploit inherent structural characteristics. While advanced compression techniques can offer better performance, they typically require extensive system modifications and add deployment complexity.
						      </p>
						      <p>
This paper presents CFseq, a lightweight and efficient framework designed to construct compression-friendly field sequences that improve the compressibility of network logs. CFseq is founded on two key observations: first, some fields exhibit high redundancy; second, others contain shared prefixes or suffixes that are well suited to compression algorithms. The framework comprises two modules: the Text Similarity Enhancement module, which ranks fields based on information entropy, and the Brute-Force Search module, which identifies the optimal field order for compression. CFseq operates without modifying existing compression or decompression pipelines, allowing for seamless and low-cost integration. Experimental results show that CFseq improves the compression ratios of general-purpose compressors by up to 32% and enhances the performance of the state-of-the-art advanced compressor Denum by up to 20%.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="hr">
                    <hr/>
                </div>
                <div class="program-session">
                    <div class="session-title">Session (4) Networking and Communications</div>
                    <div class="room-name">Prestonfield</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s4ta1_info" title="#s4ta1_title">
			      Towards dynamic message passing protocols for stencil-based communication patterns
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s4ta1_title">
				      Towards dynamic message passing protocols for stencil-based communication patterns
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s4ta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Halo-exchange communication patterns occur in many stencil-based HPC applications such as MiniAMR, MiniGhost, and MILC. In this pattern, each process performs a mix of inter-node and intra-node transfers. Depending on the input and processor grid size, the amount of time spent in inter-node or intra-node could dominate the total communication time. Therefore, in this work, we propose a dynamic protocol for intra-node and inter-node transfers that optimizes the communication time. With the proposed designs, we show up to 20% benefits in 3D stencil communication benchmarks and 18% in the MiniAMR application at a scale of 2304 processes.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s4ta2_info" title="#s4ta2_title">
			      PIAR: Path-Improved Adaptive Routing for Dragonfly Networks
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s4ta2_title">
				      PIAR: Path-Improved Adaptive Routing for Dragonfly Networks
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s4ta2_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
For the next-generation exascale supercomputing communication systems, Dragonfly topology offers strong scalability, low latency, and cost efficiency. Dragonfly networks have already been implemented in current supercomputers and will continue to expand in future systems. Adaptive routing in Dragonfly topologies is critical for network performance. The traditional UGAL routing algorithm, which uses the Valiant mechanism to select non-minimal paths, does not adequately consider the impact of high hops in non-minimal paths, often unnecessarily increasing the average path length, thereby increasing network latency and load. Furthermore, UGAL inaccurately estimates the congestion of the entire routing path based on local information, leading to suboptimal routing decisions that limit the algorithm's performance. In this paper, we propose PIAR, a novel path-improved adaptive routing algorithm. PIAR dynamically selects paths based on the status of local and global channels, prioritizing non-minimal paths with fewer hops to reduce network latency and load, thereby improving network performance. Additionally, we present the microarchitecture of the routing computation unit. Our evaluation results demonstrate that, compared with advanced algorithms such as PAR_PH, TPR, and UGAL_LE, PIAR achieves an average throughput improvement of 19.2% and reduces latency by up to 13.4% under synthetic traffic. Under mixed traffic, PIAR achieves an average throughput improvement of 23.6% and reduces the latency by up to 33.8%. For application workloads, PIAR achieves an average reduction of 24.0% in packet latency.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s4ta3_info" title="#s4ta3_title">
			      Cascade: a Collaborative Algorithm for Scalable And Efficient Neighborhood Allgather
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s4ta3_title">
				      Cascade: a Collaborative Algorithm for Scalable And Efficient Neighborhood Allgather
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s4ta3_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Neighborhood collectives are a critical feature of MPI, enabling efficient communication in applications with sparse communication patterns. This research proposes Cascade, a new algorithm for neighborhood allgather collective that organizes computing nodes along multiple paths based on their distance to the current node. In this approach, messages are forwarded along these paths and propagated until all outgoing neighbors receive them, reducing the communication time. A performance model is developed to analyze the algorithm’s efficiency. Experimental results demonstrate that the Cascade algorithm achieves up to 9.54× and 7.05× speedup over Open MPI for random sparse graphs and Moore neighborhoods, respectively. Additionally, the algorithm improves performance by up to 5.25× for a sparse matrix-matrix multiplication kernel.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">12:30-13:30</h3>
                <div class="program-session">
                    <div class="session-title">Lunch Break</div>
                </div>
                <div class="hr">
                    <hr/>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">13:30-15:00</h3>
                <div class="program-session">
                    <div class="session-title">Session (5) Optimising GPU Performance</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s5ta1_info" title="#s5ta1_title">
			      Uniconn: A Uniform High-Level Communication Library for Portable Multi-GPU Programming
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s5ta1_title">
				      Uniconn: A Uniform High-Level Communication Library for Portable Multi-GPU Programming
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s5ta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Modern HPC and AI systems increasingly rely on multi-GPU clusters, where communication libraries such as MPI, NCCL/RCCL, and NVSHMEM enable data movement across GPUs. While these libraries are widely used in frameworks and solver packages, their distinct APIs, synchronization models, and integration mechanisms introduce programming complexity and limit portability. Performance also varies across workloads and system architectures, making it difficult to achieve consistent efficiency. These issues present a significant obstacle to writing portable, high-performance code for large-scale GPU systems. We present UNICONN, a unified, portable high-level C++ communication library that supports both point-to-point and collective operations across GPU clusters. UNICONN enables seamless switching between backends and APIs (host or device) with minimal or no changes to application code. We describe its design and core constructs, and evaluate its performance using network benchmarks, a Jacobi solver, and a Conjugate Gradient solver. Across three supercomputers, we compare UNICONN’s overhead against CUDA/ROCm-aware MPI, NCCL/RCCL, and NVSHMEM on up to 64 GPUs. In most cases, UNICONN incurs negligible overhead, typically under 3% for the Jacobi solver and under 2% for the Conjugate Gradient solver.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
		    
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s5ta2_info" title="#s5ta2_title">
			      A Pattern-Aware Finite Element Matrix Assembly Method on GPUs
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s5ta2_title">
				      A Pattern-Aware Finite Element Matrix Assembly Method on GPUs
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s5ta2_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
The Finite Element Method (FEM) is a fundamental technique for solving large-scale and complex engineering problems. During the construction of the system equations, the efficiency of finite element matrix assembly plays a crucial role in the overall performance. However, existing approaches often overlook the sensitivity of assembly algorithm performance to mesh characteristics, making it difficult to achieve optimal performance across diverse problems. In this work, we propose a novel pattern-aware FEM matrix assembly method on GPUs. To this end, we thoroughly analyze the key factors affecting performance and extract a set of potentially influential mesh features and density representations. Based on this, we construct a Deep learning-based prediction model that fully captures the input mesh characteristics to predict the performance-optimal assembly strategy. Experimental results on mesh datasets with a wide range of feature variations demonstrate that our method achieves remarkable prediction accuracy and delivers up to 7.34× speedup in execution time compared to state-of-the-art approaches. To the best of our knowledge, this is the first work that introduces auto-tuning for the FEM matrix assembly process.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s5ta3_info" title="#s5ta3_title">
			      nsys2prv: detailed and quantitative analysis of large-scale GPU execution traces with Paraver
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s5ta3_title">
				      nsys2prv: detailed and quantitative analysis of large-scale GPU execution traces with Paraver
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s5ta3_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
        This work presents a tool, a methodology, a set of metrics, and practical examples for evaluating the performance of large-scale AI and traditional HPC applications using GPUs. NSYS2PRV is a tool that converts NVIDIA Nsight Systems reports into traces compatible with Paraver, enabling significantly enhanced insight compared to current performance analysis practices. By leveraging the capabilities of a well-established HPC performance analysis tool, we enable the comparison of execution traces and the quantification of microscopic-level differences to explain behaviors across hundreds or more computing devices. We argue that large-scale GPU applications and AI workloads can greatly benefit from the type of large-scale performance analysis introduced here, an approach that is not yet widely adopted in this domain. Translating nsys-generated traces to Paraver allows analysts to combine the fine-grained, highly accurate execution data obtainable from proprietary tools with the flexibility and scalability of an open-source, parallel performance analysis environment. Paraver also enables easy, customizable computation of efficiency metrics. This work demonstrates a more effective and insightful analysis experience than that offered by the native visualization tools in Nsight Systems. Additionally, we introduce a set of Paraver-compatible metrics that guide the analysis process, and we showcase examples where these metrics were successfully applied to real-world AI and HPC workloads.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="hr">
                    <hr/>
                </div>
                <div class="program-session">
                    <div class="session-title">Session (6) Systemware and System Architectures<</div>
                    <div class="room-name">Prestonfield</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s6ta1_info" title="#s6ta1_title">
			    Cache Less to Save More: A Cost-Based Distributed Caching Strategy for ICN
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s6ta1_title">
				    Cache Less to Save More: A Cost-Based Distributed Caching Strategy for ICN
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s6ta1_info">
                                          <div class="author-names">
					    
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
The rapid growth of global data traffic has exposed limitations in traditional content delivery architectures. Information-Centric Networking (ICN) addresses these challenges by leveraging in-network caching to enhance scalability, reduce latency, and improve overall performance. However, existing caching strategies either optimize single-node cache management without considering network-wide costs, or address distribution without hardware-aware cost modeling. We propose a unified, cost-aware distributed caching strategy that integrates multi-tier caching at each node with network-wide replication, guided by a comprehensive cost model including resource depreciation, bandwidth, energy, and Service Level Agreement compliance. Our approach minimizes redundant replication on the network while maximizing cache hit rates and reducing latency. Experiments show on average 19.15%, and up to 45.19%, cost reduction, 8.11%, and up to 32.15%, cache hit ratio increase, and 9.01%, and up to 27.21%, latency improvement over other methods, offering a cost-effective solution for next-generation ICN systems.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s6ta2_info" title="#s6ta2_title">
			      SoCL: Scalable and Latency-Optimized Microservices in Serverless Edge Computing
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s6ta2_title">
				      SoCL: Scalable and Latency-Optimized Microservices in Serverless Edge Computing
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s6ta2_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
       Microservices have become an important design paradigm for large-scale distributed systems, offering flexible provisioning options. A fundamental challenge is the exponential growth of the solution space with the number of user requests, posing challenges to efficient provisioning and scheduling when aiming to balance cost and latency under resource constraints in large-scale dynamic edge environments. To tackle this problem, we formulate a joint optimization model for microservice provisioning and routing that integrates cost efficiency and latency reduction while accounting for uncertainties in the origin location of requests. To establish a unified framework that facilitates decision-making, we propose an integer linear programming (ILP) model that captures the dependencies between microservices in the service chain. Our Scalable optimization framework with Cost-efficiency and Latency reduction (SoCL) comprises three stages: an initial partitioning guarantees latency bounds, a pre-provisioning stage considers provisioning cost, and a multi-scale combination stage balances cost and latency through parallel and serial local search. Extensive experiments conducted across diverse scenarios based on a commonly used data set demonstrate that the proposed SoCL framework significantly increases cost efficiency and decreases latency compared to established baselines, while reducing execution time up to one order of magnitude compared to obtaining the optimal solution by optimizer.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s6ta3_info" title="#s6ta3_title">
			    Detecting Silent Data Corruption From Hardware Counters
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s6ta3_title">
				    Detecting Silent Data Corruption From Hardware Counters
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s6ta3_info">
                                          <div class="author-names">
					    
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Silent Data Corruptions (SDCs), which can manifest at the application level despite extensive screening and testing, can disrupt meaningful scientific interpretation, thereby necessitating robust monitoring tools capable of detecting them. While prior approaches have demonstrated competitive detection performance, they often require nontrivial modifications to algorithms or prior knowledge, such as spatial or temporal data patterns, to make those approaches effective. Furthermore, the error model through standard random bit flips may not reflect realistic scenarios, potentially including relatively easy-to-detect cases with obvious deviations. In this work, we study SDCs and their effects on sparse matrix computations, prevalent kernels in many scientific applications, using hardware counters, which could serve as a holistic indicator of revealing program behavior changes due to SDCs. We experiment with a set of sparse matrix benchmarks using a method that simulates data corruption to varying degrees based on our extensive analysis of error propagation, creating realistic SDC occurrences at the application level. We detail the process of sampling hardware performance counters with minimal disturbance. Using the collected hardware counters, we train various classes of classifiers, including standard ML, neural-network-based, and unsupervised, to accurately detect SDCs. Our experimental evaluations through k-fold cross-validation indicate that hardware counters can effectively detect the presence of SDCs with a low false positive rate, incurring comparable training overheads and minor inference overhead compared to the state-of-the-art. Our approach achieves a competitive average recall (>0.91) with a realistic error rate based on the observed error propagation and low runtime overhead (<2%) while avoiding program modifications.
                                                        </p>
                                                    </blockquote>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">15:00-15:30</h3>
                <div class="program-session">
                    <div class="session-title">Break</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">15:30-17:30</h3>
                <div class="program-session">
                    <div class="session-title">Session (7) Performance Modelling and Optimisation</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s7ta1_info" title="#s7ta1_title">
			    Lessons from Profiling and Optimizing Placement in AMR Codes
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s7ta1_title">
				    Lessons from Profiling and Optimizing Placement in AMR Codes
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s7ta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Block-structured Adaptive Mesh Refinement (AMR), while essential for improving efficiency in large-scale irregular and dynamic simulations, poses unique optimization challenges. Previous work has identified load imbalance and synchronizations as key obstacles to performance, but the deep understanding of complex runtime behavior needed to systematically address them remains elusive. In this paper, we integrate telemetry collection, analysis, and intervention to bridge this understanding gap. We find that obtaining reliable, actionable telemetry requires systematic tuning across application, runtime, and hardware layers. Leveraging such trustworthy telemetry, we design CPLX, a tunable placement policy balancing compute load and communication locality, improving runtime by up to 21.6% over optimized baselines. Our experience highlights the empirical nature of tuning and motivates structured telemetry-driven optimization.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s7ta2_info" title="#s7ta2_title">
			      Fine-grain energy consumption modeling of HPC task-based programs
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s7ta2_title">
				      Fine-grain energy consumption modeling of HPC task-based programs
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s7ta2_info">
                                            <div class="author-names">
                            
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
                            The power consumption of supercomputers is and will be a major concern in the future. Therefore, reducing the power consumption of high performance computing (HPC) applications is mandatory. Monitoring the energy consumption of HPC programs is a good first step: using external or software power meters, one can measure the energy consumption of an entire compute node or some of its hardware components. Unfortunately, the differences in scope and time scale between power meters and code level functions prevents the identification of power hungry code blocks. For this work, we propose leveraging the tracing mechanism of the StarPU runtime system in order to estimate task level power consumption. We trace the execution of the application while regularly measuring coarse-grain energy consumption of central processing units (CPUs) and graphics processing units (GPUs) using vendor software interfaces. After execution, we identify the executed tasks on each processing unit for every coarse- grain energy measurement interval. We then use this information to generate an overdetermined linear system linking tasks and energy measurements. Subsequently, solving the system allows us to estimate the fine-grain power consumption of each task independently of its actual duration. We achieve mean average percentage errors (MAPE) of 0.5 to 5% on various CPUs, and 10 to 28% on GPUs. We show that a solution generated from a run can be used to predict the energy consumption of other runs with different scheduling policies.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s7ta3_info" title="#s7ta3_title">
			    A Versatile Simulated Data Transport Layer for In Situ Workflows Performance Evaluation
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s7ta3_title">
				    A Versatile Simulated Data Transport Layer for In Situ Workflows Performance Evaluation
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s7ta3_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
In situ processing does not only allow scientific applications to face the explosion in data volume and velocity but also addresses the time constraints of many simulation-analysis workflows by providing scientists with early insights about their applications at runtime. Multiple frameworks implement the concept of a data transport layer (DTL) to enable such in situ workflows. These tools are versatile, directly or indirectly access data on the same node, an another node of the same cluster, or a completely distinct node, and allow data publishers and subscribers to run on the same computing resources or not. This versatility puts on researchers the onus of taking key decisions related to resource allocation and data transport to ensure the most efficient execution of their workflows. However, they lack the appropriate tools to assess the performance of particular design and deployment options.
							</p>
							<p>
In this paper we introduce a versatile simulated DTL for the performance evaluation of in situ workflows. This open-source library builds on the SimGrid toolkit. It facilitates the evaluation of the performance behavior, at scale, of different data transport configurations and the study of the effects of resource allocation strategies. We demonstrate the scalability, versatility, and accuracy of this simulated DTL by reproducing the execution of two synthetic benchmarks and a real-world in situ workflow. Results show that the proposed library can simulate, on a single core, the interactions of tens of thousands of simulated processes in a few seconds and provide insights on the respective performance of different execution scenarios.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="hr">
                    <hr/>
                </div>
                <div class="program-session">
                    <div class="session-title">Session (8) Storage and I/O</div>
                    <div class="room-name">Prestonfield</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s8ta1_info" title="#s8ta1_title">
			      Bridging Metadata Service and CXL: A Metadata-grained and Directory-aware Storage Engine for Distributed Storage Systems
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s8ta1_title">
				      Bridging Metadata Service and CXL: A Metadata-grained and Directory-aware Storage Engine for Distributed Storage Systems
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s8ta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
To improve storage efficiency in large-scale clustered storage systems, deduplication that removes duplicate chunks has been widely deployed in distributed ways. Many distributed deduplication-related studies focus on backup storage, and some recent studies focus on deploying deduplication in clustered primary storage systems which store active data. While fragmentation is one of the traditional challenges in backup deduplication, we observe that a new fragmentation problem arises when performing deduplication in the clustered primary storage system due to the system’s concurrent file writes. However, we find that existing state-of-the-art methods that address traditional fragmentation in backup deduplication fail to work effectively for the new fragmentation problem, as they significantly incur additional redundancy or lower the deduplication ratio.
							</p>
							<p>
In this paper, we revisit fragmentation-solving methods in memory management and our main idea is inspired by the classic garbage collection methods in memory management: relocating fragments consecutively. Based on the idea, we propose an effective deduplication mechanism for clustered primary storage systems, ReoDedup, which applies: i) a cosine-similarity based chunk relocating algorithm that aims to minimize the fragmentation; ii) an adjacency-table based relocating heuristic that reduces the relocating’s time complexity by placing two chunks residing in the same file consecutively; and iii) an indexremapping update scheme that alleviates the extra fragmentation caused by updates. We implement ReoDedup atop Ceph and our experiments via Alibaba Cloud show that the average read throughput of ReoDedup can be increased by up to 1.72× over state-of-the-arts, without any deduplication ratio loss.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s8ta2_info" title="#s8ta2_title">
			      Revisiting Fragmentation for Deduplication in Clustered Primary Storage Systems
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s8ta2_title">
				      Revisiting Fragmentation for Deduplication in Clustered Primary Storage Systems
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s8ta2_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
To improve storage efficiency in large-scale clustered storage systems, deduplication that removes duplicate chunks has been widely deployed in distributed ways. Many distributed deduplication-related studies focus on backup storage, and some recent studies focus on deploying deduplication in clustered primary storage systems which store active data. While fragmentation is one of the traditional challenges in backup deduplication, we observe that a new fragmentation problem arises when performing deduplication in the clustered primary storage system due to the system’s concurrent file writes. However, we find that existing state-of-the-art methods that address traditional fragmentation in backup deduplication fail to work effectively for the new fragmentation problem, as they significantly incur additional redundancy or lower the deduplication ratio.
							</p>
							<p>
In this paper, we revisit fragmentation-solving methods in memory management and our main idea is inspired by the classic garbage collection methods in memory management: relocating fragments consecutively. Based on the idea, we propose an effective deduplication mechanism for clustered primary storage systems, ReoDedup, which applies: i) a cosine-similarity based chunk relocating algorithm that aims to minimize the fragmentation; ii) an adjacency-table based relocating heuristic that reduces the relocating’s time complexity by placing two chunks residing in the same file consecutively; and iii) an indexremapping update scheme that alleviates the extra fragmentation caused by updates. We implement ReoDedup atop Ceph and our experiments via Alibaba Cloud show that the average read throughput of ReoDedup can be increased by up to 1.72× over state-of-the-arts, without any deduplication ratio loss.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s8ta3_info" title="#s8ta3_title">
			      FIFO-MEP: An Efficient Multi-Eviction-Point FIFO Cache with Stable Demotion for Burst-Oriented Access Mitigation
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s8ta3_title">
				      FIFO-MEP: An Efficient Multi-Eviction-Point FIFO Cache with Stable Demotion for Burst-Oriented Access Mitigation
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s8ta3_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Caching technology is widely used in multiple areas particularly in distributed computing, where its performance is highly dependent on the cache efficiency. The cache eviction algorithm serves as the core component of a cache, primarily aimed at improving cache efficiency by reducing the cache miss ratio. Numerous eviction algorithms are proposed in recent decades and state-of-the-art methods tend to adopt lazy promotion and quick demotion designs. Lazy promotion simplifies cache-hit operations for higher throughput, while quick demotion effectively filters the low-popularity objects. However, the two designs either fail to identify burst objects or suffer from unstable demotion precision. In order to address the above problems, we propose FIFO-MEP, an efficient FIFO cache with Multiple Eviction Points. The key design of FIFO-MEP is to introduce multiple fixed-position eviction points near the head of a FIFO queue. These eviction points enable repeated inspections of objects, leading to effective identification of burst objects. Meanwhile, by fixing positions of these eviction points, FIFO-MEP delivers stable demotion precision. We implement FIFO-MEP using libCacheSim and evaluated it on 5439 production traces for three typical cache sizes, and further verify its efficiency based on Memcached. The evaluation results show that FIFO-MEP reduces the miss ratio by an average of 15.8% across all experimental configurations. Compared to the state-of-the-art S3-FIFO, FIFO-MEP achieves cache efficiency improvement by up to 21.8% for large cache sizes. Furthermore, FIFO-MEP yields the best performance under 51% of all tested conditions.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                      <div class="slot-wrapper">
                        <span class="ttip_object_info_very_wide" rel="#s8ta4_info" title="#s8ta4_title">
			      RAN: Accelerating Data Repair with Available Nodes in Erasure-Coded Storage
                        </span>
                        <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s8ta4_title">
				      RAN: Accelerating Data Repair with Available Nodes in Erasure-Coded Storage
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s8ta4_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Distributed storage systems ensure data availability through fault-tolerant mechanisms, with erasure coding widely adopted for its low storage overhead. While effective, erasure coding incurs substantial repair traffic during data recovery, severely degrading repair performance. Recent research proposes repair algorithms to alleviate network bottlenecks at congested nodes. However, these algorithms primarily target downlink bottlenecks while overlooking uplink bottlenecks, which fundamentally limit repair efficiency, and fail to systematically handle diverse failure scenarios, increasing implementation complexity. In this paper, we propose RAN, an aggregation-based repair algorithm that alleviates both uplink and downlink bottlenecks by optimizing bandwidth utilization across all available nodes and aggregating network transfers via programmable network devices. Additionally, RAN systematically maximizes repair performance across diverse failure scenarios through a unified procedure. Experiments on Amazon EC2 show that RAN improves repair throughput by up to 68.9% for degraded read and 266.6% for full-node recovery compared to state-of-the-art algorithms.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">19:00-22:00</h3>
                <div class="program-session">
                    <div class="session-title">Conference Dinner</div>
                    <div class="room-name"><a href="https://www.openstreetmap.org/?#map=19/55.947205/-3.187245">Playfair Library</a></div>
                </div>
                <div style="clear: both;"></div>
            </div>
        </td>

        <td style="min-width: 225px; max-width: 225px;" valign="top">
            <h2 class="section-title">Friday September 5th</h2>

            <div class="timeslot"><h3 class="session-time">8:30-9:30</h3>
                <div class="program-session">
                    <div class="session-title">Registration</div>
                </div>
                <div style="clear: both;"></div>
            </div>


            <div class="timeslot"><h3 class="session-time">09:30-10:30</h3>
                <div class="program-session">
                    <div class="session-title">Keynote (3): Garth Wells</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <a href="keynotes/index.html#keynote3" target="_parent"><div style="font-size:15px"><b>Performance analysis of GPU architectures for solving partial differential equations</b></div></a>
                        </div>
                    </div>
                </div>
<div style="clear: both;"></div>
            
                
            <div class="timeslot"><h3 class="session-time">10:30-11:00</h3>
                <div class="program-session">
                    <div class="session-title">CLUSTER 2026 Presentation</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair:

                    </div>
                </div>
            <div style="clear: both;"></div>
            
            <div class="timeslot"><h3 class="session-time">11:00-11:30</h3>
                <div class="program-session">
                    <div class="session-title">Break</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">11:30-13:00</h3>
                <div class="program-session">
                    <div class="session-title">Session (9) Networking and Communications</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s9ta1_info" title="#s9ta1_title">
			      TRACE: A Targeted Recommender for VM Assignment in Cloud Environment
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s9ta1_title">
				      TRACE: A Targeted Recommender for VM Assignment in Cloud Environment
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s9ta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Multi-tenancy in modern cloud service co-locates multiple virtual machines (VMs) into physical machines (PMs) to improve resource efficiency. However, co-location introduces interference among VMs, potentially degrading the quality-of-service (QoS) for users. Previous methods predict QoS degradation and schedule VMs accordingly, but they are hard to integrate with real-world cloud schedulers and often overlook important information provided by VM metrics. Considering the above factors, we present TRACE, a novel QoS-aware, lightweight, and decoupled recommender for VM scheduling. Firstly, TRACE employs a dual-tower feature extraction mechanism that independently extracts metrics from VMs and PMs, thereby reducing the time complexity of the model. Secondly, the dual-tower is enhanced by Deep & Cross Networks to explicitly model cross-feature interactions, and we further incorporate a Set Transformer to process overlooked multi-VM metrics from the PM. Thirdly, TRACE designs a trainable similarity gate and an adaptive mask to filter suboptimal migrations, decoupling it from the scheduler for easy integration. Experimental results on data collected from real-world clusters show that TRACE outperforms state-of-the-art methods in QoS prediction accuracy and ranking quality, achieving at least 6.3% QoS improvements.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s9ta2_info" title="#s9ta2_title">
			    Scalable and Fast Inference Serving via Hybrid Communication Scheduling on Heterogeneous Networks
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s9ta2_title">
				    Scalable and Fast Inference Serving via Hybrid Communication Scheduling on Heterogeneous Networks
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s9ta2_info">
                                          <div class="author-names">
					    
                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
							  Advances in large language models (LLMs) have opened up new possibilities across various fields, fueling a new wave of interactive AI applications such as DeepSeek and ChatGPT. Inference serving systems play a crucial role in supporting these applications. Recent research indicates that when cross-server parallelization is enabled in inference serving systems, data synchronization overhead can exceed 65% of the total inference delay, making the reduction of communication overhead essential for speeding up inference. While existing systems accelerate cross-server communications by offloading synchronization operations to programmable switches, they often suffer from limited aggregation throughput under bursty traffic conditions, posing challenges for homogeneous network environments.
							</p>
							<p>
To address these challenges, we propose HeroServe, an innovative inference serving system that leverages heterogeneous networks to accelerate data synchronization in distributed clusters. Our approach enables a fast and scalable inference serving system by employing an offline planner for joint computation allocation and communication scheduling, along with an online scheduler for dynamic traffic management and load balancing. We implement a prototype on a testbed comprising six servers and two programmable switches. Experimental results demonstrate that HeroServe improves scalability by 1.53x while achieving lower latency compared to state-of-the-art solutions.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s9ta3_info" title="#s9ta3_title">
			    Communication Notification through User-Level Interrupts for the BXI Network
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s9ta3_title">
				    Communication Notification through User-Level Interrupts for the BXI Network
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s9ta3_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
							  To reduce the cost of communications in high-performance computing, it is possible to overlap communications with computations. Some communication protocols, such as rendez-vous, multi-chunk messages, and collectives, may require a completion notification to be processed before they can further progress. With active polling or passive waiting, completions are not processed while the application is busy with computation, and thus communication does not progress. However, with an event-based method like interrupts, it is expected to be much more reactive. Nevertheless, using interrupts usually involves system calls, which are avoided with high-performance networks.
</p>
<p>
							The Intel Sapphire Rapids processors introduced user-level interrupts (UINTR), hardware interrupts designed to be used directly in user space, without going through the kernel. However, their current implementation is limited to inter-process communication. They cannot be triggered from a device.
</p>
<p>
In this paper, we propose new mechanisms to extend the scope of user-level interrupts, so as to be able to trigger them from a device and not only from a CPU. We have implemented these mechanisms in the BXI network from Eviden. We have evaluated their performance: we obtain a latency only 2.4 times higher than active polling (v.s. 6 times higher for interrupts with system calls). We have assessed their ability to make communication progress when overlapped with computation; we observe a near-perfect computation/communication overlap.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="hr">
                    <hr/>
                </div>
                <div class="program-session">
                    <div class="session-title">Session (10) Algorithms and Numerical Approaches </div>
                    <div class="room-name">Prestonfield</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s10ta1_info" title="#s10ta1_title">
			      Parallel Selected Inversion of Block-tridiagonal with Arrowhead Matrices
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s10ta1_title">
				      Parallel Selected Inversion of Block-tridiagonal with Arrowhead Matrices
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s10ta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Molecular dynamics (MD) simulations are essential tools for investigating large-scale molecular systems, yet achieving high performance and scalability on CPU-based architectures remains challenging. In this study, we present a highly optimized framework based on DeepMD-kit for conducting 500 million-atom MD simulations on an ARMv8 SVE high-performance computing (HPC) system. Key optimizations include leveraging OpenMP for multi-threaded acceleration of DeepMD-kit and utilizing the ARMv8 SVE instruction set to optimize double-precision matrix multiplication in PyTorch. These enhancements enable single ARMv8 SVE 64-core processors to achieve 1.3x the training performance of NVIDIA V100 GPU, and two ARMv8 SVE 64-core processors to achieve 1.05x the inference performance of NVIDIA V100 GPU. Leveraging this optimized framework, we achieve large-scale MD simulations across 4,096 computing nodes.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s10ta2_info" title="#s10ta2_title">
			      Parallel tall-and-skinny QR factorization based on LU-CholeskyQR algorithm
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s10ta2_title">
				      Parallel tall-and-skinny QR factorization based on LU-CholeskyQR algorithm
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s10ta2_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
We present optimal parallel QR factorization algorithms with reduced communication overhead. QR factorization is widely applied to solve various problems in numerical linear algebra. Our focus is on problems involving dense tall-and-skinny matrices in large-scale parallel distributed memory systems. Reducing data communication is essential for achieving high performance in parallel algorithms because the communication cost is much greater than the computation cost. To date, several QR factorization algorithms have been optimized to reduce communication costs. This paper provides alternative parallel QR factorization algorithms based on the LU-CholeskyQR algorithm. Numerical experiment results demonstrated the accuracy and performance of the developed algorithms against benchmarks. The results indicate that the new algorithms are numerically stable even for ill-conditioned problems, and some of these algorithms are faster than other conventional algorithms.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s10ta3_info" title="#s10ta3_title">
			      Towards High-Performance and Portable Molecular Docking on CPUs through Vectorization
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s10ta3_title">
				      Towards High-Performance and Portable Molecular Docking on CPUs through Vectorization
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s10ta3_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Recent trends in the HPC field have introduced new CPU architectures with improved vectorization capabilities that require optimization to achieve peak performance and thus pose challenges for performance portability. The deployment of high-performing scientific applications for CPUs requires adapting the codebase and optimizing for performance. Evaluating these applications provides insights into the complex interactions between code, compilers, and hardware. We evaluate compiler auto-vectorization and explicit vectorization to achieve performance portability across modern CPUs with long vectors. We select a molecular docking application as a case study, as it represents computational patterns commonly found across HPC workloads. We report insights into the technical challenges, architectural trends, and optimization strategies relevant to the future development of scientific applications for HPC. Our results show which code transformations enable portable auto-vectorization, reaching performance similar to explicit vectorization. Experimental data confirms that x86 CPUs typically achieve higher execution performance than ARM CPUs, primarily due to their wider vectorization units. However, ARM architectures demonstrate competitive energy consumption and cost-effectiveness.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">13:00-14:00</h3>
                <div class="program-session">
                    <div class="session-title">Lunch Break</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">14:00-15:30</h3>
                <div class="program-session">
                    <div class="session-title">Session (11) Applications and Optimisation Approaches</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s11ta1_info" title="#s11ta1_title">
			      MoE-Rckpt: Efficient In-Memory Checkpointing for MoE Model Training with Dynamicity Awareness
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s11ta1_title">
				      MoE-Rckpt: Efficient In-Memory Checkpointing for MoE Model Training with Dynamicity Awareness
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s11ta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Mixture-of-Experts (MoE) has been extensively adopted for its incredible capability to expand model scale with a sub-linear increase in computational requirement. Training MoE models requires substantial computing nodes and extended periods, necessitating reliable distributed training systems. Checkpointing is a common approach to enhance training reliability by periodically saving model states. Current checkpointing optimizations focus on hiding checkpoint overhead in model training computations. However, these approaches overlook the dynamicity inherent in distributed MoE training, leading to an inefficient checkpointing mechanism.
							</p>
							<p>
In this paper, we propose MoE-Rckpt, a dynamicity-aware in-memory checkpointing approach for efficient MoE model training. We observe that the dynamicity impacts computation durations at both the layer and iteration levels. At the layer level, different model layers exhibit various computation durations, while at the iteration level, the computation time of the same layer differs across iterations. To adapt to the layer-level dynamicity, MoE-Rckpt employs online profiling at the granularity of individual layers. Based on the profiling results, it strategically partitions checkpoints into chunks and schedules checkpointing communication to overlap with model computations. To deal with the dynamicity across iterations, MoE-Rckpt speculatively activates the profiling and partitioning processes utilizing the temporal locality of the experts load. It can produce an optimal activation for low runtime overhead with high checkpoint partition accuracy. For mainstream MoE models, MoE-Rckpt achieves up to 1.56x and 5.98x end-to-end training speedup over Gemini and TorchSnapshot respectively under per-iteration checkpointing.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s11ta2_info" title="#s11ta2_title">
			    Multi-agent Independent PPO-based Automatic ECN Tuning for High-Speed Data Center Networks
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                  <b id="s11ta2_title">
				    Multi-agent Independent PPO-based Automatic ECN Tuning for High-Speed Data Center Networks
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s11ta2_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
 Explicit Congestion Notification (ECN)-based congestion control schemes have been widely adopted in high-speed data center networks (DCNs), where the ECN marking threshold plays a determinant role in guaranteeing a packet lossless DCN. However, existing approaches either employ static settings with immutable thresholds that cannot be dynamically self-adjusted to adapt to network dynamics, or fail to take into account many-to-one traffic patterns and different requirements of different types of traffic, resulting in relatively poor performance. To address these problems, this paper proposes a novel learning-based automatic ECN tuning scheme, named PET, based on the multi-agent Independent Proximal Policy Optimization (IPPO) algorithm. PET dynamically adjusts ECN thresholds by fully considering pivotal congestion-contributing factors, including queue length, output data rate, output rate of ECN-marked packets, current ECN threshold, the extent of incast, and the ratio of mice and elephant flows. PET adopts the Decentralized Training and Decentralized Execution (DTDE) paradigm and combines offline and online training to accommodate network dynamics. PET is also fair and readily deployable with commodity hardware. Comprehensive experimental results demonstrate that, compared with state-of-the-art static schemes and the learning-based automatic scheme, our PET achieves better performance in terms of flow completion time, convergence rate, queue length variance, and system robustness.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
		    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s11ta3_info" title="#s11ta3_title">
			    Efficient Multi-GPU Programming in Python: Reducing Synchronization and Access Overheads			    
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s11ta3_title">
				      Efficient Multi-GPU Programming in Python: Reducing Synchronization and Access Overheads
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s11ta3_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
        Python has become increasingly significant in domains such as data science, machine learning, scientific computing, and parallel programming. The libraries CuPy and Numba enable the development of parallel GPU code, while mpi4py and CuPy’s NCCL backend enable distributed computing across multiple GPUs. Despite its versatility, Python is often criticized for itsperformance limitations. Although pre-compilation and just-in-time compilation can minimize interpreter overhead, multi-GPU applications in Python often encounter significant performance bottlenecks due to the synchronization requirements between GPU kernels and communication libraries.
							</p>
							<p>
In this work, we present a detailed performance analysis of multi-GPU programming in Python using CuPy, Numba, NCCL and mpi4py. We identify excessive synchronization and costly array conversions as key sources of overhead and demonstrate that view-based data access can significantly improve performance. Furthermore, we show that using NCCL with asynchronous CUDA streams enables better overlap of computation and communication, mitigating interpreter-induced delays. Our evaluation includes both microbenchmarks and a multi-GPU implementation of the CloverLeaf mini-application. Results show that, with careful optimization, Python implementations can reach up to 90% of the performance of equivalent C-CUDA codes.
							</p>
							<p>
These findings highlight practical strategies for minimizing Python-specific overheads in multi-GPU scenarios and provide guidance for building efficient Python applications on modern GPU clusters.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                <div class="hr">
                    <hr/>
                </div>
                <div class="program-session">
                    <div class="session-title">Session (12) Scheduling and Applications</div>
                    <div class="room-name">Prestonfield</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s12ta1_info" title="#s12ta1_title">
			      BMPipe: Bubble-Memory Co-optimization Strategy Planner for Very-large DNN Training
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s12ta1_title">
				      BMPipe: Bubble-Memory Co-optimization Strategy Planner for Very-large DNN Training
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s12ta1_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Pipeline parallelism and activation recomputation are widely adopted optimization techniques, among others, to scale DNN training on large accelerator clusters. However, as DNNs grow in complexity and heterogeneity, it becomes increasingly difficult to determine the optimal combination of pipeline partitioning and recomputation strategies. Existing solutions either propose manual optimization approaches that do not scale, or automated approaches that explore only a subset of optimization possibilities due to an explosion of search space. In this paper, we present <i>BMPipe</i>, a bubble--memory co-optimization planner that holistically optimizes computation imbalance, memory under utilization, redundant computation, and scheduling-induced idle time. At its core, <i>BMPipe</i> uses symbolic representations that unify computation, memory and bubbles into a single model that is solved by using an ILP-based planner. Using <i>BMPipe</i>, we perform a thorough experimental evaluation where we train several large, state-of-the-art DNN models on a 16K-NPU cluster. We show that <i>BMPipe</i> achieves up to 1.36$x speedup compared to the state-of-the-art solution <i>Megatron</i>. Against automatic planners <i>PipeDream</i>, <i>Merak</i> and <i>AdaPipe</i>, it yields as <b>1.27</b>x speed‑up. In addition, <i>BMPipe</i> boosts peak device‑memory utilization by <b>1.42</b>x compared with <i>Megatron</i>.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                          <span class="ttip_object_info_very_wide" rel="#s12ta2_info" title="#s12ta2_title">
			    Deadline-Aware Resource Allocation and Scheduling of Serverless Workloads on Heterogeneous Clusters
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s12ta2_title">
				      Deadline-Aware Resource Allocation and Scheduling of Serverless Workloads on Heterogeneous Clusters
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s12ta2_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Serverless computing has become widely adopted as a cloud deployment model due to its ease of use and fine-grained pay-as-you-go pricing. By hiding infrastructure complexity, it simplifies access to cloud resources and lets developers focus on application code. However, most serverless platforms operate on a best-effort basis and provide limited mechanisms for performance tuning. Combined with limited visibility into the underlying hardware, reliably achieving Service Level Objectives (SLOs) becomes increasingly difficult. To address this, we introduce DHRT, a deadline- and heterogeneity-aware scheduling and resource allocation framework for performance-critical serverless workloads. Using heuristic-driven online optimisation, DHRT iteratively refines resource estimates by leveraging real-time metrics and historical data from live executions, accounting for workload characteristics and node heterogeneity. Evaluations on synthetic workloads compare it against baseline allocation and scheduling policies commonly used in FaaS platforms. Results show that DHRT accurately estimates resource demands within a few live executions, eliminating the need for manual resource tuning. By exploiting node heterogeneity and scaling vCPU allocations as deadlines approach, it improves resource efficiency and significantly reduces deadline violations.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                            <span class="ttip_object_info_very_wide" rel="#s12ta3_info" title="#s12ta3_title">
			      Accelerating Key-Value Data Structures Using AVX-512 SIMD Extensions
                            </span>
                            <div class="info_link_object_info" style="display: none;">
                                <div>
                                    <b id="s12ta3_title">
				      Accelerating Key-Value Data Structures Using AVX-512 SIMD Extensions
                                    </b>
                                    <hr/>
                                    <div>
                                        <div id="s12ta3_info">
                                            <div class="author-names">

                                            </div>
                                            <div>
                                                <div>Abstract
                                                    <blockquote>
                                                        <p>
Advanced Vector Extensions 512 (AVX-512), a modern SIMD instruction set for x86 CPUs, enables fine-grained data-level parallelism through 512-bit ZMM registers. In this work, we present a SIMD-accelerated key-value datastore that leverages AVX-512 instructions to deliver scalable, high-throughput performance. Our memory layout partitions the allocated key space into two disjoint regions, using three hash functions to determine candidate slots for each key. Experimental results demonstrate that this layout achieves the lowest insertion failure rate among other alternative memory partitioning strategies. Our implementation achieves insertion performance within 6% of Intel TBBs multithreaded hash map, while substantially reducing synchronization overhead compared to STL, Boost, Robin-Hood, and Abseil. At 550 million entries and a 90% miss rate, our AVX-512-optimized design delivers a 4.0-5.1x speedup over all evaluated alternatives and a 2.0--2.5x improvement over Intel TBB and Abseil, with consistent performance gains across both 32-bit and 64-bit floating-point data types. These results highlight the effectiveness of SIMD-based parallelism in building scalable and efficient key-value stores, offering a cost-effective alternative to thread-level parallelism and multi-core scaling. Our findings suggest that enhancing SIMD capabilities in future CPU architectures can significantly improve performance and efficiency in data-intensive workloads.
                                                        </p>
                                                    </blockquote>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">15:30-15:45</h3>
                <div class="program-session">
                    <div class="session-title">Break</div>
                </div>
                <div style="clear: both;"></div>
            </div>
            <div class="timeslot"><h3 class="session-time">15:45-16:15</h3>
                <div class="program-session">
                    <div class="session-title">Awards and Closing Session</div>
                    <div class="room-name">Pentland</div>
                    <div class="session-chair">Chair: </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                        <span>Best Poster Awards 
                        </span>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                        <span>Best Paper Awards 
                        </span>
                        </div>
                    </div>
                    <div class="slot-title">
                        <div class="slot-wrapper">
                        <span>Closing Remarks 
                        </span>
                        </div>
                    </div>
                </div>
                <div style="clear: both;"></div>
            </div>
       </td>

    <tr>
        <td></td>
        <td colspan="2" align="center">
            <h2 style="text-align: center">Conference Room Layout</h2>
            <img src="program/includes/images/floorplan.png" style="text-align: center" alt="Conference centre layout"/></td>
    </tr>

</table>

</div>
</body>
</html>
