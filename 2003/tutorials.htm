<HTML>
<head>
<title>Cluster 2003</title>
<script language="JavaScript">
<!--
function MM_swapImgRestore() { //v3.0

  var i,x,a=document.MM_sr; for(i=0;a&&i<a.length&&(x=a[i])&&x.oSrc;i++) x.src=x.oSrc;

}



function MM_preloadImages() { //v3.0

  var d=document; if(d.images){ if(!d.MM_p) d.MM_p=new Array();

    var i,j=d.MM_p.length,a=MM_preloadImages.arguments; for(i=0; i<a.length; i++)

    if (a[i].indexOf("#")!=0){ d.MM_p[j]=new Image; d.MM_p[j++].src=a[i];}}

}



function MM_findObj(n, d) { //v4.01

  var p,i,x;  if(!d) d=document; if((p=n.indexOf("?"))>0&&parent.frames.length) {

    d=parent.frames[n.substring(p+1)].document; n=n.substring(0,p);}

  if(!(x=d[n])&&d.all) x=d.all[n]; for (i=0;!x&&i<d.forms.length;i++) x=d.forms[i][n];

  for(i=0;!x&&d.layers&&i<d.layers.length;i++) x=MM_findObj(n,d.layers[i].document);

  if(!x && d.getElementById) x=d.getElementById(n); return x;

}

function MM_swapImage() { //v3.0
  var i,j=0,x,a=MM_swapImage.arguments; document.MM_sr=new Array; for(i=0;i<(a.length-2);i+=3)
   if ((x=MM_findObj(a[i]))!=null){document.MM_sr[j++]=x; if(!x.oSrc) x.oSrc=x.src; x.src=a[i+2];}
}
//-->
</script>
<link rel="stylesheet" href="style.css" type="text/css">
<TITLE></TITLE>
</head>
<BODY BGCOLOR="black" leftmargin='0' topmargin='0' marginwidth='0' marginheight='0' background="images/mainbg.gif" onLoad="MM_preloadImages('images/menu_cluster_over.gif','images/menu_paper_over.gif','images/menu_submission_over.gif','images/menu_hotel_over.gif','images/menu_hk_over.gif','images/menu_sponsors_over.gif','images/menu_chairs_over.gif','images/tutorials_over.gif','images/exhibits_over.gif','images/related_over.gif','images/program_over.gif','images/grid_demo_over.gif','images/menu_tour_over.gif','images/menu_registration_over.gif','images/menu_photo_over.gif')">
<center>
</center>
<TABLE WIDTH=777 BORDER=0 CELLPADDING=0 CELLSPACING=0 align="center">
  <TR> 
    <TD> <IMG src="images/spacer.gif" WIDTH=241 HEIGHT=1></TD>
    <TD> <IMG src="images/spacer.gif" WIDTH=18 HEIGHT=1></TD>
    <TD> <IMG src="images/spacer.gif" WIDTH=54 HEIGHT=1></TD>
    <TD> <IMG src="images/spacer.gif" WIDTH=69 HEIGHT=1></TD>
    <TD> <IMG src="images/spacer.gif" WIDTH=75 HEIGHT=1></TD>
    <TD> <IMG src="images/spacer.gif" WIDTH=60 HEIGHT=1></TD>
    <TD> <IMG src="images/spacer.gif" WIDTH=16 HEIGHT=1></TD>
    <TD> <IMG src="images/spacer.gif" WIDTH=43 HEIGHT=1></TD>
    <TD> <IMG src="images/spacer.gif" WIDTH=69 HEIGHT=1></TD>
    <TD> <IMG src="images/spacer.gif" WIDTH=49 HEIGHT=1></TD>
    <TD> <IMG src="images/spacer.gif" WIDTH=83 HEIGHT=1></TD>
  </TR>
  <TR> 
    <TD COLSPAN=11 width="777" height="83"> 
      <table width=777 cellpadding=0 cellspacing=0 height="83" border="0" bgcolor="#8F8F8F">
        <tr valign="top" align="center" bgcolor="8F8F8F"> 
          <td colspan="2" width="777" height="50" ><img src="images/bg.gif" width="777" height="47"></td>
        </tr>
        <tr> 
          <td colspan="2" width="777" height="19"><img src="images/main_03.gif" width="777" height=19 border="0" usemap="#topbar"></td>
        </tr>
        <tr> 
          <td colspan="2" width="777" height="10"> <img src="images/main_12.gif" width=777 height=10></td>
        </tr>
      </table>
    </TD>
  </TR>
  <TR valign="middle" align="center"> 
    <TD COLSPAN=11 width="777" height="80"><img src="images/smallhk.jpg" width="777" height="80"><img src="images/main_12.gif" width="777" height="10"></TD>
  </TR>
  <TR> 
    <TD COLSPAN=11 width="777" height="1" bgcolor="black"> 
      <p><font size="1"><img src="images/main_12-1.gif" width="777" height="1" border="0"></font></p>
    </TD>
  </TR>
  <TR> 
    <TD COLSPAN=11 width="777" height="21" bgcolor="black"> 
      <p><img src="images/bar_up.gif" width="777" height="21"></p>
    </TD>
  </TR>
  <TR> 
    <TD COLSPAN=11 width="777" height="162" bgcolor="white" valign="top" > 
      <table cellpadding="0" cellspacing="0" border="0" width="777">
        <tr> 
          <td width="128" valign="top" background="images/back.gif"> 
            <table cellpadding="0" cellspacing="0" width="128" border="0">
              <tr>
                <td width="128" height="25">
                  <p><a href="paper.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Call For Paper','','images/menu_paper_over.gif',1)"><img name="Call For Paper" border="0" src="images/menu_paper.gif" width="128" height="25"></a></p>
                </td>
              </tr>
              <tr>
                <td width="128" height="25">
                  <p><a href="committees.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Chairs/Committees','','images/menu_chairs_over.gif',1)"><img name="Chairs/Committees" border="0" src="images/menu_chairs.gif" width="128" height="25"></a></p>
                </td>
              </tr>
              <tr>
                <td width="128" height="25"><a href="program.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Program','','images/program_over.gif',1)"><img name="Program" border="0" src="images/program.gif" width="128" height="25"></a></td>
              </tr>
              <tr>
                <td width="128" height="25"><a href="photos" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Photo Gallery','','images/menu_photo_over.gif',1)"><img name="Photo Gallery" border="0" src="images/menu_photo.gif" width="128" height="25"></a></td>
              </tr>
              <tr>
                <td width="128" height="25"><a href="tutorials.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Tutorials','','images/tutorials_over.gif',1)"><img name="Tutorials" border="0" src="images/tutorials.gif" width="128" height="25"></a></td>
              </tr>
              <tr>
                <td width="128" height="25"><a href="grid_demo.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Grid_Demo','','images/grid_demo_over.gif',1)"><img name="Grid_Demo" border="0" src="images/grid_demo.gif" width="128" height="25"></a></td>
              </tr>
              <tr>
                <td width="128" height="25"><a href="registration.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Registration','','images/menu_registration_over.gif',1)"><img name="Registration" border="0" src="images/menu_registration.gif" width="128" height="25"></a></td>
              </tr>
              <tr>
                <td width="128" height="25"><a href="exhibits.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Exhibits','','images/exhibits_over.gif',1)"><img name="Exhibits" border="0" src="images/exhibits.gif" width="128" height="25"></a></td>
              </tr>
              <tr>
                <td width="128" height="25"><a href="hotel.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Hotel Information','','images/menu_hotel_over.gif',1)"><img name="Hotel Information" border="0" src="images/menu_hotel.gif" width="128" height="25"></a></td>
              </tr>
              <tr>
                <td width="128" height="25"><a href="hk.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('About Hong Kong','','images/menu_hk_over.gif',1)"><img name="About Hong Kong" border="0" src="images/menu_hk.gif" width="128" height="25"></a></td>
              </tr>
              <tr>
                <td width="128" height="25"><a href="food&tour.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Tour & Food','','images/menu_tour_over.gif',1)"><img name="Tour & Food" border="0" src="images/menu_tour.gif" width="128" height="25"></a></td>
              </tr>
              <tr>
                <td width="128" height="25"><a href="hkcluster.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Cluster Computing','','images/menu_cluster_over.gif',1)"><img name="Cluster Computing" border="0" src="images/menu_cluster.gif" width="128" height="25"></a></td>
              </tr>
              <tr>
                <td width="128" height="25">
                  <p><a href="sponsors.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Sponsors','','images/menu_sponsors_over.gif',1)"><img name="Sponsors" border="0" src="images/menu_sponsors.gif" width="128" height="25"></a></p>
                </td>
              </tr>
              <tr>
                <td width="128">
                  <p><a href="related.htm" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('Related Events','','images/related_over.gif',1)"><img name="Related Events" border="0" src="images/related.gif" width="128" height="25"></a></p>
                  <br>
                </td>
              </tr>
            </table></td>
          <td width="649" border="0" valign="top"> 
            <table width="649" border="0" cellspacing="0" cellpadding="10" align="center" height="45">
              <tr> 
                <td background="images/contents_bg.gif" class="title">Tutorials
                (Monday, 1 December)</td>
              </tr>
            </table>
            <table width="649" border=0 cellspacing=0 cellpadding=15>
              <tr> 
                <td><h3 align="justify" class="contents">Morning Tutorials (8:30am - 12:00pm)</h3>
                  <p align="justify" class="contents">Tutorial 1 - <a href="#infiniband">Designing Next Generation
                      Clusters with Infiniband: Opportunities and Challenges</a>,
                      D. Panda (Ohio State University).</p>
                  <p align="justify" class="contents">Tutorial 2 - <a href="#mpi">Using MPI-2: Advanced Features
                      of the Message Passing Interface</a>, W. Gropp, E. Lusk,
                      R. Ross and R. Thakur (Argonne National Lab).</p>
                  <h4 align="justify" class="contents">Afternoon Tutorials (1:30pm - 5:00pm)</h4>
                  <p align="justify" class="contents">Tutorial 3 - <a href="#grid">The Gridbus Toolkit for Grid
                      and Utility Computing</a>, R. Buyya (University of Melbourne). </p>
                  <p align="justify" class="contents">Tutorial 4 - <a href="#npaci">Building and Managing Clusters
                      with NPACI Rocks</a>, G. Bruno, M. Katz, P. Papadopoulos, and 
                      F. Sacerdoti, (NPACI Rocks group at San Diego Supercomputer
                      Center), L. Liew and N. Ninaba (Singapore Computer Systems).</p>
                  <p align="justify">&nbsp;</p>
                  <h3 align="justify" class="title">Short Descriptions</h3>
                  <p align="justify" class="subheading"><a name="infiniband"></a>&quot;Designing
                      Next Generation Clusters with Infiniband: Opportunities
                    and Challenges&quot;</p>
                  <p align="justify" class="contents">The emerging InfiniBand Architecture (IBA) standard is generating
                    a lot of excitement toward building next generation high
                    performance computing systems in a radical different manner.
                    This is leading to the following common questions among many
                    scientists, engineers, managers, developers, and users associated
                    with Cluster Computing:</p>
                  <p align="justify" class="contents">1) What is InfiniBand Architecture?</p>
                  <p align="justify" class="contents">2) How is it different from other on-going developments
                    and standardization effort such as Virtual Interface Architecture
                    (VIA), PCI-X, Gigabit Ethernet, Rapid I/O, Hyper-transport,
                    3GIO/PCI-Express, TCP off-load engines, etc.?,</p>
                  <p align="justify" class="contents">3) How does it perform compared to other proprietary cluster
                    interconnects (Myrinet and Quadrics)?</p>
                  <p align="justify" class="contents">4) What unique features and benefits does IBA bring to designing
                    next generation cluster computing systems?</p>
                  <p align="justify" class="contents">5) How to exploit novel features of InfiniBand to build
                    clusters for high performance computing as well as cluster-based
                    servers and datacenters?</p>
                  <p align="justify" class="contents">This tutorial is designed to provide answers to the above
                    questions. We will start with the background behind the origin
                    of the IBA standard. Then we will make the attendees familiar
                    with the novel features of IBA (uniform treatment of interprocessor
                    communication and I/O; provision for multiple transport services
                    and mechanisms to support QoS and protection in the network;
                    hardware support for remote DMA, atomic, and multicast operations;
                    support for virtual lanes and service levels; and support
                    for low latency communication with Virtual Interface). We
                    will compare and contrast the IBA standard with other on-going
                    developments/standards. We will show how the IBA standard
                    facilitates the next generation computing systems to be designed
                    not only to deliver high performance but also RAS (Reliability,
                    Availability, and Serviceability). Open research challenges
                    in designing communication and I/O subsystems of next generation
                    clusters with IBA will be outlined. Challenges in designing
                    clusters for standard high-performance computing (with different
                    programming models such as MPI, DSM, Get/Put) as well as
                    cluster-based servers/file systems and data centers will
                    be outlined. Performance numbers obtained on clusters with
                    latest InfiniBand products and their comparisons with other
                    proprietary interconnects (Myrinet and Quadrics) will be
                    presented. The tutorial will conclude with an overview of
                    on-going IBA related research projects, IBA products, and
                    the market time frame for the IBA products.</p>
                  <p align="justify" class="contents"><i>Speaker Bio:</i></p>
                  <p align="justify" class="contents"><b><i>Dhabaleswar K. Panda</i></b> is a Professor of Computer
                    Science at the Ohio State University. He obtained his Ph.D.
                    in computer engineering from the University of Southern California.
                    His research interests include parallel computer architecture,
                    high performance computing, user-level communication protocols,
                    interprocessor communication and synchronization, network-based
                    computing, and Quality of Service. He has published over
                    110 papers in major journals and international conferences
                    related to these research areas. Dr. Panda and his research
                    group members have been doing extensive research on InfiniBand.
                    His research group is currently collaborating with Sandia
                    National Laboratory, IBM T.J. Watson, and leading InfiniBand
                    companies (Mellanox and InfiniSwitch) on designing various
                    subsystems of next generation High Performance Computing
                    systems with InfiniBand. The MVAPICH (MPI over VAPI for IBA)
                    package developed by his research group (http://nowlab.cis.ohio-state.edu/projects/mpi-iba/)
                    is being used by many organizations world-wide to extract
                    the potential of IBA-based clusters for HPC applications.</p>
                  <p align="justify" class="contents">Dr. Panda has served on Program Committees
                      and Organizing Committees of several parallel processing
                      and high performance
                    computing conferences and on editorial boards for several
                    parallel processing journals. He was General Co-Chair for
                    the 2001 International Conference on Parallel Processing;
                    Program Co-Chair of the 1999 International Conference on
                    Parallel Processing, 1997 and 1998 Workshops on Communication
                    and Architectural Support for Network-Based Parallel Computing
                    (CANPC); Program Co-Chair of the Int'l Workshop on Communication
                    Architecture for Clusters (CAC '01 and CAC '02); an Associate
                    Editor of the IEEE Transactions on Parallel and Distributed
                    Computing; Co-Guest-Editor for two special issue volumes
                    of Journal of Parallel and Distributed Computing on &quot;Workstation
                    Clusters and Network-based Computing&quot;' an IEEE Distinguished
                    Visitor Speaker and an IEEE Chapters Tutorials Program Speaker.
                    Currently, he is serving as a Program Co-Chair of International
                    Workshop on Communication Architecture for Clusters (CAC
                    '03). Dr. Panda is a recipient of the NSF Faculty Early CAREER
                    Development Award, the Lumley Research Award (1997 and 2001)
                    at the Ohio State University, and an Ameritech Faculty Fellow
                    Award. Dr. Panda is listed as a distinguished scientist in &quot;Who'sWho
                    in America&quot; and in &quot;American Men &amp; Women of
                    Science&quot;.</p>
                  <hr align="JUSTIFY">
                  <p align="justify" class="contents"><a name="mpi"></a><span class="subheading">&quot;Using
                    MPI-2: Advanced Features of the Message Passing Interface&quot;</span></p>
                  <p align="justify" class="contents">This tutorial is about how to use MPI-2, the collection
                    of advanced features that were added to MPI (Message-Passing
                    Interface) by the second MPI Forum. These features include
                    parallel I/O, onesided communication, dynamic-process management,
                    language interoperability, and some miscellaneous features.
                    Implementations of MPI-2 (or significant subsets thereof)
                    are now available both from vendors and from open-source
                    projects. For example, the one-sided communication functions
                    of MPI-2 are being used successfully in applications running
                    on the Earth Simulator. In other words, MPI-2 can now really
                    be used in practice.</p>
                  <p align="justify" class="contents">This tutorial explains how to use MPI-2, particularly, how
                    to use it in a way that results in high performance. We present
                    each feature of MPI-2 in the form of a series of examples
                    (in C, Fortran, and C++), starting with simple programs and
                    moving on to more complex ones. We also discuss how to combine
                    MPI with OpenMP. We assume that attendees are familiar with
                    the basic message-passing concepts of MPI-1.</p>
                  <p align="justify" class="contents">The tutorial will feature a hands-on session in which attendees
                    will be able to run MPI-2 programs on their own laptops with
                    the latest version of MPICH2, which we will distribute on
                    CDs.</p>
                  <p align="justify" class="contents"><i>Speakers Bio:</i></p>
                  <p align="justify" class="contents"><b><i>William Gropp</i></b> is a senior computer scientist
                    and associate division director in the Mathematics and Computer
                    Science Division at Argonne National Laboratory. His research
                    interests are in adaptive methods for PDEs, software for
                    scientific computing, and parallel computing. He was a member
                    of the MPI Forum from the beginning and was one of the chapter
                    authors in the MPI-2 standardization process. He is one of
                    the designers of MPICH and is a co-author of the books Using
                    MPI, Using MPI-2, and MPI - The Complete Reference: Volume
                    2, the MPI-2 Extensions. </p>
                  <p align="justify" class="contents"><b><i>Ewing Lusk</i></b> is a senior computer scientist
                    in the Mathematics and Computer Science Division at Argonne
                    National Laboratory. His research interests are in portable
                    parallel-programming libraries, performance visualization,
                    and automated theorem proving. He was a member of the MPI
                    Forum from the beginning and played a leading role in the
                    MPI-2 standardization process. He is one of the designers
                    of MPICH and is a co-author of the books Using MPI, Using
                    MPI-2, and MPI - The Complete Reference: Volume 2, the MPI-2
                    Extensions. </p>
                  <p align="justify" class="contents"><b><i>Rob Ross</i></b> is an assistant computer scientist
                    in the Mathematics and Computer Science Division at Argonne
                    National Laboratory. He received a Ph.D. in Computer Engineering
                    from Clemson University in 2000. His research interests are
                    in the area of high-performance computing, particularly cluster
                    computing and high-performance I/O. He is the primary author
                    of PVFS, a high-performance parallel file system for clusters,
                    and is currently involved in the implementation of the next-generation
                    MPICH. </p>
                  <p align="justify" class="contents"><b><i>Rajeev Thakur</i></b> is a computer scientist in the
                    Mathematics and Computer Science Division at Argonne National
                    Laboratory. He received a Ph.D. in Computer Engineering from
                    Syracuse University in 1995. His research interests are in
                    the area of high-performance computing in general and high-performance
                    networking and I/O in particular. He was a member of the
                    MPI Forum and participated actively in the definition of
                    the I/O part of the MPI-2 standard. He is the author of ROMIO,
                    a high-performance, portable implementation of MPI I/O. He
                    is also a co-author of the book Using MPI-2 together with
                    Bill Gropp and Rusty Lusk.</p>
                  <hr align="JUSTIFY">
                  <p align="justify" class="subheading"><a name="grid"></a>&quot;The Gridbus
                    Toolkit for Grid and Utility Computing&quot;</p>
                  <p align="justify" class="contents">Computational Grids enable the sharing, selection, and aggregation
                    of geographically distributed resources (such as computers,
                    data bases, scientific instruments) for solving large-scale
                    problems in science, engineering, and commerce. However,
                    application development, resource management, scheduling,
                    and supporting end-to-end quality-of-services (QoS) in these
                    environments is a complex undertaking. This is due to the
                    geographic distribution of resources that are owned by different
                    organizations having different usage policies and cost models,
                    and varying loads and availability patterns. To address these
                    challenges, we have developed distributed computational economy
                    framework for resource allocation and regulation of supply-and-demand
                    for resources. We applied this framework in the design and
                    development of scheduling systems that manage distributed
                    resources in a single administrative domain (cluster computing)
                    and also in multiple administrative domains (grid computing).</p>
                  <p align="justify" class="contents">The Gridbus Project is engaged in the design and development
                    of cluster and grid middleware technologies for service-oriented
                    computing. They include visual Grid application development
                    tools for rapid creation of distributed applications, competitive
                    economy-based Grid scheduler, cooperative economy-based cluster
                    scheduler, Web-services based Grid market directory (GMD),
                    Grid accounting services, and a widely used GridSim toolkit.
                    These tools have been used in Grid-enabling applications
                    such as molecular docking and neuroscience and deploying
                    them for distributed proceedings on Global Grids.</p>
                  <p align="justify" class="contents">This tutorial covers four topics. First, we briefly review
                    emerging trends in network-based high performance computing
                    and identify application development and resource management
                    challenges. Then, we introduce our framework on Grid Architecture
                    for Computational Economies (GRACE) that leverages existing
                    technologies such as Globus and provides new services that
                    are essential for constructing industrial-strength Grids.
                    We discuss Gridbus technologies and their use in Grid enabling
                    application, the use of our economic grid infrastructure
                    in scheduling parametric computations containing hundreds
                    of jobs for execution on the World Wide Grid (WWG) testbed.
                    Particular emphasis will be placed on Grid economy, how to
                    design and develop Grid technologies and applications capable
                    of dynamically leasing services of distributed resources
                    at runtime depending on their availability, capability, performance,
                    cost, and users' quality of service requirements. Finally,
                    we present the usage of tools in composition and distributed
                    execution of data-intensive applications (e.g., molecular
                    docking, brain activity analysis, and high-energy physics)
                    on the Grid to demonstrate capabilities of Gridbus system.</p>
                  <p align="justify" class="contents"><i>Speaker Bio:</i></p>
                  <p align="justify" class="contents"><b><i>Rajkumar Buyya</i></b> is the founder and program
                    leader of the Grid Computing and Distributed Systems (GRIDS)
                    Laboratory in the Dept. of Computer Science and Software
                    Engineering at the University of Melbourne, Australia. He
                    is one of the creators of system software for PARAM Supercomputers
                    developed by the Centre for Development of Advanced Computing
                    (C-DAC), India. He has authored three books Microprocessor
                    x86 Programming, BPB Press, New Delhi, 1995, Mastering C++,
                    Tata McGraw Hill Press, New Delhi, 1997, and Design of PARAS
                    Microkernel. The books on emerging topics that he edited
                    include, High Performance Cluster Computing published by
                    Prentice Hall, USA, 1999; and High Performance Mass Storage
                    and Parallel I/O, IEEE and Wiley Press, USA, 2001. He has
                    published over 70 research articles in international conferences
                    and journals. For further information, please visit: http://www.buyya.com/</p>
                  <hr align="JUSTIFY">
                  <p align="justify" class="subheading"><a name="npaci"></a>&quot;Building
                    and Managing Clusters with NPACI Rocks&quot;</p>
                  <p align="justify" class="contents">NPACI Rocks is an open source clustering
                      distribution for heterogeneous x86 and IA64 HPC Linux Clusters.
                      The first
                    version of Rocks was released in November of 2000, and has
                    averaged 3-4 releases each year since. NPACI Rocks is unique
                    in that it is a complete “cluster on a CD” software distribution,
                    everything from the base Red Hat OS, to <i>de facto</i> standard
                    job schedulers and monitoring systems are included. Rocks
                    was designed with the goal of making clusters easy and accessible
                    to domain application scientists. To achieve this goal, Rocks
                    automates the common and most time consuming tasks of cluster
                    administration and deployment. NPACI Rocks software and documentation
                    is available at <a href="http://www.rocksclusters.org">www.rocksclusters.org</a>.</p>
                  <p align="justify" class="contents">This tutorial assumes minimal experience with cluster administration
                    and use. Experience with common clustering toolkits is useful
                    but not required. We will cover the basic design and philosophy
                    of Rocks and the procedures to customize Rocks for unique
                    sites and non-HPC clustering roles. A hands-on lab will guide
                    participants through building their own cluster. Toward the
                    end of the lab, the individual clusters will be bound together
                    as a Grid where each participant will launch Grid-wide jobs.</p>
                  <p align="justify" class="contents"><i>Speakers Bio:</i></p>
                  <p align="justify" class="contents"><b><i>Greg Bruno</i></b> is a Programmer
                      Analyst IV for the Cluster Development Group at the San
                      Diego Supercomputer
                    Center (SDSC). Mr. Bruno received his MS from UCSD in Computer
                    Science and he is currently enrolled in the PhD program.
                    For 10 years, Mr. Bruno worked with Teradata Systems developing
                    cluster management software for the systems that supported
                    the world’s largest databases. Recently, he has spent the
                    last 3 years helping to architect, design and implement the
                    Rocks Cluster Distribution, a freely available software stack
                    that enables domain-specific scientists to build and manage
                    their own clusters.</p>
                  <p align="justify" class="contents"><b><i>Mason Katz</i></b> is Group Leader for Cluster Development
                    at the San Diego Supercomputer Center (SDSC). Mr. Katz received
                    his BS in Systems Engineering from the University of Arizona.
                    He worked for 5 years as an embedded software engineer on
                    networks of lightning detection sensors. Recently, he has
                    spent the last 6 years working at both the University of
                    Arizona, and UCSD/SDSC on projects ranging from network security
                    protocols, operating systems (x-kernel, Scout), and commodity
                    clustering (HPVM, NPACI Rocks).</p>
                  <p align="justify" class="contents"><b><i>Philip M. Papadopoulos</i></b> is the Program Director
                    for Grid and Cluster Computing at the San Diego Supercomputer
                    Center (SDSC). Dr. Papadopoulos received his BA in Applied
                    Mathematics from UCSD, MS in Mechanical Engineering from
                    UC Berkeley, and PhD from UC Santa Barbara in Electrical
                    and Computer Engineering. He worked for over 5 years at Oak
                    Ridge National Laboratory as part of the PVM development
                    team before moving the Computer Science Department at UCSD.
                    In 1999, Dr. Papadopoulos joined SDSC as a group leader for
                    cluster development and started the NPACI Rocks Clustering
                    project. He has authored more than 25 peer-reviewed papers
                    on cluster and distributed computing, has served on the organizing
                    committees for several years on the SC conference series
                    and has given numerous invited talks on cluster and distributed
                    computing. Dr. Papadopoulos will be the technical program
                    chair of the IEEE Clusters 2004 conference. Dr. Papadopoulos
                    is deeply involved in several grid and cluster computing
                    research projects including OptIPuter, GEON, The National
                    Biomedical Computational Resource (NBCR), and the Biomedical
                    Informatics Research Network (BIRN).</p>
                  <p align="justify" class="contents"><b><i>Federico Secerdoti </i></b>is currently a member of
                    the Cluster Development group, San Diego Supercomputer Center
                    (SDSC), at the University of California. Mr. Sacerdoti received
                    his BS in Computer Engineering fromWashington University
                    in St. Louis, and MS in Compute Science from University of
                    California, San Diego. Mr. Sacerdoti has been involved with
                    the Rocks cluster distribution effort at SDSC for over three
                    years, and has contributed to many HPC projects including
                    the Ganglia monitoring system and the KeLP parallel message-passing
                    libraries. His thesis work was in the area of dynamic cache
                    optimizations for parallel applications.</p>
                    <p align="justify" class="contents"><i><b>Laurence Liew</b></i> graduated from the National University of 
Singapore (NUS) with First Class Honors in Mechanical Engineering. He also holds 
a Masters in Knowledge Engineering from the Institute of Systems Science in NUS. 
Since 1998, Laurence has been actively involved with Linux and Beowulf 
supercomputing. He was involved with the design and deployment of Beowulf 
clusters in Singapore’s foremost R &amp; D and academic institutions, and also the 
deployment of enterprise Linux solutions in major government and commercial 
organizations. Laurence joined Singapore Computer Systems in 2001 and presently 
heads the SCS Linux Competency Centre (LCC) in Singapore, Malaysia and Thailand. 
Under Laurence, the LCC have partnered world-class leaders in Linux and this 
includes Red Hat as a Red Hat Certified Training and Education Centre, Oracle, 
Sendmail, Computer Associates, Myricom, VMware and others. </p>
<p align="justify" class="contents"><i><b>Najib Ninaba</b></i> graduated from Singapore Polytechnic with a 
diploma in Computer Information Systems. Najib has been a Linux user, programmer 
and administrator since 1996. He was introduced to Linux Beowulf supercomputing 
in 2001 and have since been hooked on NPACI Rocks. Najib joined Singapore 
Computer Systems in 2001 and currently leads the NPACI Rocks development in 
Singapore. A co-developer of NPACI Rocks, Najib integrated the Parallel Virtual 
Filesystem (PVFS) and Sun Grid Engine (SGE) into Rocks in 2002 and currently 
maintains the PVFS, SGE, Myrinet and other packages for NPACI Rocks.</p>

                                      <p align="left" class="contents">&nbsp;</p>
                </td>
              </tr>
            </table>
          </td>
        </tr>
      </table>
    </td>
  </tr>
  <TR> 
    <TD COLSPAN=11 width="777" height="22" bgcolor="black"> 
      <p><font size="1"><img src="images/bar_down.gif" width="777" height="22" border="0" usemap="#bottombar"></font></p>
    </TD>
  </TR>
</table>
<p>&nbsp;</p>
<map name="topbar"> 
  <area shape="rect" coords="172,3,767,17" href="http://www.csis.hku.hk" alt="Organized by Department of Computer Science and Information Systems, The University of Hong Kong" title="Organized by Department of Computer Science and Information Systems, The University of Hong Kong">
  <area shape="rect" coords="11,3,48,19" href="index.htm">
</map>
<map name="bottombar"> 
  <area shape="rect" coords="635,4,770,18" href="mailto:clwang@csis.hku.hk">
  <area shape="rect" coords="464,4,621,17" href="mailto:cluster2003@csis.hku.hk">
</map>
</BODY>
</HTML>





