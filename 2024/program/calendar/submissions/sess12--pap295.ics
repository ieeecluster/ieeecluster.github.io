BEGIN:VCALENDAR
BEGIN:VEVENT
SUMMARY:HPH: Hybrid Parallelism on Heterogeneous Clusters for Accelerating
  Large-scale DNNs Training
DTSTART;TZID=Europe/Berlin;VALUE=DATE-TIME:20220909T170000
DTEND;TZID=Europe/Berlin;VALUE=DATE-TIME:20220909T173000
DESCRIPTION:As the deep learning model grows larger\, training model with 
 a single computational resource becomes impractical. To solve this\, hybri
 d parallelism\, which combines data and pipeline parallelism emerges to tr
 ain large models with multiple GPUs. In practice\, using heterogeneous GPU
  clusters to train large models is a common need due to the upgrade of a p
 art of hardware. However\, existing hybrid parallelism approaches in the h
 eterogeneous environment do not work well in communication efficacy\, work
 load balance among GPUs and utilizing the memory constrained GPU. To addre
 ss these problems\, we present a parallel DNN training approach\, Hybrid P
 arallelism on Heterogeneous clusters (HPH). In HPH\, we propose a topology
  designer that minimizes the communication time cost. Furthermore\, HPH us
 es a partition algorithm that automatically partitions DNN layers among wo
 rkers to maximize throughput. Besides\, HPH adopts recomputation-aware sch
 eduling to reduce memory consumption and further reschedule the pipeline t
 o eliminate the extra time overhead of recomputation. Our experimental res
 ults on a 32-GPU heterogeneous cluster show that HPH achieves up to 141% t
 raining speed-up compared with the state-of-the-art approach.
END:VEVENT
END:VCALENDAR
