BEGIN:VCALENDAR
BEGIN:VEVENT
SUMMARY:Spark Meets MPI: Towards High-Performance Communication Framework 
 for Spark using MPI
DTSTART;TZID=Europe/Berlin;VALUE=DATE-TIME:20220907T143000
DTEND;TZID=Europe/Berlin;VALUE=DATE-TIME:20220907T150000
DESCRIPTION:There are several popular Big Data processing frameworks inclu
 ding Apache Spark\, Dask\, and Ray. The Apache Spark software provides an 
 easy-to-use programming API in different languages including Scala\, Java\
 , and Python. Spark supports parallel and distributed execution of user wo
 rkloads by supporting communication using an event-driven framework called
  Netty. In this context\, efforts --- including Spark-RDMA and SparkUCX --
 - were made in the past to optimize Apache Spark on High-Performance Compu
 ting (HPC) systems equipped with low-latency and high-performance intercon
 nects like InfiniBand. In the HPC community\, Message Passing Interface (M
 PI) libraries are widely adopted for parallelizing science and engineering
  applications. This paper designs and implements MPI4Spark which uses MPI 
 for communication in a parallel and distributed setting on HPC systems. Th
 is approach realizes the vision of "Converged Communication Stack" for Big
  Data\, Deep Learning\, and HPC workloads. Also\, it provides portability 
 and performance benefits since MPI4Spark is capable of utilizing popular H
 PC interconnects including InfiniBand\, Omni-Path\, Slingshot\, and others
 . MPI4Spark relies on several features offered by MPI including point-to-p
 oint communication primitives\, intra-communicators\, inter-communicators\
 , and non-blocking probe functions. A unique feature of MPI4Spark is that 
 it utilizes Dynamic Process Management (DPM) for maintaining the distribut
 ed execution model of Spark ecosystem. The performance of MPI4Spark is eva
 luated against Spark-RDMA and Vanilla Spark using OSU HiBD Benchmarks (OHB
 ) and Intel HiBench suite that contains a variety of Resilient Distributed
  Dataset (RDD)\, Graph Processing\, and Machine Learning workloads. This e
 valuation is done on three HPC systems including TACC Frontera\, TACC Stam
 pede2\, and an internal cluster.
END:VEVENT
END:VCALENDAR
