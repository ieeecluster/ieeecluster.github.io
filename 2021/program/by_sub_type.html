<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd"><html><head><meta charset="windows-1252" /><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><meta name="viewport" content="width=device-width, initial-scale=1"><title>IEEE Cluster 2018 Program</title><link href="includes/css/jquery-ui.css" rel="stylesheet" type="text/css" /><link href="includes/css/shared_styles.css" rel="stylesheet" type="text/css" /><link href="includes/css/block_styles.css?v=1" rel="stylesheet" type="text/css" /><link href="includes/css/jquery.qtip.min.css" rel="stylesheet" type="text/css" /><link href="includes/css/font-awesome-4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css" /><link href="includes/css/user_generated.css" rel="stylesheet" type="text/css" /><link href="archive_styles.css" rel="stylesheet" type="text/css" /><style>

        .tabs {
            background: #e8ca9b;
        }
        .tabs .divider,
        .tabs .bg_tab {
            background-color: #e8ca9b;
            border-top-color: #e8ca9b;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .tabs .fg_tab {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            color: #CC1A1A;
            text-transform: none;
        }
        .tab_menu_label, .tab_no_menu_label {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .qtip.qtip-rm-tab-menu {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        .filter_bar,
        .filter_bar_w_legend {
            background-color: #C9DDF9;
        }
        .filter_bar_w_legend .instr,
        .filter_bar .instr {
            background-color: #A2C2FC;
        }
        


        .role_stype_bar {
            background-color: #E0AB76;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        


        #footer {
            background-color: #EAEAEA;
            color: #999999;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        #footer a {
            color: #777777;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        


        div.banner_top,
        div.banner_top .site_title,
        div.banner_top .no_logo_banner_right,
        div.logo_banner,
        div.logo_banner .user_name,
        #header {
            background-color: #0066CC;
            color: #FFFFFF;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        


        a:link,
        a:visited,
        a:active,
        .clickable,
        a.clickable,
        a.clickable:link,
        a.clickable:visited,
        a.clickable:active,
        .ttip_object_info_blue,
        .ttip_object_info_blue_no_clone,
        .ttip_object_info_blue_wide,
        .ttip_object_info_blue_wide_no_clone,
        .ttip_object_info_blue_very_wide,
        .ttip_object_info_blue_very_wide_no_clone,
        .ttip_object_info_blue_extra_wide,
        .ttip_object_info_blue_extra_wide_no_clone,
        .ttip_object_info_blue_modal,
        .ttip_object_info_blue_modal_no_clone,
        .colorbox_object_info,
        span.menu_item_label,
        .page_box_print .contents A,
        .page_box_print #footer a   { color: #0000EE; }

        /* Light Links */
        .light_link a,
        .light_arrow,
        .light_link a:link,
        .light_link a:active,
        .light_link a:visited,
        .light_clickable,
        a.light_clickable,
        a.light_clickable:link,
        a.light_clickable:active,
        a.light_clickable:visited   { color: #5088F0; }

        /* user hovers     */
        a:hover,
        .light_link a:hover,
        .light_arrow:hover,
        .light_clickable:hover,
        a.light_clickable:hover,
        .hover_link:hover,
        .ttip_object_info_blue:hover,
        .ttip_object_info_blue_no_clone:hover,
        .ttip_object_info_blue_wide:hover,
        .ttip_object_info_blue_wide_no_clone:hover,
        .ttip_object_info_blue_very_wide:hover,
        .ttip_object_info_blue_very_wide_no_clone:hover,
        .ttip_object_info_blue_extra_wide:hover,
        .ttip_object_info_blue_extra_wide_no_clone:hover,
        .ttip_object_info_blue_modal:hover,
        .ttip_object_info_blue_modal_no_clone:hover,
        .ttip_object_info:hover,
        .ttip_object_info_no_clone:hover,
        .ttip_object_info_wide:hover,
        .ttip_object_info_wide_no_clone:hover,
        .ttip_object_info_very_wide:hover,
        .ttip_object_info_very_wide_no_clone:hover,
        .ttip_object_info_extra_wide:hover,
        .ttip_object_info_extra_wide_no_clone:hover,
        .ttip_object_info_modal:hover,
        .ttip_object_info_modal_no_clone:hover,
        .colorbox_object_info:hover,
        .subtabs .fg_tab:hover div,
        .subtabs .fg_tab:hover A,
        .subtabs .bg_tab:hover,
        .subtabs .bg_tab:hover A        { color: #0000EE; }

        ul.rm_mega_menu li.mega > div,
        ul.rm_mega_menu > li.mega-link > a:hover,
        .disp_details_header,
        .disp_details_sub_header,
        .disp_details I,    /* This is deprecated, since it clashes with font awesome using I tags. */
        .disp_red,
        .disp_label {
            color: #B32626;
        }


        


        #related_col .block-title {
            background-color: #BBBBBB;
            color: #000000;
        }
        #related_col .block-title a {
            color: #0000FF;
        }
        #related_col .block-content {
            background-color: #E5E5E5;
        }
        #related_col .block-content .instr {
            background-color: #D0D0D0;
        }
        #related_col .block-content .odd {
            background-color: #DEDEDE;
        }
        #related_col .block-content .even {
            background: #D0D0D0;
        }
        


        .contents .output_box .title {
            background-color: #D89655;
        }
        .block-content,
        .block2-content,
        .contents .output_box table tr th,
        .contents .output_box {
            background-color: #F9F4E6;
        }
        .output_box_instr,
        .contents .output_box .instr,
        .block .instr,
        .block-content .instr {
            background-color: #F2E0C5;
        }
        .odd,
        .contents .output_box .odd,
        .block-content .odd {
            background-color: #F6EAD5;
        }
        .even,
        .contents .output_box .even,
        .block-content .even {
            background-color: #F2E0C5;
        }
        


        .tabs .fg_tab {
            background-color: #F0DBBC;
            color: #CC1A1A;
            border-bottom-color: #F0DBBC;
        }
        .tab_menu_label:hover, .active .tab_menu_label,
        .tab_no_menu_label:hover,
        .tab_no_menu_label:hover a {
            color: #CC1A1A;
        }
        .subtabs {
            background-color: #F0DBBC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .subtabs .divider,
        .subtabs .bg_tab,
        .subtabs .bg_tab a {
            background-color: #F0DBBC;
            border-top-color: #F0DBBC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .subtabs .fg_tab {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            color: #CC1A1A;
            text-transform: none;
        }

        /*
        uncomment this to make the subtabs follow the selected tab color instead
        of the link color

        .subtabs .bg_tab:hover a {
            color: #CC1A1A;
        }
        .subtabs .fg_tab:hover a, {
            color: #CC1A1A;
        }
        */

        .subtabs .fg_tab a {
            color: #CC1A1A;
        }
        


        .documentation_box {
            background: #F0E0BC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        body.in_iframe {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .pagedoc {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .page_box {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .page_box_in_iframe {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents_options {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        #top-links {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .fullscreen {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .subtabs .fg_tab {
            border-bottom-color: #FFFFF7;
        }
        .subtabs .fg_tab div {
            background-color: #FFFFF7;
            border-bottom-color: #FFFFF7;
        }
        .fullscreen_schedule {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents input,
        .contents input_box,
        .contents textarea {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .qtip.rm-qtip {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        #cboxContent {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        /* For now, use the main site background color for tool tips. */
        .qtip.qtip-rm,
        .qtip.qtip-rm .qtip-titlebar {
            background-color: #FFFFF7;
        }

        /* Not sure where this should live. */
        #actions_col .block-title-text {
            font-size: 15px;
        }
        #related_col .block-title-text {
            font-size: 15px;
        }

        /* For jquery-ui. */
        .ui-widget {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .arrow-slidedown {
            background-color: #FFFFF7;
            color: #000000;
        }
        


        .contents .input .title,
        .contents .input_box .title {
            background-color: #247bf4;
        }
        .contents .input,
        .contents .input_box,
        .contents .input table tr th,
        .contents .input_box table tr th,
        .form .block-content {
            background-color: #f6f9fe;
        }
        .contents .input .instr,
        .contents .input_box .instr,
        .multi_block_button,
        .form .block .instr {
            background-color: #dbe8fa;
        }
        .contents .input .odd,
        .contents .input_box .odd,
        .form .block-content .odd {
            background-color: #edf3fc;
        }
        .contents .input .even,
        .contents .input_box .even,
        .form .block-content .even {
            background-color: #dbe8fa
        }
        


        div.rm_mega_menus_container,
        ul.rm_mega_menu.darker,
        ul.rm_mega_menu > li.mega > a,
        ul.rm_mega_menu.darker > li.mega > a,
        ul.rm_mega_menu > li.mega-link > a,
        ul.rm_mega_menu.darker > li.mega-link > a,
        ul.rm_mega_menu > li.mega-label > span {
            background-color: #777777;
            border-color: #777777;
            color: #FFFFFF;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        ul.rm_mega_menu > li.mega.selected > a {
            background-color: #777777;
            border-color: #CC1A1A;
            color: #FFFFFF;
        }
        ul.rm_mega_menu > li.mega:hover > a {
            color: #CC1A1A;
        }
        div.rm_mega_menus_container ul.rm_mega_menu > li.mega > div.menu_dropdown {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents input,
        .contents input_box,
        .contents textarea {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        #actions_col .block-title {
            background-color: #244A84;
            color: #FFFFFF;
        }
        #actions_col .block-title a {
            color: #FFFFFF;
        }
        #actions_col .block-content {
            background-color: #BDD2F8;
        }
        #actions_col .block-content .instr {
            background-color: #B0CBFC;
        }
        #actions_col .block-content .odd {
            background-color: #C2D6FB;
        }
        #actions_col .block-content .even {
            background: #B0CBFC;
        }
        </style><script src="includes/jquery/jquery-36.0.min.js" type="text/javascript"></script><script src="includes/jquery/jquery-ui.min.js" type="text/javascript"></script><script src="includes/jquery/jquery.hoverIntent.minified.js" type="text/javascript"></script><script src="includes/jquery/basic.js" type="text/javascript"></script><script src="includes/jquery/jquery.qtip.min.js" type="text/javascript"></script><script src="includes/jquery/jquery.highlight-4.js"></script><script src="includes/jquery/jquery.instaFilter.js"></script><script type="text/javascript">$(function() { setup_info_links(); });</script><script src="includes/jquery/user_generated.js"></script><script type="text/javascript">
    $(document).ready(function(){
        $program_sections = $("#sections-container");
        $("#program_filter").InstaFilter($program_sections, {
            typing_pause: 500,
            search_unit_selector: ".slot-entry",
            search_unit_ancestor_fields_selector: ".session-title, .session-chair, .room-name"
        });

        // this ensures there is enough vertical space below the search field to keep the
        // page from scrolling during a search
        $program_sections.css("min-height", $(window).height());
    });
    </script></head><body><a name="top"></a><div class="centered"><div><div style="float: left;"></div><div style="float: left; margin-top: 15px; height: 80px;"><span class="page-title">IEEE Cluster 2018 Program</span></div><div style="clear: both;"></div></div></div><br /><div class="centered"><br /><span class="page-links"><a href="at_a_glance.html">Overview</a></span> | <span class="page-links"><a href="by_date.html">By Date</a></span> | <span class="page-links">By Event Type</span> | <span class="page-links"><a href="by_room.html">By Room</a></span> | <span class="page-links"><a href="by_auth.html">Author Index</a></span><br /></div><br /><div id="main-content-box"><div class="centered"><table class="cellspacing10px"><tr><td><div class="anchor-link"><a href="#evtt101">Other</a></div></td><td><div class="anchor-link"><a href="#evtt102">Plenary</a></div></td><td></td></tr><tr><td><div class="anchor-link"><a href="#sstype101">Paper</a></div></td><td><div class="anchor-link"><a href="#sstype103">Workshop</a></div></td><td></td></tr></table><br /><hr /></div><div class="righted"><input id="program_filter" name="program_filter" placeholder="search" size="40" type="text" /></div><div class="centered" id="sections-container"><table><tr><td align="left"><div class="etype-section"><div class="centered"><a name="evtt101"></a><div class="section-title">Other</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">10:30am-11:00am</span> <a href="calendar/sessions/sess134.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">12:30pm-1:30pm</span> <a href="calendar/sessions/sess136.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">3:00pm-3:30pm</span> <a href="calendar/sessions/sess138.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">6:15pm-7:30pm</span> <a href="calendar/sessions/sess141.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Drinks Reception & Self-Guided Tours, Naughton Gallery, Queen’s University Belfast</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">10:15am-10:45am</span> <a href="calendar/sessions/sess102.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">12:15pm-1:30pm</span> <a href="calendar/sessions/sess104.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Lunch</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">3:00pm-3:30pm</span> <a href="calendar/sessions/sess130.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">5:15pm-7:00pm</span> <a href="calendar/sessions/sess110.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Poster reception</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">10:15am-10:45am</span> <a href="calendar/sessions/sess112.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">12:15pm-1:30pm</span> <a href="calendar/sessions/sess131.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Lunch</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">3:00pm-3:30pm</span> <a href="calendar/sessions/sess116.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">6:00pm-7:00pm</span> <a href="calendar/sessions/sess142.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Arrival and Tour of Titanic Belfast</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">7:00pm-10:00pm</span> <a href="calendar/sessions/sess143.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Drinks Reception and Banquet at Titanic Belfast</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">10:15am-10:45am</span> <a href="calendar/sessions/sess121.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">12:15pm-1:30pm</span> <a href="calendar/sessions/sess124.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Lunch</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">3:00pm-3:30pm</span> <a href="calendar/sessions/sess127.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="etype-section"><div class="centered"><a name="sstype101"></a><div class="section-title">Paper</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">10:45am-12:15pm</span> <a href="calendar/sessions/sess103.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Plenary, Paper</span><br /><div class="session-title">Session I: best papers (Areas 1 and 2)</div><div class="session-chair">Chair: Harald Koestler (University of Erlangen-Nuremberg)<br /></div><div class="slot-entry"><a name="pap207"></a><div class="slot-title">PaSTRI: Error-Bounded Lossy Compression for Two-Electron Integrals in Quantum Chemistry <a href="calendar/submissions/sess103--pap207.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Ali Murat Gok (Northwestern University); Sheng Di, Yuri Alexeev, and Dingwen Tao (Argonne National Laboratory); Vladimir Mironov (Lomonosov Moscow State University); and Xin Liang and Franck Cappello (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_112_1536846530_05" onclick="$('#vhsjs_view_112_1536846530_05').hide();
                $('#vhsjs_hide_112_1536846530_05').show();
                $('#111_1536846530_05').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_112_1536846530_05" onclick="$('#111_1536846530_05').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_112_1536846530_05').hide();
                $('#vhsjs_view_112_1536846530_05').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="112_1536846530_05" id="111_1536846530_05" style="display: none"><div class="arrow-slidedown"><blockquote>Computation of two-electron repulsion integrals is the critical and the most time-consuming step in a typical quantum chemistry simulation. Such calculations have massive computing and storage requirements, which scale as O(n^4) with the size of a chemical system. Compressing the integral's data and storing it on disk can avoid costly recalculation, significantly speeding  the overall quantum chemistry calculations; but it requires a fast  compression algorithm. To  this end, we developed PaSTRI (Pattern Scaling for Two-electron Repulsion Integrals) and implemented the algorithm in the data compression package SZ. PaSTRI leverages the latent pattern features in the integral dataset and optimizes the calculation of the appropriate number of bits required for the storage of the integral. We have evaluated PaSTRI using the integral datasets generated by the quantum chemistry program GAMESS. The results show an excellent 16.8 compression ratio with low overhead, while maintaining 10^-10 absolute precision based on user's requirement.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap161"></a><div class="slot-title">SALaR: Scalable and Adaptive Designs for Large Message Reduction Collectives <a href="calendar/submissions/sess103--pap161.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Mohammadreza Bayatpour, Jahanzeb Maqbool Hashmi, Sourav Chakraborty, Pouya Kousha, Hari Subramoni, and Dhabaleswar K. Panda (Ohio State University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_114_1536846530_05" onclick="$('#vhsjs_view_114_1536846530_05').hide();
                $('#vhsjs_hide_114_1536846530_05').show();
                $('#113_1536846530_05').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_114_1536846530_05" onclick="$('#113_1536846530_05').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_114_1536846530_05').hide();
                $('#vhsjs_view_114_1536846530_05').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="114_1536846530_05" id="113_1536846530_05" style="display: none"><div class="arrow-slidedown"><blockquote>Message Passing Interface (MPI), thus far, has remained a dominant programming models to program large-scale scientific applications. Collective communication operations in MPI are of significant importance due to their communication intensive nature and use in scientific applications. With the emergence of multi-/many-core systems, and rise of deep learning applications, it is important to revisit MPI collectives, particularly MPI Allreduce to exploit vast parallelism offered by modern architectures. In this paper, we take up this challenge and propose Scalable and Adaptive designs for Large message Reduction collectives (SALaR). We focus on MPI Allreduce due to its use in deep learning frameworks and propose new designs that can significantly improve its performance by exploiting architectural features of modern multi-/many-cores in tandem with high- throughput network such as InfiniBand. We also propose a theoretical model to analyze communication and computation cost and use these insights to guide our designs. The evaluation of the proposed SALaR based designs shows significant performance gains over state-of-the-art designs on a wide variety of micro-benchmarks and applications.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">1:30pm-3:00pm</span> <a href="calendar/sessions/sess105.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session II: Matrix algorithms</div><div class="session-chair">Chair: Dheeraj Sreedhar (IBM Research - India)<br /></div><div class="slot-entry"><a name="pap102"></a><div class="slot-title">Energy Analysis and Optimization for Resilient Linear Systems <a href="calendar/submissions/sess105--pap102.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Zheng Miao, Jon Calhoun, and Rong Ge (Clemson University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_116_1536846530_06" onclick="$('#vhsjs_view_116_1536846530_06').hide();
                $('#vhsjs_hide_116_1536846530_06').show();
                $('#115_1536846530_06').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_116_1536846530_06" onclick="$('#115_1536846530_06').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_116_1536846530_06').hide();
                $('#vhsjs_view_116_1536846530_06').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="116_1536846530_06" id="115_1536846530_06" style="display: none"><div class="arrow-slidedown"><blockquote>Exascale computing must simultaneously address both energy efficiency and
resilience as power limits impact scalability and faults are more common.
Unfortunately, energy efficiency and resilience have been traditionally studied
in isolation and optimizing one typically detrimentally impacts the other. To deliver the promised performance within the given
power budget, exascale computing mandates a deep understanding of the interplay
among energy efficiency, resilience, and scalability.<br><br>In this work, we propose novel methods to analyze and optimize costs of
resilience techniques including checkpoint-restart and forward recovery for large sparse linear system solvers. In
particular, we present experimental and analytical methods to analyze and
quantify the time and energy costs of recovery schemes on computer clusters. We
further develop and prototype performance optimization and power management
strategies to improve energy efficiency.  Experimental results show that
recovery schemes incur different time and energy overheads and optimization
techniques significantly reduce such overheads. This work suggests that
resilience techniques should be adaptively adjusted to a given fault rate, system
size, and power budget.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap173"></a><div class="slot-title">Load-balancing-aware Parallel Algorithms of H-matrices with Adaptive Cross Approximation for GPUs <a href="calendar/submissions/sess105--pap173.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">TETSUYA HOSHINO, AKIHIRO IDA, TOSHIHIRO HANAWA, and KENGO NAKAJIMA (The University of Tokyo)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_118_1536846530_06" onclick="$('#vhsjs_view_118_1536846530_06').hide();
                $('#vhsjs_hide_118_1536846530_06').show();
                $('#117_1536846530_06').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_118_1536846530_06" onclick="$('#117_1536846530_06').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_118_1536846530_06').hide();
                $('#vhsjs_view_118_1536846530_06').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="118_1536846530_06" id="117_1536846530_06" style="display: none"><div class="arrow-slidedown"><blockquote>Hierarchical matrices (H-matrices) are an approximation technique for dense matrices, such as the coefficient matrix of the Boundary Element Method (BEM). An H-matrix is expressed by a set of low-rank approximated and small dense sub-matrices, each of which has various rank, of the dense matrix. The use of H-matrices reduces the required memory footprint of the dense matrices from O(N^2) to O(NlogN) and is suitable for many-core processors that have relatively small memory capacity compared to traditional CPUs. However, the existing parallel Adaptive Cross Approximation (ACA) algorithms, which are low- rank approximation algorithms used to construct H-matrices, are not designed to exploit many-core processors in terms of load balancing. In the existing parallel algorithm, the ACA process is independently applied to each sub-matrix. The computational load of ACA process for each sub-matrix depends on the sub- matrix’s rank but the rank is defined after ACA applied. It makes difficult to balance the load. We propose a load-balancing aware parallel ACA algorithm for H-matrices that focus on many-core processors. We implemented the proposed algorithm into HACApK, which is an open-source H-matrices library originally developed for CPU-based clusters. The proposed algorithm was evaluated using BEM problems on an NVIDIA Tesla P100 GPU (P100) and an Intel Xeon Broadwell processor (BDW). The evaluation results demonstrate the improved performance of the proposed algorithm in all GPU cases. For example, in a case wherein it is difficult for existing parallel algorithms to balance load, the proposed algorithm achieved 12.9 times performance improvement for P100.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap290"></a><div class="slot-title">A New Approach for Sparse Matrix Classification Based on Deep Learning Techniques <a href="calendar/submissions/sess105--pap290.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Juan Carlos Pichel and Beatriz Pateiro-López (Universidade de Santiago de Compostela)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_120_1536846530_06" onclick="$('#vhsjs_view_120_1536846530_06').hide();
                $('#vhsjs_hide_120_1536846530_06').show();
                $('#119_1536846530_06').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_120_1536846530_06" onclick="$('#119_1536846530_06').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_120_1536846530_06').hide();
                $('#vhsjs_view_120_1536846530_06').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="120_1536846530_06" id="119_1536846530_06" style="display: none"><div class="arrow-slidedown"><blockquote>In this paper, a new methodology to select the best storage format for sparse matrices based on deep learning techniques is introduced. We focus on the selection of the proper format for the sparse matrix-vector multiplication (SpMV), which is one of the most important computational kernels in many scientific and engineering applications. Our approach considers the sparsity pattern of the matrices as an image, using the RGB channels to code several of the matrix properties. As a consequence, we generate image datasets that include enough information to successfully train a Convolutional Neural Network (CNN). Considering GPUs as target platforms, the trained CNN selects the best storage format 90.1% of the time, obtaining 99.4% of the highest SpMV performance among the tested formats.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">1:30pm-3:00pm</span> <a href="calendar/sessions/sess106.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Minor Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session III: Architecture and Interconnect</div><div class="session-chair">Chair: Ann Gentile (Sandia National Laboratories)<br /></div><div class="slot-entry"><a name="pap274"></a><div class="slot-title">The Tofu Interconnect D <a href="calendar/submissions/sess106--pap274.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Yuichiro Ajima, Takahiro Kawashima, Takayuki Okamoto, Naoyuki Shida, Kouichi Hirai, Toshiyuki Shimizu, Shinya Hiramoto, Yoshiro Ikeda, Takahide Yoshikawa, Kenji Uchida, and Tomohiro Inoue (Fujitsu Limited)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_122_1536846530_06" onclick="$('#vhsjs_view_122_1536846530_06').hide();
                $('#vhsjs_hide_122_1536846530_06').show();
                $('#121_1536846530_06').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_122_1536846530_06" onclick="$('#121_1536846530_06').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_122_1536846530_06').hide();
                $('#vhsjs_view_122_1536846530_06').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="122_1536846530_06" id="121_1536846530_06" style="display: none"><div class="arrow-slidedown"><blockquote>In this paper, we introduce a new and highly scalable interconnect called Tofu interconnect D that will be used in the post-K machine. This machine will officially be operational around 2021. The letter D represents high “density” node and “dynamic” packet slicing for “dual-rail” transfer. Herein we describe the design and the evaluation results of TofuD. Due to the high-density packaging, the optical link ratio of TofuD has decreased to 25% from the 66% optical link ratio of Tofu2. TofuD applies a new technique called dynamic packet slicing to reduce latency and to improve fault resilience. The evaluation results show that the one-way 8-byte Put latency is 0.49 &#956;s. This is 31% lower than the latency of Tofu2. The injection rate per node is 38.1 GB/s which is approximately 83% of the injection rate of Tofu2. The link efficiency is as high as approximately 93%.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap232"></a><div class="slot-title">Janus: A Generic QoS Framework for Software-as-a-Service Applications <a href="calendar/submissions/sess106--pap232.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Qingye Jiang (The University of Sydney), Young Choon Lee (Macquarie University), and Albert Y. Zomaya (The University of Sydney)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_124_1536846530_07" onclick="$('#vhsjs_view_124_1536846530_07').hide();
                $('#vhsjs_hide_124_1536846530_07').show();
                $('#123_1536846530_07').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_124_1536846530_07" onclick="$('#123_1536846530_07').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_124_1536846530_07').hide();
                $('#vhsjs_view_124_1536846530_07').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="124_1536846530_07" id="123_1536846530_07" style="display: none"><div class="arrow-slidedown"><blockquote>The move from the traditional Software-as-a-Product (SaaP) model to the Software-as-a-Service (SaaS) model is apparent with the adoption of cloud computing. Unlike the SaaP model, the SaaS model delivers a diverse set of software features directly from public clouds to a large number of users with varying quality of service (QoS) requirements. There are two outstanding issues with traditional QoS systems: (1) they are usually designed and developed with a special purpose, making them difficult to be reused for other use cases; and (2) they have limited scalability due to the write-intensive nature of admission control workload. In this paper, we present Janus - a generic and scalable QoS framework for SaaS applications, taking full advantage of cloud's inherent horizontal scalability. Janus uses a multi-layer architecture to eliminate the communication between nodes in the same layer achieving horizontal scalability without sacrificing vertical scalability. Janus ensures accurate admission control using a distributed set of leaky buckets with a refill mechanism. Janus adopts a key-value request-response mechanism for easy integration with the actual application. We extensively evaluate Janus on AWS with both Apache HTTP server benchmarking tool and a photo sharing web application. Our experimental results demonstrate that (a) Janus achieves linear scalability both vertically and horizontally, and (b) Janus can be integrated with existing applications with a minimum amount of code change. Janus achieves more than 100,000 requests per second with only 10 nodes in the QoS server layer and 90% of the admission control decision were made in 3 milliseconds.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap297"></a><div class="slot-title">Exploring HPC and Big Data Convergence: A Graph Processing Study on Intel Knights Landing <a href="calendar/submissions/sess106--pap297.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Alexandru Uta (VU University Amsterdam), Ana Lucia Varbanescu (University of Amsterdam), Ahmed Musaafir (Vrije Universiteit Amsterdam), Chris Lemaire (TU Delft), and Alexandru Iosup (Vrije Universiteit Amsterdam)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_126_1536846530_07" onclick="$('#vhsjs_view_126_1536846530_07').hide();
                $('#vhsjs_hide_126_1536846530_07').show();
                $('#125_1536846530_07').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_126_1536846530_07" onclick="$('#125_1536846530_07').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_126_1536846530_07').hide();
                $('#vhsjs_view_126_1536846530_07').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="126_1536846530_07" id="125_1536846530_07" style="display: none"><div class="arrow-slidedown"><blockquote>The question “Can big data and HPC infrastructure converge?” has important implications for many operators and clients of modern computing. However, answering it is challenging. The hardware is currently different, and fast evolving: big data uses machines with modest numbers of fat cores per socket, large caches, and much memory, whereas HPC uses machines with larger numbers of (thinner) cores, non-trivial NUMA architectures, and fast interconnects. In this work, we investigate the convergence of big data and HPC infrastructure for one of the most challenging application domains, the highly irregular graph processing. We contrast through a systematic, experimental study of over 300,000 core-hours the performance of a modern multicore, Intel Knights Landing (KNL) and of traditional big data hardware, in processing representative graph workloads using state-of-the-art graph analytics platforms. The experimental results indicate KNL is convergence-ready, performance-wise, but only after extensive and expert-level tuning of software and hardware parameters.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">3:30pm-5:15pm</span> <a href="calendar/sessions/sess107.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session IV: Generating and Optimizing Applications</div><div class="session-chair">Chair: Nathan Tallent (Pacific Northwest National Laboratory)<br /></div><div class="slot-entry"><a name="pap115"></a><div class="slot-title">Whole Program Generation of Massively Parallel Shallow Water Equation Solvers <a href="calendar/submissions/sess107--pap115.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Sebastian Kuckuk and Harald Köstler (Friedrich-Alexander-Universität Erlangen-Nürnberg)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_128_1536846530_07" onclick="$('#vhsjs_view_128_1536846530_07').hide();
                $('#vhsjs_hide_128_1536846530_07').show();
                $('#127_1536846530_07').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_128_1536846530_07" onclick="$('#127_1536846530_07').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_128_1536846530_07').hide();
                $('#vhsjs_view_128_1536846530_07').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="128_1536846530_07" id="127_1536846530_07" style="display: none"><div class="arrow-slidedown"><blockquote>The study of ocean currents has been an active area of research for decades.
As a model close to the water surface, the shallow water equations (SWE) can be used.
For realistic simulations, efficient numerical solvers are necessary that exhibit a good node-level performance while still maintaining scalability.
When comparing the discretized model and the actual implementation, one often finds that they differ vastly.
This gap makes it hard for domain experts to implement their models and high performance computing (HPC) experts are required to ensure an optimal implementation.
Using domain-specific languages (DSLs) and code generation techniques can be a useful tool to bridge this gap.
In recent years, ExaStencils and its DSL ExaSlang have proven to provide a suitable platform for this.
We present an extension from up to now elliptic to hyperbolic partial differential equations (PDEs) in this work, namely the SWE. After setting up a suitable discretization, we demonstrate how it can be mapped to ExaSlang code.
This code is still quite similar to the original, mathematically motivated specification and can be easily written by domain experts.
Still, solvers generated from this abstract representation can be run on large-scale clusters.
We demonstrate this by giving performance and scalability results on the state-of-the-art GPU cluster Piz Daint where we solve for close to a trillion unknowns on 2048 GPUs.
From there, we discuss the performance impact of different optimizations such as overlapping computation and communication, or switching to a hybrid CPU-GPU parallelization scheme.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap134"></a><div class="slot-title">OpenACC vs the Native Programming on Sunway TaihuLight: A Case Study with GTC-P <a href="calendar/submissions/sess107--pap134.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Linjin Cai, Yichao Wang, and James Lin (Shanghai Jiao Tong University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_130_1536846530_07" onclick="$('#vhsjs_view_130_1536846530_07').hide();
                $('#vhsjs_hide_130_1536846530_07').show();
                $('#129_1536846530_07').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_130_1536846530_07" onclick="$('#129_1536846530_07').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_130_1536846530_07').hide();
                $('#vhsjs_view_130_1536846530_07').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="130_1536846530_07" id="129_1536846530_07" style="display: none"><div class="arrow-slidedown"><blockquote>Sunway TaihuLight is China's recent top-ranked supercomputer worldwide that was the first to be built entirely with home-grown processors. This supercomputer can be programmed with two approaches: directive-based OpenACC and native programming. These approaches are studied here using GTC-P, a particle-in-cell code for investigating micro-turbulence in magnetic fusion plasmas. We have compared the performance and programming efforts between the OpenACC and the native version of GTC-P. Associated results show that in the OpenACC version, the kernel with irregular memory access becomes the main performance bottleneck due to poor data locality. To address this issue, we have applied two optimizations on the native version: (1) register level communication (RLC); and (2) an “asynchronization” strategy. With these two optimizations, the native version can achieve up to 2.5X speedup for the memory-bound kernel compared with the OpenACC version. In addition, we have now scaled GTC-P on 4,259,840 cores of TaihuLight and demonstrate performance comparisons with several world-leading supercomputers.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap278"></a><div class="slot-title">Parallel Approximation of the Maximum Likelihood Estimation for the Prediction of Large-Scale Geostatistics Simulations <a href="calendar/submissions/sess107--pap278.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Sameh Abdulah, Hatem Ltaief, Ying Sun, Marc Genton, and David Keyes (King Abdullah University of Science and Technology)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_132_1536846530_08" onclick="$('#vhsjs_view_132_1536846530_08').hide();
                $('#vhsjs_hide_132_1536846530_08').show();
                $('#131_1536846530_08').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_132_1536846530_08" onclick="$('#131_1536846530_08').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_132_1536846530_08').hide();
                $('#vhsjs_view_132_1536846530_08').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="132_1536846530_08" id="131_1536846530_08" style="display: none"><div class="arrow-slidedown"><blockquote>Maximum likelihood estimation is an important statistical technique for estimating missing data,  for example in climate and environmental applications, which are usually large and feature data points that are irregularly spaced. In particular, the Gaussian log-likelihood function is the de facto model, which operates on the resulting sizable dense covariance matrix. The advent of high-performance systems with advanced computing power and memory capacity have enabled full simulations only for rather small dimensional climate problems, solved at the machine precision accuracy. The challenge for high dimensional problems lies in the computation requirements of the log-likelihood function, which necessitates O(n^2) storage and  O(n^3) operations, where n represents the number of given spatial locations. This prohibitive computational cost may be reduced by using approximation techniques that not only enable large-scale simulations otherwise intractable but also maintain the accuracy and the fidelity of the spatial statistics model. In this paper, we extend the Exascale GeoStatistics software (i.e., ExaGeoStat) to support the Tile Low-Rank (TLR) approximation, which exploits the data sparsity of the dense covariance matrix by compressing the off-diagonal tiles up to a user-defined accuracy threshold. The underlying linear algebra operations may then be carried out on this data compression format, which may ultimately reduce the arithmetic complexity of the maximum likelihood estimation and the corresponding memory footprint. Performance results of TLR-based computations on shared and distributed-memory systems attain up to 13X and 5X speedups,  respectively, compared to full accuracy simulations using synthetic and real datasets (up to 2M), while ensuring adequate prediction accuracy.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap231"></a><div class="slot-title">Modeling I/O performance variability using conditional variational auto encoders <a href="calendar/submissions/sess107--pap231.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Sandeep Madireddy, Prasanna Balaprakash, Philip Carns, Robert Latham, Robert Ross, Snyder Shane, and Stefan M. Wild (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_134_1536846530_08" onclick="$('#vhsjs_view_134_1536846530_08').hide();
                $('#vhsjs_hide_134_1536846530_08').show();
                $('#133_1536846530_08').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_134_1536846530_08" onclick="$('#133_1536846530_08').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_134_1536846530_08').hide();
                $('#vhsjs_view_134_1536846530_08').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="134_1536846530_08" id="133_1536846530_08" style="display: none"><div class="arrow-slidedown"><blockquote>Storage system performance modeling is crucial for 
efficient use of heterogeneous shared resources on leadership-class
computers. 
Variability in application performance, particularly variability arising
from concurrent applications sharing I/O resources, 
is a major hurdle in the development of accurate performance models. 
We adopt a deep learning approach based on conditional variational auto 
encoders (CVAE) for I/O 
performance modeling, and use it to quantify performance variability. 
We illustrate our approach using the data collected on Edison, a production
supercomputing system at the National Energy Research Scientific Computing 
Center (NERSC).
The CVAE approach is investigated by comparing it to a previously proposed
sensitivity-based Gaussian process (GP) model. We find that the CVAE model 
performs slightly better than the GP model in cases where training and testing 
data come from different applications, since CVAE can inherently leverage the 
whole data from multiple applications whereas GP partitions the data and 
builds separate models for each partition. 
Hence, the CVAE offers an alternative modeling approach that does not
need pre-processing; it has enough flexibility to handle data from a wide variety
of applications without changing the inference approach.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">3:30pm-5:15pm</span> <a href="calendar/sessions/sess108.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Minor Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session V: Filesystems and Applications</div><div class="session-chair">Chair: Hans Vandierendonck (Queen's University Belfast)<br /></div><div class="slot-entry"><a name="pap244"></a><div class="slot-title">SciDP: Support HPC and Big Data Applications via Integrated Scientific Data Processing <a href="calendar/submissions/sess108--pap244.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Kun Feng and Xian-He Sun (Illinois Institute of Technology), Xi Yang (Teradata Inc), and Shujia Zhou (Northrop Grumman Information Technology)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_136_1536846530_08" onclick="$('#vhsjs_view_136_1536846530_08').hide();
                $('#vhsjs_hide_136_1536846530_08').show();
                $('#135_1536846530_08').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_136_1536846530_08" onclick="$('#135_1536846530_08').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_136_1536846530_08').hide();
                $('#vhsjs_view_136_1536846530_08').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="136_1536846530_08" id="135_1536846530_08" style="display: none"><div class="arrow-slidedown"><blockquote>Modern High Performance Computing (HPC) applications, such as Earth science simulations, produce large amounts of data due to the surging of computing power, while big data applications have become more compute-intensive due to increasingly sophisticated analysis algorithms. The needs of both HPC and big data technologies for advanced HPC and big data applications create a demand for integrated system support. In this study, we introduce Scientific Data Processing (SciDP) to support both HPC and big data applications via integrated scientific data processing. SciDP can directly process scientific data stored on a Parallel File System (PFS), which is typically deployed in an HPC environment, in a big data programming environment running atop Hadoop Distributed File System (HDFS). SciDP seamlessly integrates PFS, HDFS, and the widely-used R data analysis system to support highly efficient processing of scientific data. It utilizes the merits of both PFS and HDFS for fast data transfer, overlaps computing with data accessing, and integrates R into the data transfer process. Experimental results show that SciDP accelerates analysis and visualization of a production NASA Center for Climate Simulation (NCCS) climate and weather application by 6x to 8x when compared to existing solutions.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap179"></a><div class="slot-title">Applying Pwrake Workflow System and Gfarm File System to Telescope Data Processing <a href="calendar/submissions/sess108--pap179.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Masahiro Tanaka (Graduate School of Media and Governance, Keio University); Osamu Tatebe (Center for Computational Sciences, University of Tsukuba); and Hideyuki Kawashima (Faculty of Environment and Information Studies, Keio University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_138_1536846530_08" onclick="$('#vhsjs_view_138_1536846530_08').hide();
                $('#vhsjs_hide_138_1536846530_08').show();
                $('#137_1536846530_08').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_138_1536846530_08" onclick="$('#137_1536846530_08').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_138_1536846530_08').hide();
                $('#vhsjs_view_138_1536846530_08').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="138_1536846530_08" id="137_1536846530_08" style="display: none"><div class="arrow-slidedown"><blockquote>In this paper, we describe a use case applying a scientific workflow system and a distributed file system to improve the performance of telescope data processing. The application is pipeline processing of data generated by Hyper Suprime-Cam (HSC) which is a focal plane camera mounted on the Subaru telescope. In this paper, we focus on the scalability of parallel I/O and core utilization. The IBM Spectrum Scale (GPFS) used for actual operation has a limit on scalability due to the configuration using storage servers. Therefore, we introduce the Gfarm file system which uses the storage of the worker node for parallel I/O performance. To improve core utilization, we introduce the Pwrake workflow system instead of the parallel processing framework developed for the HSC pipeline. Descriptions of task dependencies are necessary to further improve core utilization by overlapping different types of tasks. We discuss the usefulness of the workflow description language with the function of scripting language for defining complex task dependency. In the experiment, the performance of the pipeline is evaluated using a quarter of the observation data per night (input files: 80 GB, output files: 1.2 TB). Measurements on strong scaling from 48 to 576 cores show that the processing with Gfarm file system is more scalable than that with GPFS. Measurement using 576 cores shows that our method improves the processing speed of the pipeline by 2.2 times compared with the method used in actual operation.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap284"></a><div class="slot-title">UniviStor: Integrated Hierarchical and Distributed Storage for HPC <a href="calendar/submissions/sess108--pap284.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Teng Wang, Suren Byna, Bin Dong, and Houjun Tang (Lawrence Berkeley National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_140_1536846530_09" onclick="$('#vhsjs_view_140_1536846530_09').hide();
                $('#vhsjs_hide_140_1536846530_09').show();
                $('#139_1536846530_09').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_140_1536846530_09" onclick="$('#139_1536846530_09').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_140_1536846530_09').hide();
                $('#vhsjs_view_140_1536846530_09').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="140_1536846530_09" id="139_1536846530_09" style="display: none"><div class="arrow-slidedown"><blockquote>High performance computing (HPC) architectures have been adding new layers of storage, such as burst buffers to tolerate latency between memory and disk-based file systems.
However, existing file system and burst buffer solutions typically manage each storage layer individually. Consequently, the burden of managing data across multiple layers falls upon HPC system users. <br><br>To hide the complexity of managing the scattered storage devices from applications, we introduce \textit{UniviStor}, a novel system offering a unified view of  storage layers. By considering each layer's distinct characteristics, UniviStor provides performance optimization algorithms and data structures for distributed and hierarchical data placement, interference-aware data movement scheduling, adaptive data striping, and lightweight workflow management. UniviStor supports parallel I/O libraries, such as MPI-IO and HDF5. Our evaluations on a large-scale supercomputer demonstrated that UniviStor outperforms Data Elevator, a state-of-the-art transparent caching solution for burst buffers by up to 17x and Lustre by up to 46x</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap124"></a><div class="slot-title">HIDStore: A Hierarchical Intermediate Data Storage System for Seismic Processing Application <a href="calendar/submissions/sess108--pap124.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Yida Wang (Beihang University); Changhai Zhao and Zengbo Wang (Research & Development Center, BGP Inc. CNPC); Chao Liu, Chao Li, and Haihua Yan (Beihang University); and Jiamin Wen (Research & Development Center, BGP Inc. CNPC)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_142_1536846530_09" onclick="$('#vhsjs_view_142_1536846530_09').hide();
                $('#vhsjs_hide_142_1536846530_09').show();
                $('#141_1536846530_09').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_142_1536846530_09" onclick="$('#141_1536846530_09').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_142_1536846530_09').hide();
                $('#vhsjs_view_142_1536846530_09').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="142_1536846530_09" id="141_1536846530_09" style="display: none"><div class="arrow-slidedown"><blockquote>Seismic processing is an important technology in petroleum industry. During the execution of seismic processing applications, large amount of intermediate data are generated and accessed. Providing high-performance services for intermediate data in the traditional storage architecture is expensive. In addition, because of the existence of new storage devices, the heterogeneity of storage environment has brought much inconvenience to the application developers and petroleum scientists. In this paper, we present a hierarchical intermediate data storage system called HIDStore. HIDStore employs distributed storage system based on the local storage devices and idle network resources to accelerate intermediate data access. Our experiments show that using HIDStore could improve the performance of various seismic processing applications and the resource utilization in compute cluster. HIDStore also abstracts different kinds of storage devices into hierarchical logical volumes and provides easy-to-use API to access data. Developers could deal with intermediate data in a high level of abstraction. Applications based on the HIDStore could fit into different storage environment and gain optimal performance automatically. Intermediate data in HIDStore could be automatically evicted once they expire.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">10:45am-12:15pm</span> <a href="calendar/sessions/sess113.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Plenary, Paper</span><br /><div class="session-title">Session VI: Best papers - Areas 3 and 4</div><div class="session-chair">Chair: Ron Brightwell (Sandia National Laboratories)<br /></div><div class="slot-entry"><a name="pap113"></a><div class="slot-title">Neural network based silent error detector <a href="calendar/submissions/sess113--pap113.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Chen Wang and Nikoli Dryden (University of Illinois at Urbana-Champaign), Franck Cappello (Argonne National Laboratory), and Marc Snir (University of Illinois at Urbana-Champaign)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_144_1536846530_09" onclick="$('#vhsjs_view_144_1536846530_09').hide();
                $('#vhsjs_hide_144_1536846530_09').show();
                $('#143_1536846530_09').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_144_1536846530_09" onclick="$('#143_1536846530_09').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_144_1536846530_09').hide();
                $('#vhsjs_view_144_1536846530_09').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="144_1536846530_09" id="143_1536846530_09" style="display: none"><div class="arrow-slidedown"><blockquote>As we move toward exascale platforms, silent data corruptions (SDC) are likely to occur more frequently. Such errors can lead to incorrect results. Attempts have been made to use generic algorithms to detect such errors. Such detectors have demonstrated high precision and recall for detecting errors, but only if they run immediately after an error has been injected. In this paper, we propose a neural network based detector that can detect SDCs even multiple iterations after they were injected. We have evaluated our detector with 6 FLASH applications and 2 Mantevo mini-apps. Experiments show that our detector can detect more than 89% of SDCs with a false positive rate of less than 2%.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap131"></a><div class="slot-title">An Efficient Transformation Scheme for Lossy Data Compression with Point-wise Relative Error Bound <a href="calendar/submissions/sess113--pap131.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Xin Liang (UC Riverside), Sheng Di (Argonne National Laboratory), Dingwen Tao and Zizhong Chen (UC Riverside), and Franck Cappello (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_146_1536846530_09" onclick="$('#vhsjs_view_146_1536846530_09').hide();
                $('#vhsjs_hide_146_1536846530_09').show();
                $('#145_1536846530_09').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_146_1536846530_09" onclick="$('#145_1536846530_09').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_146_1536846530_09').hide();
                $('#vhsjs_view_146_1536846530_09').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="146_1536846530_09" id="145_1536846530_09" style="display: none"><div class="arrow-slidedown"><blockquote>Because of the ever-increasing execution scale of scientific applications, how to store the extremely large volume of data efficiently is becoming a serious issue. A significant reduction of the scientific data size can effectively mitigate the I/O burden and save considerable storage space. Since lossless compressors suffer from limited compression ratios, error-controlled lossy compressors have been studied for years. Existing error-controlled lossy compressors, however,  focus mainly on absolute error bounds, which cannot meet users' diverse demands such as pointwise relative error bounds. Although some of the state-of-the-art lossy compressors support pointwise relative error bound, the compression ratios are generally low because of the limitation in their designs and possible spiky data changes in local data regions. In this work, we propose a novel, efficient approach to perform compression based on the pointwise relative error bound with higher compression ratios than existing solutions provide. Our contribution is threefold. (1) We propose a novel transformation scheme that can transfer the pointwise relative-error-bounded compression problem to an absolute-error-bounded compression issue. We also analyze the practical properties of our transformation scheme both theoretically and experimentally. (2) We implement the proposed technique in two of the most popular absolute-error-bounded lossy compressors, SZ and ZFP. (3) We evaluate our solution using multiple real-world application data across different scientific domains on a supercomputer with up to 4,096 cores and 12 TB of data. Experiments show that our solution achieves over $1.38X$ dumping and $1.31X$ loading performance over the second-best lossy compressor, respectively.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">1:30pm-3:00pm</span> <a href="calendar/sessions/sess115.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session VII: Benchmarking and Modeling</div><div class="session-chair">Chair: Tal Ben-Nun (ETH Zurich)<br /></div><div class="slot-entry"><a name="pap212"></a><div class="slot-title">A Methodology for Characterizing the Correspondence Between Real and Proxy Applications <a href="calendar/submissions/sess115--pap212.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Omar Aaziz and Jeanine Cook (Sandia National Labs), Jonathan Cook (New Mexico State University), Tanner Juedeman (Sandia National Labs), David Richards (Lawrence Livermore National Laboratory), and Courtenay Vaughan (Sandia National Labs)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_148_1536846530_1" onclick="$('#vhsjs_view_148_1536846530_1').hide();
                $('#vhsjs_hide_148_1536846530_1').show();
                $('#147_1536846530_1').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_148_1536846530_1" onclick="$('#147_1536846530_1').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_148_1536846530_1').hide();
                $('#vhsjs_view_148_1536846530_1').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="148_1536846530_1" id="147_1536846530_1" style="display: none"><div class="arrow-slidedown"><blockquote>Proxy applications are a simplified means for stakeholders to evaluate how both hardware and software stacks might perform on the class of real applications that they are meant to model. However, characterizing the re- lationship between them and their behavior is not an easy task. We present a data-driven methodology for characterizing the relationship between real and proxy applications based on collecting runtime data from both and then using data analytics to find their correspondence and divergence. We use new capabilities for application-level monitoring within LDMS (Lightweight Distributed Monitoring System) to cap- ture hardware performance counter and MPI-related data. To demonstrate the utility of this methodology, we present experimental evidence from two system platforms, using four proxy applications from the current ECP Proxy Application Suite and their corresponding parent applications (in the ECP application portfolio). Results show that each proxy analyzed is representative of its parent with respect to computation and memory behavior. We also analyze communication patterns separately using mpiP data and show that communication for these four proxy/parent pairs is also similar.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap174"></a><div class="slot-title">Lightweight Requirements Engineering for Exascale Co-design <a href="calendar/submissions/sess115--pap174.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Alexandru Calotoiu and Alexander Graf (TU Darmstadt), Torsten Hoefler (ETH Zurich), and Daniel Lorenz and Felix Wolf (TU Darmstadt)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_150_1536846530_1" onclick="$('#vhsjs_view_150_1536846530_1').hide();
                $('#vhsjs_hide_150_1536846530_1').show();
                $('#149_1536846530_1').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_150_1536846530_1" onclick="$('#149_1536846530_1').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_150_1536846530_1').hide();
                $('#vhsjs_view_150_1536846530_1').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="150_1536846530_1" id="149_1536846530_1" style="display: none"><div class="arrow-slidedown"><blockquote>Given the tremendous cost of an exascale system, its architecture
  must match the requirements of the applications it is supposed to
  run as precisely as possible. Conversely, applications must be
  designed such that building an appropriate system becomes feasible,
  motivating the idea of co-design. In this process, a fundamental
  aspect of the application requirements are the rates at which the
  demand for different resources grows as a code is scaled to a larger
  machine. However, if the anticipated scale exceeds the size of
  available platforms this demand can no longer be measured. This is
  clearly the case when designing an exascale system. Moreover,
  creating analytical models to predict these requirements is often
  too laborious - especially when the number and complexity of target
  applications is high. In this paper, we show how automated
  performance modeling can be used to quickly predict application
  requirements for varying scales and problem sizes. Following this
  approach, we determine the exascale requirements of five scientific codes
  and use them to illustrate system design tradeoffs.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap204"></a><div class="slot-title">Next Stop “NoOps": Enabling Cross-System Diagnostics Through Graph-based Composition of Logs and Metrics <a href="calendar/submissions/sess115--pap204.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Micha&#322; Zasadzi&#324;ski and Marc Solé (CA Technologies), Alvaro Brandon (Universitat Politecnica de Madrid), Victor Muntés-Mulero (CA Technologies), and David Carrera (Universitat Politecnica de Catalunya)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_152_1536846530_1" onclick="$('#vhsjs_view_152_1536846530_1').hide();
                $('#vhsjs_hide_152_1536846530_1').show();
                $('#151_1536846530_1').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_152_1536846530_1" onclick="$('#151_1536846530_1').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_152_1536846530_1').hide();
                $('#vhsjs_view_152_1536846530_1').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="152_1536846530_1" id="151_1536846530_1" style="display: none"><div class="arrow-slidedown"><blockquote>Performing diagnostics in IT systems is an increasingly complicated task, and it is not doable in satisfactory time by even most skillful operators. Systems and their architecture change very rapidly in response to business and user demand. Many organizations see value in the maintenance and management model of NoOps that stands for No Operations. One of the implementations of this model is a system that is maintained automatically without any human intervention. The path to NoOps involves not only precise and fast diagnostics but also reusing as much knowledge as possible after the system is reconfigured or changed. The biggest challenge is to leverage knowledge on one IT system and reuse this knowledge for diagnostics of another, different system. We propose a framework of weighted graphs which can transfer knowledge, and perform high-quality diagnostics of IT systems. We encode all possible data in a graph representation of a system state and automatically calculate weights of these graphs. Then, thanks to the evaluation of similarity between graphs, we transfer knowledge about failures from one system to another and use it for diagnostics. We successfully evaluate the proposed approach on Spark, Hadoop, Kafka and Cassandra systems.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">1:30pm-3:00pm</span> <a href="calendar/sessions/sess114.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Minor Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session VIII: Managing Heterogeneity and Imbalance</div><div class="session-chair">Chair: Rong Ge (Clemson University)<br /></div><div class="slot-entry"><a name="pap243"></a><div class="slot-title">Cutting the Tail: Designing High Performance Message Brokers to Reduce Tail Latencies in Stream Processing <a href="calendar/submissions/sess114--pap243.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">M. Haseeb Javed, Xiaoyi Lu, and Dhabaleswar K. Panda (The Ohio State University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_154_1536846530_1" onclick="$('#vhsjs_view_154_1536846530_1').hide();
                $('#vhsjs_hide_154_1536846530_1').show();
                $('#153_1536846530_1').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_154_1536846530_1" onclick="$('#153_1536846530_1').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_154_1536846530_1').hide();
                $('#vhsjs_view_154_1536846530_1').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="154_1536846530_1" id="153_1536846530_1" style="display: none"><div class="arrow-slidedown"><blockquote>Over the last decade, organizations have become heavily reliant on providing near-instantaneous insights to the end user based on vast amounts of data collected from various sources in real-time. To accomplish this task, a stream processing pipeline is constructed which consists of a Stream Processing Engine (SPE) and a Message Broker (MB). The SPE is responsible for performing computations on the data and providing insights from it. MB acts as an intermediate queue to which data is written by ephemeral sources and then fetched by the SPE to perform computations on. Due to the inherent real-time nature of such a pipeline, low latency is a highly desirable feature for them. Thus, many existing research works have focused on improving latency and throughput for the streaming pipeline. However, there is a dearth of studies optimizing the tail latencies of such pipelines. Moreover, the root cause of this high tail latency is still vague. In this paper, we propose a model-based approach to analyze in-depth the reasons behind high tail latency in streaming systems such as Apache Kafka. Having found the MB to be a major contributor of messages with high tail latencies in a streaming pipeline, we design and implement a high-performance MB, called Frieda, with the higher goal of accelerating any arbitrary stream processing pipeline regardless of the SPE used. Our experiments show a reduction of up to 89%x in 99.9th percentile latency for microbenchmarks and up to 31% for full-fledged stream processing pipeline constructed using Yahoo! Streaming Benchmark.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap103"></a><div class="slot-title">Dynamic Control of CPU Usage in a Lambda Platform <a href="calendar/submissions/sess114--pap103.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Young Ki Kim and M. Reza HoseinyFarahabady (The University of Sydney), Young Choon Lee (Macquarie University), Albert Y. Zomaya (The University of Sydney), and Raja Jurdak (CSIRO)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_156_1536846530_11" onclick="$('#vhsjs_view_156_1536846530_11').hide();
                $('#vhsjs_hide_156_1536846530_11').show();
                $('#155_1536846530_11').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_156_1536846530_11" onclick="$('#155_1536846530_11').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_156_1536846530_11').hide();
                $('#vhsjs_view_156_1536846530_11').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="156_1536846530_11" id="155_1536846530_11" style="display: none"><div class="arrow-slidedown"><blockquote>Lambda platform is a new concept based on an event-driven server-less computation that empowers application developers to build scalable enterprise software in a virtualized environment without provisioning or managing any physical servers (a server-less solution).
In reality, however, devising an effective consolidation method to host multiple Lambda functions into a single machine is challenging.
The existing simple resource allocation algorithms, such as the round-robin policy used in many commercial server-less systems, suffer from lack of responsiveness to a sudden surge in the incoming workload.
This will result in an unsatisfactory performance degradation that is directly experienced by the end-user of a Lambda application.
In this paper, we address the problem of CPU cap management in a Lambda platform for ensuring different QoS enforcement levels in a platform with shared resources, in case of fluctuations and sudden surges in the incoming workload requests.
To this end, we present a closed-loop (feedback-based) CPU cap controller, which fulfills the QoS levels enforced by the application owners.
The controller adjusts the number of working threads per QoS class and dispatches the outstanding Lambda functions along with the associated events to the most appropriate working thread.
The proposed solution reduces the QoS violations by an average of 6.36 times compared to the round-robin policy.
It can also maintain the end-to-end response time of applications belonging to the highest priority QoS class close to the target set-point while decreasing the overall response time by up to 52%.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap105"></a><div class="slot-title">A Heterogeneity-Aware Task Scheduler for Spark <a href="calendar/submissions/sess114--pap105.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Luna Xu and Ali R. Butt (Virginia Tech) and Seung-Hwan Lim and Ramakrishnan Kannan (Oak Ridge National Lab)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_158_1536846530_11" onclick="$('#vhsjs_view_158_1536846530_11').hide();
                $('#vhsjs_hide_158_1536846530_11').show();
                $('#157_1536846530_11').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_158_1536846530_11" onclick="$('#157_1536846530_11').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_158_1536846530_11').hide();
                $('#vhsjs_view_158_1536846530_11').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="158_1536846530_11" id="157_1536846530_11" style="display: none"><div class="arrow-slidedown"><blockquote>Big data processing systems such as Spark are employed in an increasing number of diverse applications such as machine learning, graph computation, and scientific computing, each with dynamic and different resource needs. These applications increasingly run on heterogeneous hardware, e.g., with out-of-core accelerators. However, big data platforms do not factor in this multi-dimensional heterogeneity of applications and hardware leading to a fundamental mismatch between the application and hardware characteristics, and the resource scheduling adopted in big data platforms. For example, Hadoop and Spark consider only data locality when assigning tasks to nodes and typically disregard the hardware capabilities and their suitability to specific application requirements.<br><br>In this paper, we present RUPAM, a heterogeneity-aware task scheduling system for big data platforms, which considers both task-level resource characteristics and underlying hardware characteristics, as well as preserves data locality. RUPAM adopts a simple yet effective heuristic to decide the dominant scheduling factor, e.g., CPU, memory, or I/O, given a task in a particular stage. Our experiments show that RUPAM is able to improve the performance of representative applications by up to 62.26% compared to the standard Spark scheduler.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">3:30pm-5:00pm</span> <a href="calendar/sessions/sess117.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session IX: Graphs and Big Data Analytics</div><div class="session-chair">Chair: Ali R. Butt (Virginia Tech)<br /></div><div class="slot-entry"><a name="pap255"></a><div class="slot-title">Computing Exact Vertex Eccentricity on Massive-scale Distributed Graphs <a href="calendar/submissions/sess117--pap255.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Keita Iwabuchi, Geoff Sanders, Keith Henderson, and Roger Pearce (Lawrence Livermore National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_160_1536846530_11" onclick="$('#vhsjs_view_160_1536846530_11').hide();
                $('#vhsjs_hide_160_1536846530_11').show();
                $('#159_1536846530_11').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_160_1536846530_11" onclick="$('#159_1536846530_11').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_160_1536846530_11').hide();
                $('#vhsjs_view_160_1536846530_11').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="160_1536846530_11" id="159_1536846530_11" style="display: none"><div class="arrow-slidedown"><blockquote>The eccentricity of a vertex is defined as the length of the longest shortest path to any other vertex. While eccentricity is an important measure of vertex centrality, directly computing exact eccentricity for all vertices on large-scale graphs is prohibitively costly. Takes and Kosters proposed an iterative algorithm that uses multiple runs of single-source shortest path (SSSP) to compute lower and upper bounds on eccentricity at every vertex. Their technique converges to exact eccentricity by performing SSSP from only a small percentage of vertices, when sources are efficiently selected. However, their source selection strategies do not always yield rapid convergence.<br><br>We propose a pincer movement source selection algorithm that efficiently selects source vertices based on analysis of the lower and upper bounds produced by SSSP. We also leverage k-BFS, which runs breadth-first search (BFS) from multiple sources concurrently on HavoqGT, a high-performance vertex-centric message-passing graph processing framework, to achieve an additional significant performance improvement on distributed-memory systems.<br><br>We demonstrate that our novel source vertex selection strategy has better performance on
various real-world graph datasets compared with the previous strategy. In addition, we compute exact eccentricity for graphs with more than 1000X more edges (112B undirected edges) than graphs in the previous literature.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap214"></a><div class="slot-title">A Scalable Distributed Louvain Algorithm for Large-scale Graph Community Detection <a href="calendar/submissions/sess117--pap214.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jianping Zeng and Hongfeng Yu (University of Nebraska-Lincoln)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_162_1536846530_11" onclick="$('#vhsjs_view_162_1536846530_11').hide();
                $('#vhsjs_hide_162_1536846530_11').show();
                $('#161_1536846530_11').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_162_1536846530_11" onclick="$('#161_1536846530_11').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_162_1536846530_11').hide();
                $('#vhsjs_view_162_1536846530_11').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="162_1536846530_11" id="161_1536846530_11" style="display: none"><div class="arrow-slidedown"><blockquote>We present a new distributed community detection algorithm for large graphs based on the Louvain method. We exploit a distributed delegate partitioning to ensure the workload and communication balancing among processors. In addition, we design a new heuristic strategy to carefully coordinate the community constitution in a distributed environment, and ensure the convergence of the distributed clustering algorithm. Our intensive experimental study has demonstrated the scalability and the correctness of our algorithm with various large-scale real-world and synthetic graph datasets using up to 32,768 processors.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap218"></a><div class="slot-title">Optimizing Distributed Data-Intensive Workflows <a href="calendar/submissions/sess117--pap218.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Ryan D. Friese, Nathan Tallent, Malachi Schram, Mahantesh Halappanavar, and Kevin J. Barker (Pacific Northwest National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_164_1536846530_12" onclick="$('#vhsjs_view_164_1536846530_12').hide();
                $('#vhsjs_hide_164_1536846530_12').show();
                $('#163_1536846530_11').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_164_1536846530_12" onclick="$('#163_1536846530_11').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_164_1536846530_12').hide();
                $('#vhsjs_view_164_1536846530_12').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="164_1536846530_12" id="163_1536846530_11" style="display: none"><div class="arrow-slidedown"><blockquote>We present techniques for optimizing the performance of data-intensive workflows that execute on geographically distributed and heterogeneous resources. We optimize for both throughput and response time. Optimizing for throughput, we alleviate data-transfer bottlenecks. To hide access times of accessing remote data, we transparently introduce prefetching (overlapping data transfer and computation), without changing workflow source code. Optimizing for response time, we introduce intelligent scheduling for a set of high-priority tasks. We replace a greedy scheduler that assigns tasks without accounting for differing performance on heterogeneous resources, leading to long latencies. Intelligent scheduling rapidly selects a near-optimal solution for a bi-objective optimization problem. One objective is a good task assignment; the other objective is minimize I/O contention by distributing load across resources and time. To reason about task completion times, we use modeling tools to generate accurate predictions of execution times. We show performance results for Belle II workflow for high energy physics. The combination of these techniques can improve throughput over production Belle II configurations by 20-40%. Our work is general and adaptable to other distributed workflows.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">3:30pm-5:00pm</span> <a href="calendar/sessions/sess118.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Minor Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session X: Performance Engineering in Filesystems</div><div class="session-chair">Chair: Anthony Skjellum (University of Tennessee at Chattanooga)<br /></div><div class="slot-entry"><a name="pap283"></a><div class="slot-title">Harmonia: An Interference-Aware Dynamic I/O Scheduler for Shared Non-Volatile Burst Buffers <a href="calendar/submissions/sess118--pap283.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Anthony Kougkas (Illinois Institute of Technology Chicago), Hariharan Devarajan and Xian-He Sun (Illinois Institute of Technology), and Jay Lofstead (Sandia National Laboratories)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_166_1536846530_12" onclick="$('#vhsjs_view_166_1536846530_12').hide();
                $('#vhsjs_hide_166_1536846530_12').show();
                $('#165_1536846530_12').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_166_1536846530_12" onclick="$('#165_1536846530_12').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_166_1536846530_12').hide();
                $('#vhsjs_view_166_1536846530_12').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="166_1536846530_12" id="165_1536846530_12" style="display: none"><div class="arrow-slidedown"><blockquote>Modern HPC systems employ burst buffer installations to reduce the peak I/O requirements for external storage and deal with the burstiness of I/O in modern scientific applications.  These I/O buffering resources are shared between multiple applications that run concurrently. This leads to severe performance degradation due to contention, a phenomenon called cross-application I/O interference. In this paper, we first explore the negative effects of interference at the burst buffer layer and we present two new metrics that can quantitatively describe the slowdown applications experience due to interference. We introduce Harmonia, a new dynamic I/O scheduler that is aware of interference, adapts to the underlying system, implements a new 2-way decision-making process and employs several scheduling policies to maximize the system efficiency and applications' performance. Our evaluation shows that Harmonia, through better I/O scheduling, can outperform by 3x existing state-of-the-art buffering management solutions and can lead to better resource utilization.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap205"></a><div class="slot-title">CRUM: Checkpoint-Restart Support for CUDA's Unified Memory <a href="calendar/submissions/sess118--pap205.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Rohan Garg and Apoorve Mohan (Northeastern University), Michael Sullivan (Nvidia Research), and Gene Cooperman (Northeastern University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_168_1536846530_12" onclick="$('#vhsjs_view_168_1536846530_12').hide();
                $('#vhsjs_hide_168_1536846530_12').show();
                $('#167_1536846530_12').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_168_1536846530_12" onclick="$('#167_1536846530_12').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_168_1536846530_12').hide();
                $('#vhsjs_view_168_1536846530_12').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="168_1536846530_12" id="167_1536846530_12" style="display: none"><div class="arrow-slidedown"><blockquote>Unified Virtual Memory (UVM) was recently introduced with CUDA version 8 and the Pascal GPU. The older CUDA programming style is akin to older large-memory UNIX applications which used to directly load and unload memory segments. Newer CUDA programs have started taking advantage of UVM for the same reasons of superior programmability that UNIX applications long ago switched to assuming the presence of virtual memory. Therefore, checkpointing of UVM has become increasing important, especially as NVIDIA CUDA continues to gain wider popularity: 87 of the top 500 supercomputers in the latest listings use NVIDIA GPUs, with a current trend of ten additional NVIDIA-based supercomputers each year.             <br><br>A new scalable checkpointing mechanism, CRUM (Checkpoint-Restart for Unified Memory), is demonstrated for hybrid CUDA/MPI computations across multiple computer nodes.  The support for UVM is particularly attractive for programs requiring more memory than resides on the GPU, since the alternative to UVM is for the application to directly copy memory between device and host.  Furthermore, CRUM supports a fast, forked checkpointing, which mostly overlaps the CUDA computation with storage of the checkpoint image in stable storage.  The runtime overhead of using CRUM is 6% on average, and the time for forked checkpointing is seen to be a factor of up to 40 times less than traditional, synchronous checkpointing.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap111"></a><div class="slot-title">Fixed-PSNR Lossy Compression for Scientific Data <a href="calendar/submissions/sess118--pap111.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Dingwen Tao (The University of Alabama); Sheng Di (Argonne National Laboratory); Xin Liang and Zizhong Chen (University of California, Riverside); and Franck Cappello (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_170_1536846530_12" onclick="$('#vhsjs_view_170_1536846530_12').hide();
                $('#vhsjs_hide_170_1536846530_12').show();
                $('#169_1536846530_12').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_170_1536846530_12" onclick="$('#169_1536846530_12').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_170_1536846530_12').hide();
                $('#vhsjs_view_170_1536846530_12').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="170_1536846530_12" id="169_1536846530_12" style="display: none"><div class="arrow-slidedown"><blockquote>Error-controlled lossy compression has been studied for years because of extremely large volumes of data being produced by today’s scientific simulations. None of existing lossy compressors, however, allow users to fix the peak signal-to-noise ratio (PSNR) during compression, although PSNR has been considered as one of the most significant indicators to assess compression quality. In this paper, we propose a novel technique providing a fixed-PSNR lossy compression for scientific data sets. We implement our proposed method based on the SZ lossy compression framework and release the code as an open-source toolkit. We evaluate our fixed-PSNR compressor on three real world high-performance computing data sets. Experiments show that our solution has a high accuracy in controlling PSNR, with an average deviation of 0.1 &#8764; 5.0 dB on the tested data sets.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap227"></a><div class="slot-title">GekkoFS - A temporary distributed file system for HPC applications <a href="calendar/submissions/sess118--pap227.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Marc-André Vef, Nafiseh Moti, and Tim Süß (Johannes Gutenberg University Mainz); Tommaso Tocci, Ramon Nou, and Alberto Miranda (Barcelona Supercomputing Center); Toni Cortes (Barcelona Supercomputing Center, Universitat Politecnica de Catalunya); and André Brinkmann (Johannes Gutenberg University Mainz)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_172_1536846530_13" onclick="$('#vhsjs_view_172_1536846530_13').hide();
                $('#vhsjs_hide_172_1536846530_13').show();
                $('#171_1536846530_13').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_172_1536846530_13" onclick="$('#171_1536846530_13').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_172_1536846530_13').hide();
                $('#vhsjs_view_172_1536846530_13').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="172_1536846530_13" id="171_1536846530_13" style="display: none"><div class="arrow-slidedown"><blockquote>We present GekkoFS, a temporary, highly-scalable burst buffer file system which has been specifically optimized for new access patterns of data-intensive High-Performance Computing (HPC) applications. The file system provides relaxed POSIX semantics, only offering features which are actually required by most (not all) applications. It is able to provide scalable I/O performance and reaches millions of metadata operations already for a small number of nodes, significantly outperforming the capabilities of general-purpose parallel file systems.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">10:45am-12:15pm</span> <a href="calendar/sessions/sess122.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session XI: Hierarchy and Sharing in System Architecture</div><div class="session-chair">Chair: Giorgis Georgakoudis (Queen's University Belfast)<br /></div><div class="slot-entry"><a name="pap128"></a><div class="slot-title">Hierarchical Clock Synchronization in MPI <a href="calendar/submissions/sess122--pap128.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Sascha Hunold (TU Wien) and Alexandra Carpen-Amarie (Fraunhofer ITWM)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_174_1536846530_13" onclick="$('#vhsjs_view_174_1536846530_13').hide();
                $('#vhsjs_hide_174_1536846530_13').show();
                $('#173_1536846530_13').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_174_1536846530_13" onclick="$('#173_1536846530_13').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_174_1536846530_13').hide();
                $('#vhsjs_view_174_1536846530_13').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="174_1536846530_13" id="173_1536846530_13" style="display: none"><div class="arrow-slidedown"><blockquote>MPI benchmarks are used for analyzing or tuning the performance of MPI libraries. Generally, every MPI library should be adjusted to the given parallel machine, especially on supercomputers. System operators can define which algorithm should be selected for a specific MPI operation, and this decision which algorithm to select is usually made after analyzing bench- mark results. The problem is that the latency of communication operations in MPI is very sensitive to the chosen data acquisition and data processing method. For that reason, depending on how the performance is measured, system operators may end up with a completely different MPI library setup.
In the present work, we focus on the problem of precisely measuring the latency of collective operations, in particular, for small payloads, where external experimental factors play a significant role. We present a novel clock synchronization algorithm, which exploits the hierarchical architecture of compute clusters, and we show that it outperforms previous approaches, both in run-time and in precision. We also propose a different scheme to obtain precise MPI run-time measurements (called Round-Time), which is based on given, fixed time slices, as opposed to the traditional way of measuring for a predefined number of repetitions. We also highlight that the use of MPI_Barrier has a significant effect on experimentally determined latency values of MPI collectives. We argue that MPI_Barrier should be avoided if the average run- time of the barrier function is in the same order of magnitude as the run-time of the MPI function to be measured.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap292"></a><div class="slot-title">Hybrid MPI+OpenMP Reactive Work Stealing in Distributed Memory in the PDE Framework sam(oa)^2 <a href="calendar/submissions/sess122--pap292.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Philipp Samfass (Technical University of Munich), Jannis Klinkenberg (RWTH Aachen University), and Michael Bader (Technical University of Munich)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_176_1536846530_13" onclick="$('#vhsjs_view_176_1536846530_13').hide();
                $('#vhsjs_hide_176_1536846530_13').show();
                $('#175_1536846530_13').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_176_1536846530_13" onclick="$('#175_1536846530_13').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_176_1536846530_13').hide();
                $('#vhsjs_view_176_1536846530_13').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="176_1536846530_13" id="175_1536846530_13" style="display: none"><div class="arrow-slidedown"><blockquote>“Equal work results in equal execution time” is an
assumption that has fundamentally driven design and implemen-
tation of parallel applications for decades. However, increasing
hardware variability on current architectures (e.g., through
Turbo Boost, dynamic voltage and frequency scaling or thermal
effects) necessitate a revision of this assumption. Expecting an
increase of these effects on future (exascale-)systems, we develop a
novel MPI+OpenMP-only distributed work stealing concept that
– based on on-line performance monitoring – selectively steals
and remotely executes tasks across MPI boundaries. This concept
has been implemented in the parallel adaptive mesh refinement
(AMR) framework sam(oa)^2 for OpenMP tasks of traversing
a grid section. Corresponding performance measurements in
the presence of enforced CPU clock frequency imbalances
demonstrate that a state-of-the-art cost-based (chains-on-chains
partitioning) load balancing mechanism is insufficient and can
even degrade performance, whereas additional distributed work
stealing successfully mitigates the frequency-induced imbalances.
Furthermore, our results indicate that our approach is also
suitable for load balancing work-induced imbalances in a realistic
AMR test case.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap175"></a><div class="slot-title">Co-scheduling HPC workloads on cache-partitioned CMP platforms <a href="calendar/submissions/sess122--pap175.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Guillaume Aupy (Inria); Anne Benoit (ENS de Lyon, Georgia Institute of Technology); Brice Goglin (Inria); Loïc Pottier (ENS de Lyon); and Yves Robert (ENS de Lyon, University of Tennessee)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_178_1536846530_13" onclick="$('#vhsjs_view_178_1536846530_13').hide();
                $('#vhsjs_hide_178_1536846530_13').show();
                $('#177_1536846530_13').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_178_1536846530_13" onclick="$('#177_1536846530_13').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_178_1536846530_13').hide();
                $('#vhsjs_view_178_1536846530_13').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="178_1536846530_13" id="177_1536846530_13" style="display: none"><div class="arrow-slidedown"><blockquote>Co-scheduling techniques are used to improve the throughput of applications on chip multiprocessors (CMP), but sharing resources often generates critical interferences. We focus on the interferences in the last level of cache (LLC) and use the Cache Allocation Technology (CAT) recently provided by Intel to partition the LLC and give each co-scheduled application their own cache area. We consider m iterative HPC applications running concurrently and answer to the following questions: (i) how to precisely model the behavior of these applications on the cache partitioned platform? and (ii) how many cores and cache fractions should be assigned to each application to maximize the platform efficiency? Here, platform efficiency is defined as maximizing the performance either globally, or as guaranteeing a fixed ratio of iterations per second for each application. Through extensive experiments using CAT, we demonstrate the impact of cache partitioning when multiple HPC application are co-scheduled onto CMP platforms.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">10:45am-12:15pm</span> <a href="calendar/sessions/sess123.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Minor Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session XII: Scheduling, Elasticity and Energy</div><div class="session-chair">Chair: Felix Wolf (TU Darmstadt)<br /></div><div class="slot-entry"><a name="pap308"></a><div class="slot-title">Leveraging Dependency in Scheduling and Preemption for High Throughput in Data-Parallel Clusters <a href="calendar/submissions/sess123--pap308.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jinwei Liu (Clemson University) and Haiying Shen and Ankur Sarker (University of Virginia)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_180_1536846530_14" onclick="$('#vhsjs_view_180_1536846530_14').hide();
                $('#vhsjs_hide_180_1536846530_14').show();
                $('#179_1536846530_14').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_180_1536846530_14" onclick="$('#179_1536846530_14').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_180_1536846530_14').hide();
                $('#vhsjs_view_180_1536846530_14').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="180_1536846530_14" id="179_1536846530_14" style="display: none"><div class="arrow-slidedown"><blockquote>Task scheduling and preemption are two important functions in data-parallel clusters. Though directed acyclic graph task dependencies are common in data-parallel clusters, previous task scheduling and preemption methods do not fully utilize such task dependency to increase throughput since they simply schedule precedent tasks prior to their dependent tasks or neglect the dependency. We notice that in both scheduling and preemption, choosing a task with more dependent tasks to run allows more tasks to be runnable next, which facilitates to select a task that can more increase throughput. Accordingly, in this paper, we propose a Dependency-aware Scheduling and Preemption system (DSP) to achieve high throughput. First, we build an integer linear programming model to minimize the makespan (i.e., the time when all jobs finish execution) with the consideration of task dependency and deadline, and derive the target server and start time for each task, which can minimize the makespan. Second, we utilize task dependency to determine tasks' priorities for preemption. Finally, we propose a method to reduce the number of unnecessary preemptions that cause more overhead than the throughput gain. Extensive experimental results based on a real cluster and Amazon EC2 cloud service show that DSP achieves much higher throughput compared to existing strategies.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap257"></a><div class="slot-title">Self-Consumption Optimization of Renewable Energy Production in Distributed Clouds <a href="calendar/submissions/sess123--pap257.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Benjamin Camus and Anne Blavette (CNRS), Fanny Dufossé (Inria), and Anne-Cécile Orgerie (CNRS)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_182_1536846530_14" onclick="$('#vhsjs_view_182_1536846530_14').hide();
                $('#vhsjs_hide_182_1536846530_14').show();
                $('#181_1536846530_14').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_182_1536846530_14" onclick="$('#181_1536846530_14').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_182_1536846530_14').hide();
                $('#vhsjs_view_182_1536846530_14').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="182_1536846530_14" id="181_1536846530_14" style="display: none"><div class="arrow-slidedown"><blockquote>The growing appetite of new technologies, such as Internet-of-Things, for Cloud resources leads to an unprecedented energy consumption for these infrastructures. In order to make these energy-hungry distributed systems more sustainable, Cloud providers resort more and more to on-site renewable energy production facilities like photovoltaic panels. Yet, this intermittent and variable electricity production is often uncorrelated with the Cloud consumption induced by its workload. Geographical load balancing, virtual machine (VM) migration and consolidation can be used to exploit multiple Cloud data centers’ locations and their associated photovoltaic panels for increasing their renewable energy consumption. However, these techniques cost energy and network bandwidth, and this limits their utilization. In this paper, we propose to rely on the flexibility brought by Smart Grids to exchange renewable energy between distributed sites and thus, to further increase the overall Cloud’s self-consumption of the locally-produced renewable energy. Our solution is named SCORPIUS: Self-Consumption Optimization of Renewable energy Production In distribUted cloudS. It takes into account telecommunication network constraints and electrical grid requirements to optimize the Cloud’s self-consumption by trading-off between VM migration and renewable energy exchange. Our simulation-based results show that SCORPIUS outperforms existing solutions on various workload traces of production Clouds in terms of both renewable self-consumption and overall energy consumption.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap149"></a><div class="slot-title">Elasticity in Graph Analytics? A Benchmarking Framework for Elastic Graph Processing <a href="calendar/submissions/sess123--pap149.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Alexandru Uta (Vrije Universiteit Amsterdam), Sietse Au and Alexey Ilyushkin (TU Delft), and Alexandru Iosup (Vrije Universiteit Amsterdam)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_184_1536846530_14" onclick="$('#vhsjs_view_184_1536846530_14').hide();
                $('#vhsjs_hide_184_1536846530_14').show();
                $('#183_1536846530_14').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_184_1536846530_14" onclick="$('#183_1536846530_14').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_184_1536846530_14').hide();
                $('#vhsjs_view_184_1536846530_14').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="184_1536846530_14" id="183_1536846530_14" style="display: none"><div class="arrow-slidedown"><blockquote>Graphs are a natural fit for modeling concepts used in solving diverse problems in science, commerce, engineering, and governance. Responding to the diversity of graph data and algorithms, many parallel and distributed graph-processing systems exist. However, until now these platforms use a static model of deployment: they only run on a pre-defined set of machines. This raises many conceptual and pragmatic issues, including misfit with the highly dynamic nature of graph processing, and could lead to resource waste and high operational costs. In contrast, in this work we explore the benefits and drawbacks of the dynamic model of deployment. Building a three-layer benchmarking framework for assessing elasticity in graph analytics, we conduct an in-depth elasticity study of distributed graph processing. Our framework is composed of state-of-the-art workloads, autoscalers, and metrics, derived from the LDBC Graphalytics benchmark and SPEC RG Cloud Group’s elasticity metrics. We uncover the benefits and cost of elasticity in graph processing: while elasticity allows for fine-grained resource management, and does not degrade application performance, we find that graph workloads are sensitive to data migration while leasing or releasing resources. Moreover, we identify non-trivial interactions between scaling policies and graph workloads, which add an extra level of complexity to resource management and scheduling for graph processing.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">1:30pm-3:00pm</span> <a href="calendar/sessions/sess125.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session XIII: Deep Learning</div><div class="session-chair">Chair: Michela Taufer (University of Delaware)<br /></div><div class="slot-entry"><a name="pap138"></a><div class="slot-title">Efficient Training of Convolutional Neural Nets on Large Distributed Systems <a href="calendar/submissions/sess125--pap138.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Dheeraj Sreedhar, Vaibhav Saxena, Yogish Sabharwal, and Ashish Verma (IBM Research - India) and Sameer Kumar (Google Inc.)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_186_1536846530_14" onclick="$('#vhsjs_view_186_1536846530_14').hide();
                $('#vhsjs_hide_186_1536846530_14').show();
                $('#185_1536846530_14').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_186_1536846530_14" onclick="$('#185_1536846530_14').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_186_1536846530_14').hide();
                $('#vhsjs_view_186_1536846530_14').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="186_1536846530_14" id="185_1536846530_14" style="display: none"><div class="arrow-slidedown"><blockquote>Deep Neural Networks (DNNs) have achieved impressive accuracy in many application domains including image classification. Training of DNNs is an extremely compute intensive process and is solved using variants of the stochastic gradient descent (SGD) algorithm. A lot of recent research has focused on improving the performance of DNN training. In this paper, we present optimization techniques to improve the performance of the data parallel synchronous SGD algorithm using the Torch framework: (i) we maintain data in-memory to avoid file I/O overheads, (ii) we propose optimizations to the Torch data parallel table framework that handles multithreading, and (iii) we present MPI optimization to minimize communication overheads. We evaluate the performance of our optimizations on a Power 8 Minsky cluster with 64 nodes and 256 NVidia Pascal P100 GPUs. With our optimizations, we are able to train 90 epochs of the ResNet-50 model on the Imagenet-1k dataset using 256 GPUs in just 48 minutes. This significantly improves on the previously best known performance of training 90 epochs of the ResNet-50 model on the same dataset using the same number of GPUs in 65 minutes. To the best of our knowledge, this is the best known training performance demonstrated for the Imagenet-1k dataset using 256 GPUs.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap206"></a><div class="slot-title">Accelerating Deep Learning Frameworks with Micro-batches <a href="calendar/submissions/sess125--pap206.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Yosuke Oyama (Tokyo Institute of Technology); Tal Ben-Nun and Torsten Hoefler (ETH Zurich); and Satoshi Matsuoka (RIKEN Center for Computational Science, Tokyo Institute of Technology)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_188_1536846530_15" onclick="$('#vhsjs_view_188_1536846530_15').hide();
                $('#vhsjs_hide_188_1536846530_15').show();
                $('#187_1536846530_15').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_188_1536846530_15" onclick="$('#187_1536846530_15').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_188_1536846530_15').hide();
                $('#vhsjs_view_188_1536846530_15').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="188_1536846530_15" id="187_1536846530_15" style="display: none"><div class="arrow-slidedown"><blockquote>cuDNN is a low-level library that provides GPU kernels frequently used in deep learning. Specifically, cuDNN implements several equivalent convolution algorithms, whose performance and memory footprint may vary considerably, depending on the layer dimensions. When an algorithm is automatically selected by cuDNN, the decision is performed on a per-layer basis, and thus it often resorts to slower algorithms that fit the workspace size constraints. We present u-cuDNN, a thin wrapper library for cuDNN that transparently divides layers’ mini-batch computation into multiple micro-batches, both on a single GPU and a heterogeneous set of GPUs. Based on Dynamic Programming and Integer Linear Programming (ILP), u-cuDNN enables faster algorithms by decreasing the workspace requirements. At the same time, u-cuDNN does not decrease the accuracy of the results, effectively decoupling statistical efficiency from the hardware efficiency. We demonstrate the effectiveness of u-cuDNN for the Caffe and TensorFlow frameworks, achieving speedups of 1.63x for AlexNet and 1.30x for ResNet-18 on V100-SXM2 GPU. We also show that u-cuDNN achieves speedups of up to 4.54x, and 1.60x on average for DeepBench's convolutional layers. In a distributed setting, u-cuDNN attains a speedup of 2.20x when training ResNet-18 on a heterogeneous GPU cluster over a single GPU. These results indicate that using micro-batches can seamlessly increase the performance of deep learning, while maintaining the same overall memory footprint.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap277"></a><div class="slot-title">swCaffe: a Parallel Framework for Accelerating Deep Learning Applications on Sunway TaihuLight <a href="calendar/submissions/sess125--pap277.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Liandeng Li, Jiarui Fang, and Haohuan Fu (Tsinghua Univ., National Supercomputing Center in Wuxi); Jinlei Jiang (Tsinghua Univ.); Wenlai Zhao and Conghui He (Tsinghua Univ., National Supercomputing Center in Wuxi); Xin You (Beihang Univ.); and Guangwen Yang (Tsinghua Univ., National Supercomputing Center in Wuxi)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_190_1536846530_15" onclick="$('#vhsjs_view_190_1536846530_15').hide();
                $('#vhsjs_hide_190_1536846530_15').show();
                $('#189_1536846530_15').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_190_1536846530_15" onclick="$('#189_1536846530_15').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_190_1536846530_15').hide();
                $('#vhsjs_view_190_1536846530_15').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="190_1536846530_15" id="189_1536846530_15" style="display: none"><div class="arrow-slidedown"><blockquote>This paper reports our efforts on swCaffe, a high-efficient parallel framework for accelerating deep neural networks (DNNs) training on Sunway TaihuLight, one of the fastest supercomputers in the world that adopts a unique heterogeneous many-core architecture. First, we point out some insightful principles to fully exploit the performance of the innovative many-core architecture. Second, we propose a set of optimization strategies for redesigning a variety of neural network layers based on Caffe. Third, we put forward a topology-aware parameter synchronization scheme to scale the synchronous Stochastic Gradient Descent (SGD) method to multiple processors efficiently. We evaluate our framework by training a variety of widely used neural networks with the ImageNet dataset. On a single node, swCaffe can achieve 23% &#771;119% overall performance compared with Caffe running on K40m GPU. As compared with Caffe on CPU, swCaffe runs 3.04 &#771;7.84× faster on all networks. When training ResNet50 and AlexNet with 1024 nodes, swCaffe can achieve up to 715.45× and 928.15× speedup.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">1:30pm-3:00pm</span> <a href="calendar/sessions/sess126.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Minor Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session XIV: Languages and Programming Models</div><div class="session-chair">Chair: Sebastian Kuckuk (Friedrich-Alexander-Universität Erlangen-Nürnberg)<br /></div><div class="slot-entry"><a name="pap239"></a><div class="slot-title">Charmpy: A Python Parallel Programming Model <a href="calendar/submissions/sess126--pap239.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Juan J. Galvez, Karthik Senthil, and Laxmikant V. Kale (University of Illinois at Urbana-Champaign)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_192_1536846530_15" onclick="$('#vhsjs_view_192_1536846530_15').hide();
                $('#vhsjs_hide_192_1536846530_15').show();
                $('#191_1536846530_15').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_192_1536846530_15" onclick="$('#191_1536846530_15').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_192_1536846530_15').hide();
                $('#vhsjs_view_192_1536846530_15').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="192_1536846530_15" id="191_1536846530_15" style="display: none"><div class="arrow-slidedown"><blockquote>Parallel programming can be extremely challenging. Programming models have been proposed to simplify this task, but wide acceptance of these remains elusive for many reasons, including the demand for greater accessibility and productivity.<br><br>In this paper, we introduce a parallel programming model and framework called Charmpy, based on the Python language. Charmpy builds on Charm++, and runs on top of its C++ runtime. It presents several unique features in the form of a simplified model and API, increased flexibility, and the ability to write everything in Python. Charmpy is a high-level model based on the paradigm of distributed migratable objects. It retains the benefits of the Charm++ runtime, 
including dynamic load balancing, asynchronous execution model with automatic 
overlap of communication and computation, high performance, and scalability from 
laptops to supercomputers. By being Python-based, Charmpy also benefits from 
modern language features, access to popular scientific computing and data 
science software, and interoperability with existing technologies like C, 
Fortran and OpenMP.<br><br>To illustrate the simplicity of the model, we show how to implement a 
distributed parallel map function based on the Master-Worker pattern using 
Charmpy, supporting asynchronous concurrent jobs. We also present 
performance results running stencil code and molecular dynamics mini-apps fully 
written in Python, on Blue Waters and Cori supercomputers. For 
stencil3d, we show performance similar to an equivalent MPI-based program,
and significantly improved performance for imbalanced computations. Using 
Numba to JIT-compile the critical parts of the code, we show performance for 
both mini-apps similar to the equivalent C++ code.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap125"></a><div class="slot-title">And Now for Something Completely Different: Running Lisp on GPUs <a href="calendar/submissions/sess126--pap125.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Tim Suess (Johannes Gutenberg University Mainz, Institute of Computer Science); Nils Doering and André Brinkmann (Johannes Gutenberg University Mainz, ZDV); and Lars Nagel (Loughborough University, Department of Computer Science)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_194_1536846530_15" onclick="$('#vhsjs_view_194_1536846530_15').hide();
                $('#vhsjs_hide_194_1536846530_15').show();
                $('#193_1536846530_15').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_194_1536846530_15" onclick="$('#193_1536846530_15').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_194_1536846530_15').hide();
                $('#vhsjs_view_194_1536846530_15').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="194_1536846530_15" id="193_1536846530_15" style="display: none"><div class="arrow-slidedown"><blockquote>The internal parallelism of compute resources increases permanently, and graphics processing units (GPUs) and other accelerators have been gaining importance in many domains. Researchers from life science, bioinformatics or artificial intelligence, for example, use GPUs to accelerate their computations. However, languages typically used in some of these disciplines often do not benefit from the technical developments because they cannot be executed natively on GPUs. Instead existing programs must be rewritten in other, less dynamic programming languages. On the other hand, the gap in programming features between accelerators and common CPUs shrinks permanently. Since accelerators are becoming more competitive with regard to general computations, they will not be mere special-purpose processors in the future. It is a valid assumption that future GPU generations can be used in a similar or even the same way as CPUs and that compilers or interpreters will be needed for a wider range of computer languages.
We present CuLi, an interactive Lisp interpreter, that performs all computations on a CUDA-capable GPU. The host system is needed only for the input and the output. At the moment, Lisp programs running on CPUs outperform Lisp programs on GPUs, but we present trends indicating that this might change in the future. Our study gives an outlook on the possibility of running Lisp programs or other dynamic programming languages on next-generation accelerators.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap142"></a><div class="slot-title">The AllScale Runtime Application Model <a href="calendar/submissions/sess126--pap142.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Herbert Jordan (University of Innsbruck); Thomas Heller (University of Erlangen-Nuremberg); Philipp Gschwandtner, Peter Zangerl, and Peter Thoman (University of Innsbruck); Dietmar Fey (University of Erlangen-Nuremberg); and Thomas Fahringer (University of Innsbruck)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_196_1536846530_15" onclick="$('#vhsjs_view_196_1536846530_15').hide();
                $('#vhsjs_hide_196_1536846530_15').show();
                $('#195_1536846530_15').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_196_1536846530_15" onclick="$('#195_1536846530_15').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_196_1536846530_15').hide();
                $('#vhsjs_view_196_1536846530_15').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="196_1536846530_15" id="195_1536846530_15" style="display: none"><div class="arrow-slidedown"><blockquote>Contemporary state-of-the-art runtime systems underlying widely utilized general purpose parallel programming languages and libraries like OpenMP, MPI, or OpenCL provide the foundation for accessing the parallel capabilities of modern computing architectures. In the tradition of their respective imperative host languages those runtime systems' main focus is on providing means for the distribution and synchronization of operations --- while the organization and management of manipulated data is left to application developers. Consequently, the distribution of data remains inaccessible to those runtime systems. However, many desirable system-level features depend on a runtime system's ability to exercise control on the distribution of data. Thus, program models underlying traditional systems lack the potential for the support of those features.<br><br>In this paper, we present a novel application model granting parallel runtime systems system-wide control over the distribution of user-defined shared data structures. Our model utilizes the high-level nature of parallel programming languages, in particular, the usage of well-typed data structures and the associated hiding of implementation details from the application developers. By being based on a generalization of such data structures and extending the resulting abstraction with features facilitating the automated management of the distribution of those, our model enables runtime systems to dynamically influence the placement and replication of shared data. This paper covers a rigorous formal description of our application model, as well as details on our prototype implementation and experimental results demonstrating its ability to efficiently and scalably manage various data structures in real-world environments.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">3:30pm-5:00pm</span> <a href="calendar/sessions/sess129.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session XV: Scalable Filesystems</div><div class="session-chair">Chair: Jay Lofstead (Sandia National Laboratories)<br /></div><div class="slot-entry"><a name="pap287"></a><div class="slot-title">PDFE: Flexible Parallel State Machine Replication for Cloud Computing <a href="calendar/submissions/sess129--pap287.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Lihui Wu, Weigang Wu, and Ning Huang (Sun Yat-sen University) and Zhiguang Chen (Sun Yat -sen University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_198_1536846530_16" onclick="$('#vhsjs_view_198_1536846530_16').hide();
                $('#vhsjs_hide_198_1536846530_16').show();
                $('#197_1536846530_16').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_198_1536846530_16" onclick="$('#197_1536846530_16').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_198_1536846530_16').hide();
                $('#vhsjs_view_198_1536846530_16').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="198_1536846530_16" id="197_1536846530_16" style="display: none"><div class="arrow-slidedown"><blockquote>State machine replication (SMR) is a fundamental fault tolerant technique for distributed systems to guarantee consistency among replicas via sequential execution of commands. With the development of cloud computing, parallel SMR has been recently proposed for large scale cloud datacenters. In this paper, we propose PDFE, a novel parallel SMR scheme, which realizes flexible dispatch of parallel ordered commands for parallel executing. In PDFE, the mapping/binding between ordering threads and work threads becomes dynamic, and commands can be dis-patched according to the work load level of different threads. Such flexibility can help achieve two levels of load balancing: load balancing between ordering threads and work threads, and load balancing among work threads. The major challenge in our work lies in the inconsistency problem caused by dynamic changes in command dispatch, and it is addressed by a specially designed mechanism. Compared with existing parallel SMR schemes, PDFE can achieve better load balancing and higher system efficiency. Such advantages are validated by experimental perfor-mance evaluation.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap147"></a><div class="slot-title">IOMiner: Large-scale Analytics Framework for Gaining Knowledge from I/O Logs <a href="calendar/submissions/sess129--pap147.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Teng Wang, Suren Byna, Glenn Lockwood, and Nicholas Wright (Lawrence Berkeley National Laboratory) and Phil Carns and Shane Snyder (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_200_1536846530_16" onclick="$('#vhsjs_view_200_1536846530_16').hide();
                $('#vhsjs_hide_200_1536846530_16').show();
                $('#199_1536846530_16').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_200_1536846530_16" onclick="$('#199_1536846530_16').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_200_1536846530_16').hide();
                $('#vhsjs_view_200_1536846530_16').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="200_1536846530_16" id="199_1536846530_16" style="display: none"><div class="arrow-slidedown"><blockquote>Modern HPC systems are collecting abundant I/O performance
instrumentation. The massive volume and heterogeneity
of this instrumentation has made it difficult to
perform in-depth integrated analysis in a timely manner,
however. To overcome this gap and to allow users to identify
the root causes of poor application I/O performance, we
present IOMiner, an I/O log analytics framework. IOMiner
provides easy to use interfaces for analyzing instrumentation
data, a unified storage schema that hides the heterogeneity
of the raw instrumentation data, and a sweep-line based
algorithm for root cause analysis of poor application I/O
performance. IOMiner is implemented atop Spark to facilitate
efficient, interactive, parallel analytics. We demonstrate
the capabilities of IOMiner by using it to analyze logs
collected on a large-scale production HPC system. Our
analysis techniques not only uncover the root cause of poor
I/O performance in key application case studies but also
provide new insight into HPC I/O workload characterization.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap164"></a><div class="slot-title">A Server-managed Transparent Object Storage Abstraction for HPC <a href="calendar/submissions/sess129--pap164.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jingqing Mu and Jerome Soumagne (The HDF Group); Houjun Tang, Suren Byna, and Quincey Koziol (Lawrence Berkeley National Laboratory); and Richard Warren (The HDF Group)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_202_1536846530_16" onclick="$('#vhsjs_view_202_1536846530_16').hide();
                $('#vhsjs_hide_202_1536846530_16').show();
                $('#201_1536846530_16').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_202_1536846530_16" onclick="$('#201_1536846530_16').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_202_1536846530_16').hide();
                $('#vhsjs_view_202_1536846530_16').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="202_1536846530_16" id="201_1536846530_16" style="display: none"><div class="arrow-slidedown"><blockquote>On the road to exascale, the high-performance computing (HPC) community is seeing the emergence of multi-tier storage systems that take aim at the unprecedented amount of data being produced by scientific applications. Multi-tier storage in HPC systems take advantage of new technologies such as non-volatile memory, burst buffer, etc., and bring the benefit of increased I/O bandwidth. However, existing data management solutions for HPC applications, which were mainly developed for single-tier storage, are no longer suitable at handling the increased level of complexity and currently delegate the task of handling deep memory hierarchies back to the user. This presents programming challenges for application developers and is an opportunity for defining new storage and I/O paradigms. 
We describe a novel object-based data abstraction that takes advantage of deep memory hierarchies by providing a simplified programming interface that enables autonomous, asynchronous, and transparent data movement with a server-driven architecture. Users can define a mapping between application memory and these abstract storage objects, creating a linkage between either all or part of an object’s content without making data copy or transfer calls, avoiding explicit management of complex data movement across multiple storage hierarchies. We evaluate our system by storing plasma physics simulation data with different storage layouts.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">3:30pm-5:00pm</span> <a href="calendar/sessions/sess128.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Minor Hall<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Paper session XVI: Algorithms, Applications and Performance</div><div class="session-chair">Chair: Kenneth B. Kent (University of New Brunswick)<br /></div><div class="slot-entry"><a name="pap272"></a><div class="slot-title">Efficient Algorithms for the Summed Area Tables Primitive on GPUs <a href="calendar/submissions/sess128--pap272.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">peng chen (Tokyo Institute of Technology; AIST-Tokyo Tech Real World Big-Data Computation Open Innovation Laboratory, National Institute of Advanced Industrial Science and Technology); Mohamed Wahib (National Institute of Advanced Industrial Science and Technology, Tokyo, Japan); Shinichiro Takizawa (AIST-Tokyo Tech Real World Big-Data Computation Open Innovation Laboratory, National Institute of Advanced Industrial Science and Technology); Ryousei Takano (National Institute of Advanced Industrial Science and Technology, Tokyo, Japan); and Satoshi Matsuoka (Tokyo Institute of Technology; RIKEN Center for Computational Science, Hyogo, Japan)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_204_1536846530_16" onclick="$('#vhsjs_view_204_1536846530_16').hide();
                $('#vhsjs_hide_204_1536846530_16').show();
                $('#203_1536846530_16').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_204_1536846530_16" onclick="$('#203_1536846530_16').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_204_1536846530_16').hide();
                $('#vhsjs_view_204_1536846530_16').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="204_1536846530_16" id="203_1536846530_16" style="display: none"><div class="arrow-slidedown"><blockquote>Two-dimensional Summed Area Tables (SAT) is a fundamental primitive used in image processing and machine learning applications. We present a collection of optimization methods for computing SAT on CUDA-enabled GPUs. Conventional approaches rely on computing the prefix sum in one dimension in parallel, transposing the matrix, then computing the prefix sum for the other dimension in parallel. Additionally, conventional methods use the scratchpad memory as cache. We propose a collection of algorithms that are scalable with respect to problem size. We use the register cache technique instead of the scratchpad memory and also employ a naive serial scan on the thread level for computing the prefix sum for one of the dimensions. Using a novel transpose-in-registers method we increase the inter-thread parallelism and outperform conventional SAT implementations. In addition, we significantly reduce both the communication between threads and the number of arithmetic instructions. On an Nvidia Pascal P100 GPU and Volta V100, our evaluations demonstrate that our implementations outperform state of the art libraries and yield up to 2.3x and 3.2x speedup over OpenCV and Nvidia NPP libraries, respectively.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap120"></a><div class="slot-title">Relaxing Scalability Limits with Speculative Parallelism in Sequential Monte Carlo <a href="calendar/submissions/sess128--pap120.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Balazs Nemeth, Tom Haber, Jori Liesenborgs, and Wim Lamotte (Universiteit Hasselt)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_206_1536846530_17" onclick="$('#vhsjs_view_206_1536846530_17').hide();
                $('#vhsjs_hide_206_1536846530_17').show();
                $('#205_1536846530_17').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_206_1536846530_17" onclick="$('#205_1536846530_17').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_206_1536846530_17').hide();
                $('#vhsjs_view_206_1536846530_17').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="206_1536846530_17" id="205_1536846530_17" style="display: none"><div class="arrow-slidedown"><blockquote>Sequential Monte Carlo methods are a useful tool to tackle non-linear problems in a Bayesian setting. A target posterior distribution is approximated by moving a set of weighted particles through a sequence of distributions. To counteract degeneracy caused by sequentially changing the underlying distribution, particles occasionally need to be resampled. Deciding if this is necessary requires a reduction operation on the weights after each update. Hence, scalability on a cluster is not only determined by the number of particles used, but also by how well load is balanced. This paper shows how speculative execution in Sequential Monte Carlo with Markov Chain Monte Carlo steps can improve parallel scalability. The key insight is that decisions taken based on the reduction result in each step can be accurately predicted. Consequently, synchronization inherent in the reduction can, in most cases, be avoided, relaxing the limit imposed by load imbalance. Particles are renumbered during resampling to further improve accuracy. Multiple test scenarios, each with different load balance characteristics, are studied empirically on a compute cluster. Tests show that when decisions are predicted correctly, execution time is reduced drastically for use cases with high load imbalance. Furthermore, the maximum theoretical gain, derived from execution characteristics, is compared with the measured improvement to verify that most speculative evaluations are actually useful. If predictions are incorrect, or load is balanced, speculation has no measurable negative impact. Performance is also evaluated in a weak scaling setting on cluster with 36 cores in each system.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap252"></a><div class="slot-title">Predicting Performance Using Collaborative Filtering <a href="calendar/submissions/sess128--pap252.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Shweta Salaria (Tokyo Institute of Technology, AIST-Tokyo Tech Real World Big-Data Computation Open Innovation Laboratory); Aleksandr Drozd and Artur Podobas (Tokyo Institute of Technology); and Satoshi Matsuoka (RIKEN Center for Computational Science, Tokyo Institute of Technology)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_208_1536846530_17" onclick="$('#vhsjs_view_208_1536846530_17').hide();
                $('#vhsjs_hide_208_1536846530_17').show();
                $('#207_1536846530_17').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_208_1536846530_17" onclick="$('#207_1536846530_17').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_208_1536846530_17').hide();
                $('#vhsjs_view_208_1536846530_17').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="208_1536846530_17" id="207_1536846530_17" style="display: none"><div class="arrow-slidedown"><blockquote>Performance prediction of parallel applications across systems becomes increasingly important in today's diverse computing environments. A wide range of choices in execution platforms pose new challenges to researchers in choosing a system which best fits their workloads and administrators in scheduling applications to the best performing systems. While previous studies have employed simulation- or profile-based prediction approaches, such solutions are time-consuming to be deployed on multiple platforms. To address this problem, we use two collaborative filtering techniques to build analytical models which can quickly and accurately predict the performance of workloads across different multicore systems. The first technique leverages information gained from performance observed for certain applications on a subset of systems and use it to discover similarities among applications as well as systems. The second collaborative filtering based model learns latent features of systems and workloads automatically and use these features to characterize the performance of applications on different platforms. We evaluated both the methods using 30 workloads chosen from NAS Parallel Benchmarks, BOTS and Rodinia benchmarking suites on ten different systems. Our results show that such collaborative filtering methods can make predictions with RMSE as low as 0.6 and with an average RMSE of 1.6.</blockquote></div></div></div></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="etype-section"><div class="centered"><a name="evtt102"></a><div class="section-title">Plenary</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">9:00am-10:15am</span> <a href="calendar/sessions/sess101.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Plenary</span><br /><div class="session-title">Welcome and Keynote I</div><div class="session-chair">Chair: Paul H. J. Kelly (Imperial College London)<br /></div><div class="slot-entry"><a name="key102"></a><div class="slot-title">Crossing the Chasm: How to develop weather and climate models for next generation computers</div><div class="slot-authors">Chris Maynard (University of Reading, Met Office)</div><div class="auth-pics-section"></div><div><a class="clickable no-decoration" id="vhsjs_view_212_1536846530_17" onclick="$('#vhsjs_view_212_1536846530_17').hide();
                $('#vhsjs_hide_212_1536846530_17').show();
                $('#211_1536846530_17').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Biography</span></a><a class="clickable no-decoration" id="vhsjs_hide_212_1536846530_17" onclick="$('#211_1536846530_17').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_212_1536846530_17').hide();
                $('#vhsjs_view_212_1536846530_17').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Biography</span></a><div data-display-control="212_1536846530_17" id="211_1536846530_17" style="display: none"><div class="arrow-slidedown"><div><div class="biography-sect">Chris Maynard (University of Reading, Met Office)<blockquote>Chris Maynard has more than 20 years experience developing scientific software for supercomputers in diverse fields such as Quantum Field Theory, Magnetic Materials, Group Theory and Computational Fluid Dynamics. He is leading the software development of the Met Office’s new weather and climate model capable of running on so called Exascale computing architectures.. In January 2018 he joined the University of Reading Computer Science department part-time as an Associate Professor, whilst retaining his former role with the Met Office. His research interests include scientific software development, Programming Models, Performance Modelling and Optimisations and Parallel Computing, as well as new and novel processors necessary to build an Exascale Computer.</blockquote></div></div></div></div></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_210_1536846530_17" onclick="$('#vhsjs_view_210_1536846530_17').hide();
                $('#vhsjs_hide_210_1536846530_17').show();
                $('#209_1536846530_17').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_210_1536846530_17" onclick="$('#209_1536846530_17').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_210_1536846530_17').hide();
                $('#vhsjs_view_210_1536846530_17').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="210_1536846530_17" id="209_1536846530_17" style="display: none"><div class="arrow-slidedown"><blockquote>Energy constraints in processor design have led to radically different computer architectures than for example the ubiquitous x86 architecture. The newer generations of processors are not faster but inherently more parallel. For scientific and numerical computing in general this poses a number of challenges. Rapidly evolving hardware and the absence of a programming model in which to express this explosion of parallelism further compound these problems. This is particular acute for weather and climate codes which are large and complex, with many hundreds of thousands of lines of code, representing a significant investment in scientific development and have long development cycles of a decade or more. In this talk I will outline some approaches to this software challenge, such as developing domain specific languages, being taken by several European groups to cross the chasm between our scientific aspiration to exploit &nbsp;Exascale computing and our ability to port, develop and adapt existing code to new architectures.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">10:45am-12:15pm</span> <a href="calendar/sessions/sess103.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Plenary, Paper</span><br /><div class="session-title">Session I: best papers (Areas 1 and 2)</div><div class="session-chair">Chair: Harald Koestler (University of Erlangen-Nuremberg)<br /></div><div class="slot-entry"><a name="pap207"></a><div class="slot-title">PaSTRI: Error-Bounded Lossy Compression for Two-Electron Integrals in Quantum Chemistry <a href="calendar/submissions/sess103--pap207.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Ali Murat Gok (Northwestern University); Sheng Di, Yuri Alexeev, and Dingwen Tao (Argonne National Laboratory); Vladimir Mironov (Lomonosov Moscow State University); and Xin Liang and Franck Cappello (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_214_1536846530_18" onclick="$('#vhsjs_view_214_1536846530_18').hide();
                $('#vhsjs_hide_214_1536846530_18').show();
                $('#213_1536846530_18').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_214_1536846530_18" onclick="$('#213_1536846530_18').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_214_1536846530_18').hide();
                $('#vhsjs_view_214_1536846530_18').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="214_1536846530_18" id="213_1536846530_18" style="display: none"><div class="arrow-slidedown"><blockquote>Computation of two-electron repulsion integrals is the critical and the most time-consuming step in a typical quantum chemistry simulation. Such calculations have massive computing and storage requirements, which scale as O(n^4) with the size of a chemical system. Compressing the integral's data and storing it on disk can avoid costly recalculation, significantly speeding  the overall quantum chemistry calculations; but it requires a fast  compression algorithm. To  this end, we developed PaSTRI (Pattern Scaling for Two-electron Repulsion Integrals) and implemented the algorithm in the data compression package SZ. PaSTRI leverages the latent pattern features in the integral dataset and optimizes the calculation of the appropriate number of bits required for the storage of the integral. We have evaluated PaSTRI using the integral datasets generated by the quantum chemistry program GAMESS. The results show an excellent 16.8 compression ratio with low overhead, while maintaining 10^-10 absolute precision based on user's requirement.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap161"></a><div class="slot-title">SALaR: Scalable and Adaptive Designs for Large Message Reduction Collectives <a href="calendar/submissions/sess103--pap161.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Mohammadreza Bayatpour, Jahanzeb Maqbool Hashmi, Sourav Chakraborty, Pouya Kousha, Hari Subramoni, and Dhabaleswar K. Panda (Ohio State University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_216_1536846530_18" onclick="$('#vhsjs_view_216_1536846530_18').hide();
                $('#vhsjs_hide_216_1536846530_18').show();
                $('#215_1536846530_18').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_216_1536846530_18" onclick="$('#215_1536846530_18').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_216_1536846530_18').hide();
                $('#vhsjs_view_216_1536846530_18').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="216_1536846530_18" id="215_1536846530_18" style="display: none"><div class="arrow-slidedown"><blockquote>Message Passing Interface (MPI), thus far, has remained a dominant programming models to program large-scale scientific applications. Collective communication operations in MPI are of significant importance due to their communication intensive nature and use in scientific applications. With the emergence of multi-/many-core systems, and rise of deep learning applications, it is important to revisit MPI collectives, particularly MPI Allreduce to exploit vast parallelism offered by modern architectures. In this paper, we take up this challenge and propose Scalable and Adaptive designs for Large message Reduction collectives (SALaR). We focus on MPI Allreduce due to its use in deep learning frameworks and propose new designs that can significantly improve its performance by exploiting architectural features of modern multi-/many-cores in tandem with high- throughput network such as InfiniBand. We also propose a theoretical model to analyze communication and computation cost and use these insights to guide our designs. The evaluation of the proposed SALaR based designs shows significant performance gains over state-of-the-art designs on a wide variety of micro-benchmarks and applications.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">9:00am-10:15am</span> <a href="calendar/sessions/sess111.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Plenary</span><br /><div class="session-title">Announcements and keynote II</div><div class="session-chair">Chair: Dimitrios Nikolopoulos (Queen's University Belfast)<br /></div><div class="slot-entry"><a name="key103"></a><div class="slot-title">Unconventional Computing with Reconfigurable Devices in the Cloud</div><div class="slot-authors">Michaela Blott (Xilinx Research)</div><div class="auth-pics-section"></div><div><a class="clickable no-decoration" id="vhsjs_view_220_1536846530_18" onclick="$('#vhsjs_view_220_1536846530_18').hide();
                $('#vhsjs_hide_220_1536846530_18').show();
                $('#219_1536846530_18').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Biography</span></a><a class="clickable no-decoration" id="vhsjs_hide_220_1536846530_18" onclick="$('#219_1536846530_18').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_220_1536846530_18').hide();
                $('#vhsjs_view_220_1536846530_18').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Biography</span></a><div data-display-control="220_1536846530_18" id="219_1536846530_18" style="display: none"><div class="arrow-slidedown"><div><div class="biography-sect">Michaela Blott (Xilinx Research)<blockquote>Michaela Blott is a Principal Engineer at Xilinx Research, where she is heading a team of international scientists, driving research into new application domains for FPGAs, such as machine learning, and hyperscale deployments. She graduated from the University of Kaiserslautern in Germany and brings over 25 years of experience in computer architecture, FPGA and board design, working in both research institutions (ETH and Bell Labs) as well as development organizations. She is strongly involved with the international research community as technical co-chair of FPL’2018, industry advisor on numerous projects, and serves on numerous technical program committees (DATE, FPGA, FPL, GLOBALSIP, Hipeac).</blockquote></div></div></div></div></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_218_1536846530_18" onclick="$('#vhsjs_view_218_1536846530_18').hide();
                $('#vhsjs_hide_218_1536846530_18').show();
                $('#217_1536846530_18').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_218_1536846530_18" onclick="$('#217_1536846530_18').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_218_1536846530_18').hide();
                $('#vhsjs_view_218_1536846530_18').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="218_1536846530_18" id="217_1536846530_18" style="display: none"><div class="arrow-slidedown"><blockquote>Conventional von-Neumann architectures are suffering from rising power densities and performance scaling is slowing down with next generation technology nodes, while at the same time we are faced with an explosion of data and sky-high compute requirements associated with the roll-out of machine learning algorithms. Reconfigurable logic with FPGAs can tailor the hardware to the application through customized datapaths and memory architectures. Thereby FPGAs can achieve much higher energy efficiencies compared to conventional CPU- and GPU-based solutions. This has stimulated interest in their exploitation within power-hungry data centers with recent benchmarks showing that FPGA-based application acceleration can bring orders of magnitude improvement in regards to performance and performance per Watt compared to their counterparts. During this talk, we broadly characterize a range of applications and explore how through these unconventional customized compute architectures new levels of performance scalability and compute efficiency can be unleashed.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">10:45am-12:15pm</span> <a href="calendar/sessions/sess113.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Plenary, Paper</span><br /><div class="session-title">Session VI: Best papers - Areas 3 and 4</div><div class="session-chair">Chair: Ron Brightwell (Sandia National Laboratories)<br /></div><div class="slot-entry"><a name="pap113"></a><div class="slot-title">Neural network based silent error detector <a href="calendar/submissions/sess113--pap113.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Chen Wang and Nikoli Dryden (University of Illinois at Urbana-Champaign), Franck Cappello (Argonne National Laboratory), and Marc Snir (University of Illinois at Urbana-Champaign)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_222_1536846530_18" onclick="$('#vhsjs_view_222_1536846530_18').hide();
                $('#vhsjs_hide_222_1536846530_18').show();
                $('#221_1536846530_18').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_222_1536846530_18" onclick="$('#221_1536846530_18').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_222_1536846530_18').hide();
                $('#vhsjs_view_222_1536846530_18').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="222_1536846530_18" id="221_1536846530_18" style="display: none"><div class="arrow-slidedown"><blockquote>As we move toward exascale platforms, silent data corruptions (SDC) are likely to occur more frequently. Such errors can lead to incorrect results. Attempts have been made to use generic algorithms to detect such errors. Such detectors have demonstrated high precision and recall for detecting errors, but only if they run immediately after an error has been injected. In this paper, we propose a neural network based detector that can detect SDCs even multiple iterations after they were injected. We have evaluated our detector with 6 FLASH applications and 2 Mantevo mini-apps. Experiments show that our detector can detect more than 89% of SDCs with a false positive rate of less than 2%.</blockquote></div></div></div></div></div><div class="slot-entry"><a name="pap131"></a><div class="slot-title">An Efficient Transformation Scheme for Lossy Data Compression with Point-wise Relative Error Bound <a href="calendar/submissions/sess113--pap131.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Xin Liang (UC Riverside), Sheng Di (Argonne National Laboratory), Dingwen Tao and Zizhong Chen (UC Riverside), and Franck Cappello (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_224_1536846530_18" onclick="$('#vhsjs_view_224_1536846530_18').hide();
                $('#vhsjs_hide_224_1536846530_18').show();
                $('#223_1536846530_18').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_224_1536846530_18" onclick="$('#223_1536846530_18').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_224_1536846530_18').hide();
                $('#vhsjs_view_224_1536846530_18').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="224_1536846530_18" id="223_1536846530_18" style="display: none"><div class="arrow-slidedown"><blockquote>Because of the ever-increasing execution scale of scientific applications, how to store the extremely large volume of data efficiently is becoming a serious issue. A significant reduction of the scientific data size can effectively mitigate the I/O burden and save considerable storage space. Since lossless compressors suffer from limited compression ratios, error-controlled lossy compressors have been studied for years. Existing error-controlled lossy compressors, however,  focus mainly on absolute error bounds, which cannot meet users' diverse demands such as pointwise relative error bounds. Although some of the state-of-the-art lossy compressors support pointwise relative error bound, the compression ratios are generally low because of the limitation in their designs and possible spiky data changes in local data regions. In this work, we propose a novel, efficient approach to perform compression based on the pointwise relative error bound with higher compression ratios than existing solutions provide. Our contribution is threefold. (1) We propose a novel transformation scheme that can transfer the pointwise relative-error-bounded compression problem to an absolute-error-bounded compression issue. We also analyze the practical properties of our transformation scheme both theoretically and experimentally. (2) We implement the proposed technique in two of the most popular absolute-error-bounded lossy compressors, SZ and ZFP. (3) We evaluate our solution using multiple real-world application data across different scientific domains on a supercomputer with up to 4,096 cores and 12 TB of data. Experiments show that our solution achieves over $1.38X$ dumping and $1.31X$ loading performance over the second-best lossy compressor, respectively.</blockquote></div></div></div></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">9:00am-10:15am</span> <a href="calendar/sessions/sess120.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Assembly Hall<br /></span><span class="session-event-type">Plenary</span><br /><div class="session-title">Cluster 2019 presentation and Keynote III</div><div class="session-chair">Chair: Bronis R. de Supinski (Lawrence Livermore National Laboratory)<br /></div><div class="slot-entry"><a name="key101"></a><div class="slot-title">Programmability, Portability and Performance: Challenges and Opportunities in Enabling Usable Systems in the Exascale Era</div><div class="slot-authors">Kathryn O’Brien (IBM)</div><div class="auth-pics-section"></div><div><a class="clickable no-decoration" id="vhsjs_view_228_1536846530_19" onclick="$('#vhsjs_view_228_1536846530_19').hide();
                $('#vhsjs_hide_228_1536846530_19').show();
                $('#227_1536846530_19').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Biography</span></a><a class="clickable no-decoration" id="vhsjs_hide_228_1536846530_19" onclick="$('#227_1536846530_19').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_228_1536846530_19').hide();
                $('#vhsjs_view_228_1536846530_19').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Biography</span></a><div data-display-control="228_1536846530_19" id="227_1536846530_19" style="display: none"><div class="arrow-slidedown"><div><div class="biography-sect">Kathryn O’Brien (IBM)<blockquote>Kathryn O’Brien is a Principal Research Staff Member at IBM T.J. Watson Research Center, where she has worked for over 25 years, holding various technical, managerial and staff positions. Early on, as part of the Research compiler team, she worked on the first automatic parallelizing and vectorizing compilers, and later she managed the compiler team that implemented OpenMP on the CELL heterogeneous architecture. Since that time she has been heavily engaged in the adoption of OpenMP across a range of product and research compiler efforts. Over the last 8 years she has been part of the leadership team driving IBM Research’s Exascale program, where her focus has been on the evolution and development of the broader software programming and tools environment. She is currently the IBM Research Compiler Strategist and Programming Models technical lead, as well as the PI for the CORAL NRE program. As part of this latter role she has been responsible for driving the Programming Model strategy for the IBM CORAL and future HPC and large-scale systems.</blockquote></div></div></div></div></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_226_1536846530_19" onclick="$('#vhsjs_view_226_1536846530_19').hide();
                $('#vhsjs_hide_226_1536846530_19').show();
                $('#225_1536846530_19').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_226_1536846530_19" onclick="$('#225_1536846530_19').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_226_1536846530_19').hide();
                $('#vhsjs_view_226_1536846530_19').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="226_1536846530_19" id="225_1536846530_19" style="display: none"><div class="arrow-slidedown"><blockquote>IBM recently assumed 1st and 3rd position on the Top 500 with the DOE CORAL Summit and Sierra systems respectively. This achievement represents the culmination of a strong technical collaboration between various groups within IBM as well as the CORAL partners. In this talk I will discuss the challenges and opportunities encountered in the course of building a CORAL programming environment to address the critical requirements of Programmability, Performance and Portability. I will also briefly look forward to the ongoing challenges in evolving these programming environment beyond purely HPC, to address the broader and ubiquitous requirements of machine learning applications.</blockquote></div></div></div></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div class="etype-section"><div class="centered"><a name="sstype103"></a><div class="section-title">Workshop</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">9:00am-10:30am</span> <a href="calendar/sessions/sess133.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Workshop</span><br /><div class="session-title">Welcome and Registration</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">11:00am-12:30pm</span> <a href="calendar/sessions/sess135.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Workshop</span><br /><div class="session-title">Workshop sessions I</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">1:30pm-3:00pm</span> <a href="calendar/sessions/sess137.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Workshop</span><br /><div class="session-title">Workshop sessions II</div><span class="slot-entry"></span></div><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">3:30pm-5:00pm</span> <a href="calendar/sessions/sess139.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="session-event-type">Workshop</span><br /><div class="session-title">Workshop sessions III</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div></td></tr></table></div></div><div class="created-date righted">Created 2018-9-13 8:48</div></body></html>
