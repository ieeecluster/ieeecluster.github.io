<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd"><html><head><meta charset="windows-1252" /><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><meta name="viewport" content="width=device-width, initial-scale=1"><title>IEEE Cluster 2019 Program</title><link href="includes/css/jquery-ui.css" rel="stylesheet" type="text/css" /><link href="includes/css/shared_styles.css" rel="stylesheet" type="text/css" /><link href="includes/css/block_styles.css?v=1" rel="stylesheet" type="text/css" /><link href="includes/css/jquery.qtip.min.css" rel="stylesheet" type="text/css" /><link href="includes/css/font-awesome-4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css" /><link href="includes/css/user_generated.css" rel="stylesheet" type="text/css" /><link href="archive_styles.css" rel="stylesheet" type="text/css" /><style>

        .tabs {
            background: #e8ca9b;
        }
        .tabs .divider,
        .tabs .bg_tab {
            background-color: #e8ca9b;
            border-top-color: #e8ca9b;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .tabs .fg_tab {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            color: #CC1A1A;
            text-transform: none;
        }
        .tab_menu_label, .tab_no_menu_label {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .qtip.qtip-rm-tab-menu {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        .filter_bar,
        .filter_bar_w_legend {
            background-color: #C9DDF9;
        }
        .filter_bar_w_legend .instr,
        .filter_bar .instr {
            background-color: #A2C2FC;
        }
        


        .role_stype_bar {
            background-color: #E0AB76;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        


        #footer {
            background-color: #EAEAEA;
            color: #999999;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        #footer a {
            color: #777777;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        


        div.banner_top,
        div.banner_top .site_title,
        div.banner_top .no_logo_banner_right,
        div.logo_banner,
        div.logo_banner .user_name,
        #header {
            background-color: #0066CC;
            color: #FFFFFF;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }

        div.logo_banner .site_title a,
        div.banner_top .site_title a,
        #header #site_title a {
            color: #FFFFFF;
            text-decoration: none;
        }
        


        a:link,
        a:visited,
        a:active,
        .clickable,
        a.clickable,
        a.clickable:link,
        a.clickable:visited,
        a.clickable:active,
        .ttip_object_info_blue,
        .ttip_object_info_blue_no_clone,
        .ttip_object_info_blue_wide,
        .ttip_object_info_blue_wide_no_clone,
        .ttip_object_info_blue_very_wide,
        .ttip_object_info_blue_very_wide_no_clone,
        .ttip_object_info_blue_extra_wide,
        .ttip_object_info_blue_extra_wide_no_clone,
        .ttip_object_info_blue_modal,
        .ttip_object_info_blue_modal_no_clone,
        .colorbox_object_info,
        span.menu_item_label,
        .page_box_print .contents A,
        .page_box_print #footer a   { color: #0000EE; }

        /* Light Links */
        .light_link a,
        .light_arrow,
        .light_link a:link,
        .light_link a:active,
        .light_link a:visited,
        .light_clickable,
        a.light_clickable,
        a.light_clickable:link,
        a.light_clickable:active,
        a.light_clickable:visited   { color: #5088F0; }

        /* user hovers     */
        a:hover,
        .light_link a:hover,
        .light_arrow:hover,
        .light_clickable:hover,
        a.light_clickable:hover,
        .hover_link:hover,
        .ttip_object_info_blue:hover,
        .ttip_object_info_blue_no_clone:hover,
        .ttip_object_info_blue_wide:hover,
        .ttip_object_info_blue_wide_no_clone:hover,
        .ttip_object_info_blue_very_wide:hover,
        .ttip_object_info_blue_very_wide_no_clone:hover,
        .ttip_object_info_blue_extra_wide:hover,
        .ttip_object_info_blue_extra_wide_no_clone:hover,
        .ttip_object_info_blue_modal:hover,
        .ttip_object_info_blue_modal_no_clone:hover,
        .ttip_object_info:hover,
        .ttip_object_info_no_clone:hover,
        .ttip_object_info_wide:hover,
        .ttip_object_info_wide_no_clone:hover,
        .ttip_object_info_very_wide:hover,
        .ttip_object_info_very_wide_no_clone:hover,
        .ttip_object_info_extra_wide:hover,
        .ttip_object_info_extra_wide_no_clone:hover,
        .ttip_object_info_modal:hover,
        .ttip_object_info_modal_no_clone:hover,
        .colorbox_object_info:hover,
        .subtabs .fg_tab:hover div,
        .subtabs .fg_tab:hover A,
        .subtabs .bg_tab:hover,
        .subtabs .bg_tab:hover A        { color: #0000EE; }

        ul.rm_mega_menu li.mega > div,
        ul.rm_mega_menu > li.mega-link > a:hover,
        .disp_details_header,
        .disp_details_sub_header,
        .disp_details I,    /* This is deprecated, since it clashes with font awesome using I tags. */
        .disp_red,
        .disp_label {
            color: #B32626;
        }


        


        #related_col .block-title {
            background-color: #BBBBBB;
            color: #000000;
        }
        #related_col .block-title a {
            color: #0000FF;
        }
        #related_col .block-content {
            background-color: #E5E5E5;
        }
        #related_col .block-content .instr {
            background-color: #D0D0D0;
        }
        #related_col .block-content .odd {
            background-color: #DEDEDE;
        }
        #related_col .block-content .even {
            background: #D0D0D0;
        }
        


        .contents .output_box .title {
            background-color: #D89655;
        }
        .block-content,
        .block2-content,
        .contents .output_box table tr th,
        .contents .output_box {
            background-color: #F9F4E6;
        }
        .output_box_instr,
        .contents .output_box .instr,
        .block .instr,
        .block-content .instr {
            background-color: #F2E0C5;
        }
        .odd,
        .contents .output_box .odd,
        .block-content .odd {
            background-color: #F6EAD5;
        }
        .even,
        .contents .output_box .even,
        .block-content .even {
            background-color: #F2E0C5;
        }
        


        .tabs .fg_tab {
            background-color: #F0DBBC;
            color: #CC1A1A;
            border-bottom-color: #F0DBBC;
        }
        .tab_menu_label:hover, .active .tab_menu_label,
        .tab_no_menu_label:hover,
        .tab_no_menu_label:hover a {
            color: #CC1A1A;
        }
        .subtabs {
            background-color: #F0DBBC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .subtabs .divider,
        .subtabs .bg_tab,
        .subtabs .bg_tab a {
            background-color: #F0DBBC;
            border-top-color: #F0DBBC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        .subtabs .fg_tab {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            color: #CC1A1A;
            text-transform: none;
        }

        /*
        uncomment this to make the subtabs follow the selected tab color instead
        of the link color

        .subtabs .bg_tab:hover a {
            color: #CC1A1A;
        }
        .subtabs .fg_tab:hover a, {
            color: #CC1A1A;
        }
        */

        .subtabs .fg_tab a {
            color: #CC1A1A;
        }
        


        .documentation_box {
            background: #F0E0BC;
            color: #000000;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        body.in_iframe {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .pagedoc {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .page_box {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .page_box_in_iframe {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents_options {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        #top-links {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .fullscreen {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .subtabs .fg_tab {
            border-bottom-color: #FFFFF7;
        }
        .subtabs .fg_tab div {
            background-color: #FFFFF7;
            border-bottom-color: #FFFFF7;
        }
        .fullscreen_schedule {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents input,
        .contents input_box,
        .contents textarea {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .qtip.rm-qtip {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        #cboxContent {
            background-color: #FFFFF7;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        /* For now, use the main site background color for tool tips. */
        .qtip.qtip-rm,
        .qtip.qtip-rm .qtip-titlebar {
            background-color: #FFFFF7;
        }

        /* Not sure where this should live. */
        #actions_col .block-title-text {
            font-size: 15px;
        }
        #related_col .block-title-text {
            font-size: 15px;
        }

        /* For jquery-ui. */
        .ui-widget {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }

        .arrow-slidedown {
            background-color: #FFFFF7;
            color: #000000;
        }
        


        .contents .input .title,
        .contents .input_box .title {
            background-color: #247bf4;
        }
        .contents .input,
        .contents .input_box,
        .contents .input table tr th,
        .contents .input_box table tr th,
        .form .block-content {
            background-color: #f6f9fe;
        }
        .contents .input .instr,
        .contents .input_box .instr,
        .multi_block_button,
        .form .block .instr {
            background-color: #dbe8fa;
        }
        .contents .input .odd,
        .contents .input_box .odd,
        .form .block-content .odd {
            background-color: #edf3fc;
        }
        .contents .input .even,
        .contents .input_box .even,
        .form .block-content .even {
            background-color: #dbe8fa
        }
        


        div.rm_mega_menus_container,
        ul.rm_mega_menu.darker,
        ul.rm_mega_menu > li.mega > a,
        ul.rm_mega_menu.darker > li.mega > a,
        ul.rm_mega_menu > li.mega-link > a,
        ul.rm_mega_menu.darker > li.mega-link > a,
        ul.rm_mega_menu > li.mega-label > span {
            background-color: #777777;
            border-color: #777777;
            color: #FFFFFF;
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
            text-transform: none;
        }
        ul.rm_mega_menu > li.mega.selected > a {
            background-color: #777777;
            border-color: #FFFFFF;
            color: #FFFFFF;
        }
        ul.rm_mega_menu > li.mega:hover > a {
            color: #CC1A1A;
        }
        div.rm_mega_menus_container ul.rm_mega_menu > li.mega > div.menu_dropdown {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        .contents input,
        .contents input_box,
        .contents textarea {
            font-family: Optima, Helvetica, Verdana, "Lucida Grande", Arial, sans-serif;
            font-size: 15px;
        }
        


        #actions_col .block-title {
            background-color: #244A84;
            color: #FFFFFF;
        }
        #actions_col .block-title a {
            color: #FFFFFF;
        }
        #actions_col .block-content {
            background-color: #BDD2F8;
        }
        #actions_col .block-content .instr {
            background-color: #B0CBFC;
        }
        #actions_col .block-content .odd {
            background-color: #C2D6FB;
        }
        #actions_col .block-content .even {
            background: #B0CBFC;
        }
        </style><script src="includes/jquery/jquery-1.8.3.min.js" type="text/javascript"></script><script src="includes/jquery/jquery-ui.min.js" type="text/javascript"></script><script src="includes/jquery/jquery.hoverIntent.minified.js" type="text/javascript"></script><script src="includes/jquery/basic.js" type="text/javascript"></script><script src="includes/jquery/jquery.qtip.min.js" type="text/javascript"></script><script src="includes/jquery/jquery.highlight-4.js"></script><script src="includes/jquery/jquery.instaFilter.js"></script><script type="text/javascript">$(function() { setup_info_links(); });</script><script src="includes/jquery/user_generated.js"></script><script type="text/javascript">
    $(document).ready(function(){
        $date_sections = $("#date-sections");
        $("#program_filter").InstaFilter($date_sections, {
            typing_pause: 500,
            search_unit_selector: ".slot-entry",
            search_unit_ancestor_fields_selector: ".session-title, .session-chair, .room-name"
        });

        // this ensures there is enough vertical space below the search field to keep the
        // page from scrolling during a search
        $date_sections.css("min-height", $(window).height());
    });
    </script></head><body><a name="top"></a><div class="centered"><div><div style="float: left;"></div><div style="float: left; margin-top: 15px; height: 80px;"><span class="page-title">IEEE Cluster 2019 Program</span></div><div style="clear: both;"></div></div></div><br /><div class="centered"><br /><span class="page-links"><a href="at_a_glance.html">Overview</a></span> | <span class="page-links">By Date</span> | <span class="page-links"><a href="by_sub_type.html">By Event Type</a></span> | <span class="page-links"><a href="by_room.html">By Room</a></span> | <span class="page-links"><a href="by_auth.html">Author Index</a></span><br /></div><br /><div id="main-content-box"><div class="centered"><table class="cellspacing10px"><tr><td><div class="anchor-link"><a href="#Monday 7:00am-9:00am">Monday 7:00am-9:00am</a></div></td><td><div class="anchor-link"><a href="#Tuesday 4:00pm-5:00pm">Tuesday 4:00pm-5:00pm</a></div></td><td><div class="anchor-link"><a href="#Wednesday 4:00pm-5:15pm">Wednesday 4:00pm-5:15pm</a></div></td></tr><tr><td><div class="anchor-link"><a href="#Monday 9:00am-3:30pm">Monday 9:00am-3:30pm</a></div></td><td><div class="anchor-link"><a href="#Tuesday 5:00pm-5:30pm">Tuesday 5:00pm-5:30pm</a></div></td><td><div class="anchor-link"><a href="#Wednesday 4:00pm-5:30pm">Wednesday 4:00pm-5:30pm</a></div></td></tr><tr><td><div class="anchor-link"><a href="#Monday 9:00am-6:00pm">Monday 9:00am-6:00pm</a></div></td><td><div class="anchor-link"><a href="#Tuesday 5:30pm-8:00pm">Tuesday 5:30pm-8:00pm</a></div></td><td><div class="anchor-link"><a href="#Wednesday 6:30pm-10:00pm">Wednesday 6:30pm-10:00pm</a></div></td></tr><tr><td><div class="anchor-link"><a href="#Tuesday 7:00am-8:45am">Tuesday 7:00am-8:45am</a></div></td><td><div class="anchor-link"><a href="#Wednesday 7:00am-8:45am">Wednesday 7:00am-8:45am</a></div></td><td><div class="anchor-link"><a href="#Thursday 7:00am-8:45am">Thursday 7:00am-8:45am</a></div></td></tr><tr><td><div class="anchor-link"><a href="#Tuesday 8:45am-9:00am">Tuesday 8:45am-9:00am</a></div></td><td><div class="anchor-link"><a href="#Wednesday 8:45am-9:00am">Wednesday 8:45am-9:00am</a></div></td><td><div class="anchor-link"><a href="#Thursday 8:45am-9:00am">Thursday 8:45am-9:00am</a></div></td></tr><tr><td><div class="anchor-link"><a href="#Tuesday 9:00am-10:00am">Tuesday 9:00am-10:00am</a></div></td><td><div class="anchor-link"><a href="#Wednesday 9:00am-10:00am">Wednesday 9:00am-10:00am</a></div></td><td><div class="anchor-link"><a href="#Thursday 9:00am-10:00am">Thursday 9:00am-10:00am</a></div></td></tr><tr><td><div class="anchor-link"><a href="#Tuesday 10:00am-10:30am">Tuesday 10:00am-10:30am</a></div></td><td><div class="anchor-link"><a href="#Wednesday 10:00am-10:30am">Wednesday 10:00am-10:30am</a></div></td><td><div class="anchor-link"><a href="#Thursday 10:00am-10:30am">Thursday 10:00am-10:30am</a></div></td></tr><tr><td><div class="anchor-link"><a href="#Tuesday 10:30am-11:00am">Tuesday 10:30am-11:00am</a></div></td><td><div class="anchor-link"><a href="#Wednesday 10:30am-11:00am">Wednesday 10:30am-11:00am</a></div></td><td><div class="anchor-link"><a href="#Thursday 10:30am-11:00am">Thursday 10:30am-11:00am</a></div></td></tr><tr><td><div class="anchor-link"><a href="#Tuesday 11:00am-12:30pm">Tuesday 11:00am-12:30pm</a></div></td><td><div class="anchor-link"><a href="#Wednesday 11:00am-12:30pm">Wednesday 11:00am-12:30pm</a></div></td><td><div class="anchor-link"><a href="#Thursday 11:00am-12:30pm">Thursday 11:00am-12:30pm</a></div></td></tr><tr><td><div class="anchor-link"><a href="#Tuesday 12:30pm-2:00pm">Tuesday 12:30pm-2:00pm</a></div></td><td><div class="anchor-link"><a href="#Wednesday 12:30pm-2:00pm">Wednesday 12:30pm-2:00pm</a></div></td><td><div class="anchor-link"><a href="#Thursday 12:30pm-12:45pm">Thursday 12:30pm-12:45pm</a></div></td></tr><tr><td><div class="anchor-link"><a href="#Tuesday 2:00pm-3:30pm">Tuesday 2:00pm-3:30pm</a></div></td><td><div class="anchor-link"><a href="#Wednesday 2:00pm-3:30pm">Wednesday 2:00pm-3:30pm</a></div></td><td></td></tr><tr><td><div class="anchor-link"><a href="#Tuesday 3:30pm-4:00pm">Tuesday 3:30pm-4:00pm</a></div></td><td><div class="anchor-link"><a href="#Wednesday 3:30pm-4:00pm">Wednesday 3:30pm-4:00pm</a></div></td><td></td></tr></table><br /><hr /></div><div class="righted"><input id="program_filter" name="program_filter" placeholder="search" size="40" type="text" /></div><div class="centered" id="date-sections"><table><tr><td align="left"><div><div class="centered"><a name="Monday 7:00am-9:00am"></a><div class="section-title">Monday 7:00am-9:00am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">7:00am-9:00am</span> <a href="calendar/sessions/sess141.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Foyer<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Registration and Badge Pickup</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Monday 9:00am-3:30pm"></a><div class="section-title">Monday 9:00am-3:30pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">9:00am-3:30pm</span> <a href="calendar/sessions/sess142.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Regal<br /></span><span class="session-event-type">Workshop</span><br /><div class="session-title">Workshop on Monitoring and Analysis for High Performance Computing Systems Plus Applications</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Monday 9:00am-6:00pm"></a><div class="section-title">Monday 9:00am-6:00pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Monday</span> <span class="session-time">9:00am-6:00pm</span> <a href="calendar/sessions/sess143.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Workshop</span><br /><div class="session-title">First Extreme-scale Scientific Software Stack Forum (E4S Forum)</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 7:00am-8:45am"></a><div class="section-title">Tuesday 7:00am-8:45am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">7:00am-8:45am</span> <a href="calendar/sessions/sess138.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Foyer<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Registration and Badge Pickup</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 8:45am-9:00am"></a><div class="section-title">Tuesday 8:45am-9:00am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">8:45am-9:00am</span> <a href="calendar/sessions/sess117.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Plenary</span><br /><div class="session-title">Cluster 2019 Opening</div><div class="session-chair">Chair: Ron Brightwell (Sandia National Laboratories); Patrick G. Bridges (University of New Mexico); Martin Schulz (Technical University of Munich); Patrick McCormick (Los Alamos National Laboratory)<br /></div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 9:00am-10:00am"></a><div class="section-title">Tuesday 9:00am-10:00am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">9:00am-10:00am</span> <a href="calendar/sessions/sess101.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Keynote</span><br /><div class="session-title">Keynote 1</div><div class="session-chair">Chair: Martin Schulz (Technical University of Munich)<br /></div><div class="slot-entry"><a name="pec103"></a><div class="slot-title">RAPIDS: Open Source Python Data Science with GPU Acceleration and Dask (Joe Eaton)</div><div class="slot-authors">Joe Eaton (Nvidia)</div><div class="auth-pics-section"></div><div><a class="clickable no-decoration" id="vhsjs_view_4_1569346001_15" onclick="$('#vhsjs_view_4_1569346001_15').hide();
                $('#vhsjs_hide_4_1569346001_15').show();
                $('#3_1569346001_15').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Biography</span></a><a class="clickable no-decoration" id="vhsjs_hide_4_1569346001_15" onclick="$('#3_1569346001_15').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_4_1569346001_15').hide();
                $('#vhsjs_view_4_1569346001_15').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Biography</span></a><div data-display-control="4_1569346001_15" id="3_1569346001_15" style="display: none"><div class="arrow-slidedown"><div><div class="biography-sect">Joe Eaton (Nvidia)<blockquote>Joe Eaton is the Principal System Engineer for Data and Graph Analytics at NVIDIA. He has spent the last 6 years at NVIDIA working on applications of sparse linear algebra: CUDA Libraries, cuSOLVER, cuSPARSE, nvGRAPH and AmgX.  Now 100% on RAPIDS, developing Python APIs, cuML and cuGRAPH. RAPIDS is an end-to-end platform for data science, including IO, ETL, model training, inference and visualization.  Previously he spent 18 years in Oil & Gas reservoir simulation. He is a frequent speaker at SC, GTC, and directly interfaces with engineers and mathematicians across industries.</blockquote></div></div></div></div></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_2_1569346001_12" onclick="$('#vhsjs_view_2_1569346001_12').hide();
                $('#vhsjs_hide_2_1569346001_12').show();
                $('#1_1569346001_12').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_2_1569346001_12" onclick="$('#1_1569346001_12').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_2_1569346001_12').hide();
                $('#vhsjs_view_2_1569346001_12').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="2_1569346001_12" id="1_1569346001_12" style="display: none"><div class="arrow-slidedown"><blockquote>Getting large scale data science done is a tricky business.  There are multiple platforms, the data science algorithms are developed in Python but sometimes deployed in Java, managing the whole workflow is complicated.  RAPIDS is a recent open-source package that tries to bring the Python ease-of-use to large scale data science.  This talk will present RAPIDS, how it can be used to accelerate workflows in Jupyter notebooks based on Pandas and Scikit-Learn, and how a single notebook can scale to cluster deployment with minimal changes.<br><br>Python is often seen as a 'high-productivity' language, but seldom as 'high-performance'.  RAPIDS tried to bridge that gap and bring cutting edge algorithms and performance to the standard Python APIs like Pandas.<br><br><a href="https://clustercomp.org/2019/program/JoeEatonKeynote.pdf" target="_blank">slides</a><br><br>-----<br><br>Joe Eaton is the Principal System Engineer for Data and Graph Analytics at NVIDIA. He has spent the last 6 years at NVIDIA working on applications of sparse linear algebra: CUDA Libraries, cuSOLVER, cuSPARSE, nvGRAPH and AmgX.  Now 100% on RAPIDS, developing Python APIs, cuML and cuGRAPH. RAPIDS is an end-to-end platform for data science, including IO, ETL, model training, inference and visualization.  Previously he spent 18 years in Oil & Gas reservoir simulation. He is a frequent speaker at SC, GTC, and directly interfaces with engineers and mathematicians across industries.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 10:00am-10:30am"></a><div class="section-title">Tuesday 10:00am-10:30am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">10:00am-10:30am</span> <a href="calendar/sessions/sess104.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Top 3 Papers of Cluster 2019 (1/3)</div><div class="session-chair">Chair: Martin Schulz (Technical University of Munich)<br /></div><div class="slot-entry"><a name="pap110"></a><div class="slot-title">Evaluating Burst Buffer Placement in HPC Systems</div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Harsh Khetawat (NCSU), Christopher Zimmer (ORNL), Frank Mueller (NCSU), Scott Atchley and Sudharshan Vazhkudai (ORNL), and Misbah Mubarak (ANL)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_6_1569346001_16" onclick="$('#vhsjs_view_6_1569346001_16').hide();
                $('#vhsjs_hide_6_1569346001_16').show();
                $('#5_1569346001_16').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_6_1569346001_16" onclick="$('#5_1569346001_16').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_6_1569346001_16').hide();
                $('#vhsjs_view_6_1569346001_16').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="6_1569346001_16" id="5_1569346001_16" style="display: none"><div class="arrow-slidedown"><blockquote>Burst buffers are increasingly exploited in contemporary supercomputers to bridge the performance gap between compute and storage systems. The design of burst buffers, particularly the placement of these devices and the underlying network topology, impacts both performance and cost. As the cost of other components such as memory and accelerators is increasing, it is becoming more important that HPC centers provision burst buffers tailored to their workloads.
This work contributes a provisioning system to provide accurate, multi-tenant simulations that model realistic application and storage workloads from HPC systems. The framework aids HPC centers in modeling their workloads against multiple network and burst buffer configurations rapidly. In experiments with our framework, we provide acomparison of representative OLCF I/O workloads against multiple burst buffer designs. We analyze the impact of these designs on latency, I/O phase lengths, contention for network and storage devices, and choice of network topology.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 10:30am-11:00am"></a><div class="section-title">Tuesday 10:30am-11:00am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">10:30am-11:00am</span> <a href="calendar/sessions/sess130.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Wurlitzer<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 11:00am-12:30pm"></a><div class="section-title">Tuesday 11:00am-12:30pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">11:00am-12:30pm</span> <a href="calendar/sessions/sess113.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Deep Learning 1</div><div class="session-chair">Chair: Judy Qiu (Indiana University)<br /></div><div class="slot-entry"><a name="pap300"></a><div class="slot-title">Efficient User-Level Storage Disaggregation for Deep Learning <a href="calendar/submissions/sess113--pap300.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Yue Zhu, Weikuan Yu, and Bing Jiao (Florida State University); Kathryn Mohror and Adam Moody (Lawrence Livermore National Laboratory); and Fahim Chowdhury (Florida State University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_8_1569346001_17" onclick="$('#vhsjs_view_8_1569346001_17').hide();
                $('#vhsjs_hide_8_1569346001_17').show();
                $('#7_1569346001_17').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_8_1569346001_17" onclick="$('#7_1569346001_17').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_8_1569346001_17').hide();
                $('#vhsjs_view_8_1569346001_17').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="8_1569346001_17" id="7_1569346001_17" style="display: none"><div class="arrow-slidedown"><blockquote>On large-scale High-Performance Computing (HPC) systems, applications are provisioned with aggregated resources to meet their peak demands for brief periods. This results in resource underutilization because application requirements vary a lot during execution. This problem is particularly pronounced for deep learning applications that are running on leadership HPC systems with a large pool of burst buffers in the form of flash or non-volatile memory (NVM) devices. In this paper, we examine the I/O patterns of deep neural networks and reveal their critical need of loading many small samples randomly for successful training. We have designed a special Deep Learning File System (DLFS) that provides a thin set of APIs. Particularly, we design the metadata management of DLFS through an in-memory tree-based sample directory and its file services through the user-level SPDK protocol that can disaggregate the capabilities of NVM Express (NVMe) devices to parallel training tasks. Our experimental results show that DLFS can dramatically
improve the throughput of training for deep neural networks on NVMe over Fabric, compared with the kernel-based Ext4 file system. Furthermore, DLFS achieves efficient user-level storage
disaggregation with very little CPU utilization.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap289"></a><div class="slot-title">FluentPS: A Parameter Server Design with Low-frequency Synchronization for Distributed Deep Learning <a href="calendar/submissions/sess113--pap289.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Xin Yao, Xueyu Wu, and Cho-Li Wang (HKU)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_10_1569346001_17" onclick="$('#vhsjs_view_10_1569346001_17').hide();
                $('#vhsjs_hide_10_1569346001_17').show();
                $('#9_1569346001_17').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_10_1569346001_17" onclick="$('#9_1569346001_17').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_10_1569346001_17').hide();
                $('#vhsjs_view_10_1569346001_17').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="10_1569346001_17" id="9_1569346001_17" style="display: none"><div class="arrow-slidedown"><blockquote>With pursuing high accuracy on big datasets, current research prefers designing complex neural networks, which need to maximize data parallelism for short training time. Many distributed deep learning systems, such as MXNet and Petuum, widely use parameter server framework with relaxed synchronization models. Although these models could cost less on each synchronization, its frequency is still high among many workers, e.g., the soft barrier introduced by Stale Synchronous Parallel (SSP) model. In this paper, we introduce our parameter server design, namely FluentPS, which can reduce frequent synchronization and optimize communication overhead in a large-scale cluster. Different from using a single scheduler to manage all parameters' synchronization in some previous designs, our system allows each server to independently adjust schemes for synchronizing its own parameter shard and overlaps the push and pull processes of different servers. We also explore two methods to improve the SSP model: (1) lazy execution of buffered pull requests to reduce the synchronization frequency and (2) a probability-based strategy to pause the fast worker at a probability under SSP condition, which avoids unnecessary waiting of fast workers. We evaluate ResNet-56 with the same large batch size at different cluster scales. While guaranteeing robust convergence, FluentPS gains up to 6x speedup and reduce 93.7% communication costs than PS-Lite. The raw SSP model causes up to 131x delayed pull requests than our improved synchronization model, which can provide fine-tuned staleness controls and achieve higher accuracy.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap280"></a><div class="slot-title">Performance Characterization of DNN Training using TensorFlow and PyTorch on Modern Clusters <a href="calendar/submissions/sess113--pap280.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Arpan Jain, Ammar Ahmad Awan, Quentin Anthony, Hari Subramoni, and Dhabaleswar K. Panda (The Ohio State University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_12_1569346001_18" onclick="$('#vhsjs_view_12_1569346001_18').hide();
                $('#vhsjs_hide_12_1569346001_18').show();
                $('#11_1569346001_18').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_12_1569346001_18" onclick="$('#11_1569346001_18').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_12_1569346001_18').hide();
                $('#vhsjs_view_12_1569346001_18').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="12_1569346001_18" id="11_1569346001_18" style="display: none"><div class="arrow-slidedown"><blockquote>The recent surge of Deep Learning (DL) models and applications can be attributed to the rise in computational resources, availability of large-scale datasets, and accessible DL frameworks such as TensorFlow and PyTorch. Because these frameworks have been heavily optimized for NVIDIA GPUs, several performance characterization studies exist for GPU-based Deep Neural Network (DNN) training. However, there exist very few research studies that focus on CPU-based DNN training. In this paper, we provide an in-depth performance characterization of state-of-the-art DNNs such as ResNet(s) and Inception-v3/v4 on multiple CPU architectures including Intel Xeon Broadwell, three variants of the Intel Xeon Skylake, AMD EPYC, and NVIDIA GPUs like K80, P100, and V100. We provide three key insights: 1) Multi-process (MP) training should be used even for a single-node, because the single-process (SP) approach cannot fully exploit all the cores, 2) Performance of both SP and MP depend on various features such as the number of cores, the processes per node (ppn), and DNN architecture, and 3) There is a non-linear and complex relationship between CPU/system characteristics (core-count, ppn, hyper-threading, etc) and DNN specifications such as inherent parallelism between layers. We further provide a comparative analysis for CPU and GPU-based training and profiling analysis for Horovod. The fastest Skylake we had access to is up to 2.35x better than a K80 GPU but up to 3.32x slower than a V100 GPU. For ResNet-152 training, we observed that MP is up to 1.47x faster than SP and achieves 125x speedup on 128 Skylake nodes.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">11:00am-12:30pm</span> <a href="calendar/sessions/sess110.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Regal<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Parallel Applications Using Alternate Models</div><div class="session-chair">Chair: David Boehme (Lawrence Livermore National Laboratory)<br /></div><div class="slot-entry"><a name="pap192"></a><div class="slot-title">Leveraging Task-Based Polar Decomposition Using PARSEC on Massively Parallel Systems <a href="calendar/submissions/sess110--pap192.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Dalal Sukkari (KAUST), Mathieu Faverge (INRIA), and Hatem Ltaief and David Keyes (KAUST)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_14_1569346001_18" onclick="$('#vhsjs_view_14_1569346001_18').hide();
                $('#vhsjs_hide_14_1569346001_18').show();
                $('#13_1569346001_18').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_14_1569346001_18" onclick="$('#13_1569346001_18').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_14_1569346001_18').hide();
                $('#vhsjs_view_14_1569346001_18').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="14_1569346001_18" id="13_1569346001_18" style="display: none"><div class="arrow-slidedown"><blockquote>This paper describes how to leverage task-based implementation of the polar decomposition on massively parallel systems using the PARSEC dynamic runtime system. Based on a formulation of the iterative QR Dynamically-Weighted Halley (QDWH) algorithm, our novel implementation reduces data traffic while exploiting high concurrency from the underlying hardware architecture. First, we replace the most time-consuming classical QR factorization phase with a new hierarchical variant, customized for the specific structure of the matrix during the QDWH iterations. The newly developed hierarchical QR for QDWH exploits not only the matrix structure, but also shortens the length of the critical path to maximize hardware occupancy. We then deploy PARSEC to seamlessly orchestrate, pipeline, and track the data dependencies of the various linear algebra building blocks involved during the iterative QDWH algorithm. PARSEC enables to overlap communications with computations thanks to its asynchronous scheduling of fine-grained computational tasks. It employs look-ahead techniques to further expose parallelism, while actively pursuing the critical path. In addition, we identify synergistic opportunities between the task-based QDWH algorithm and the PARSEC framework. We exploit them during the hierarchical QR factorization to enforce a locality- aware task execution. The latter feature permits to minimize the expensive inter-node communication, which represents one of the main bottlenecks for scaling up applications on challenging distributed-memory systems. We report numerical accuracy and performance results using well and ill-conditioned matrices. The benchmarking campaign reveals up to 2X performance speedup against the existing state-of-the-art implementation for the polar decomposition on 36,864 cores.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap205"></a><div class="slot-title">Engineering a Distributed Histogram Sort <a href="calendar/submissions/sess110--pap205.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Roger Kowalewski, Pascal Jungblut, and Karl Fuerlinger (LMU Munich)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_16_1569346001_19" onclick="$('#vhsjs_view_16_1569346001_19').hide();
                $('#vhsjs_hide_16_1569346001_19').show();
                $('#15_1569346001_19').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_16_1569346001_19" onclick="$('#15_1569346001_19').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_16_1569346001_19').hide();
                $('#vhsjs_view_16_1569346001_19').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="16_1569346001_19" id="15_1569346001_19" style="display: none"><div class="arrow-slidedown"><blockquote>Sorting is one of the most critical non-numerical algorithms and covers use cases in a wide spectrum of scientific applications. Although we can build upon excellent research over the last decades, scaling to thousands of processing units on modern many-core architectures reveals a gap between theory and practice.  We adopt ideas of the well-known quickselect and sample sort algorithms to minimize data movement. Our evaluation demonstrates that we can keep up with recently proposed distribution sort algorithms in large-scale experiments, without any assumptions on the input keys. Additionally, our implementation outperforms an efficient multi-threaded merge sort on a single node. Our implementation is based on a C++ PGAS approach with an STL-like interface and can easily be integrated into many application codes.  As part of the presented experiments, we further reveal challenges with multi-threaded MPI and one-sided communication.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap182"></a><div class="slot-title">Asynchronous Task-Based Execution of the Reverse Time Migration for the Oil and Gas Industry <a href="calendar/submissions/sess110--pap182.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Amani Alonazi and Hatem Ltaief (KAUST); Issam Said (NVIDIA); Samuel Thibault (University of Bordeaux, LaBRI  INRIA Bordeaux Sud-Ouest); and David Keyes (KAUST)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_18_1569346001_19" onclick="$('#vhsjs_view_18_1569346001_19').hide();
                $('#vhsjs_hide_18_1569346001_19').show();
                $('#17_1569346001_19').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_18_1569346001_19" onclick="$('#17_1569346001_19').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_18_1569346001_19').hide();
                $('#vhsjs_view_18_1569346001_19').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="18_1569346001_19" id="17_1569346001_19" style="display: none"><div class="arrow-slidedown"><blockquote>We propose a new framework for deploying Reverse Time Migration (RTM) simulations on distributed-memory systems equipped with multiple GPUs. Our software, TB-RTM, infrastructure engine relies on the StarPU dynamic runtime system to orchestrate the asynchronous scheduling of RTM computational tasks on the underlying resources. Besides dealing with the challenging hardware heterogeneity, TB-RTM supports tasks with different workload characteristics, which stress disparate components of the hardware system. RTM is challenging in that it operates intensively at both ends of the memory hierarchy, with compute kernels running at the highest level of the memory system, possibly in GPU main memory, while I/O kernels are saving solution data to fast storage. We consider how to span the wide performance gap between the two extreme ends of the memory system, i.e., GPU memory and fast storage, on which large-scale RTM simulations routinely execute. To maximize hardware occupancy while maintaining high memory bandwidth throughout the memory subsystem, our framework presents the new out-of-core (OOC) feature from StarPU to prefetch data solutions in and out not only from/to the GPU/CPU main memory but also from/to the fast storage system. The OOC technique may trigger opportunities for overlapping expensive data movement with computations. TB-RTM framework addresses this challenging problem of heterogeneity with a systematic approach that is oblivious to the targeted hardware architectures. Our resulting RTM framework can effectively be deployed on massively parallel GPU-based systems, while delivering performance scalability up to 500 GPUs.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 12:30pm-2:00pm"></a><div class="section-title">Tuesday 12:30pm-2:00pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">12:30pm-2:00pm</span> <a href="calendar/sessions/sess131.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Other<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Lunch (on your own)</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 2:00pm-3:30pm"></a><div class="section-title">Tuesday 2:00pm-3:30pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">2:00pm-3:30pm</span> <a href="calendar/sessions/sess114.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Deep Learning 2</div><div class="session-chair">Chair: Hatem Ltaief (KAUST)<br /></div><div class="slot-entry"><a name="pap165"></a><div class="slot-title">A Quantitative Study of Deep Learning Training on Heterogeneous Supercomputers <a href="calendar/submissions/sess114--pap165.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Jingoo Han (Virginia Tech), Luna Xu (IBM Research), M. Mustafa Rafique (Rochester Institute of Technology), Ali R. Butt (Virginia Tech), and Seung-Hwan Lim (Oak Ridge National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_20_1569346001_21" onclick="$('#vhsjs_view_20_1569346001_21').hide();
                $('#vhsjs_hide_20_1569346001_21').show();
                $('#19_1569346001_21').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_20_1569346001_21" onclick="$('#19_1569346001_21').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_20_1569346001_21').hide();
                $('#vhsjs_view_20_1569346001_21').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="20_1569346001_21" id="19_1569346001_21" style="display: none"><div class="arrow-slidedown"><blockquote>Deep learning (DL) has become a key technique for solving complex problems in scientific research and discovery. DL training for science is substantially challenging because it has to deal with massive quantities of multi-dimensional data. High-performance computing (HPC) supercomputers are increasingly being employed for meeting the exponentially growing demand for DL. Multiple GPUs and high-speed interconnect network are needed for supporting DL on HPC systems. However, the excessive use of GPUs without considering effective benefits leads to inefficient resource utilization of these expensive setups. In this paper, we conduct a quantitative analysis to gauge the efficacy of DL workloads on the latest HPC system and identify viability of next-generation DL-optimized heterogeneous supercomputers for enabling researchers to develop more efficient resource management and distributed DL middleware. We evaluate well-known DL models with large-scale datasets using the popular TensorFlow framework, and provide a thorough evaluation including scalability, accuracy, variability, storage resource, GPU-GPU/GPU-CPU data transfer, and GPU utilization. Our analysis reveals that the latest heterogeneous supercomputing cluster shows varying performance trend as compared to the existing literature for single- and multi-node training. To the best of our knowledge, this is the first work to conduct such a quantitative and comprehensive study of DL training on a supercomputing system with multiple GPUs.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap303"></a><div class="slot-title">Parallelizing Training of Deep Generative Models on Massive Scientific Datasets <a href="calendar/submissions/sess114--pap303.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Sam Ade Jacobs, Brian Van Essen, Tim Moon, Jae Seung Yeom, David Hysom, Brian Spears, Rushil Anirudh, Jayaraman Thiagaranjan, Shusen Liu, Jim Gaffney, Peer-Timo Bremer, Tom Benson, Peter Robinson, and Luc Peterson (Lawrence Livermore National Lab)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_22_1569346001_21" onclick="$('#vhsjs_view_22_1569346001_21').hide();
                $('#vhsjs_hide_22_1569346001_21').show();
                $('#21_1569346001_21').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_22_1569346001_21" onclick="$('#21_1569346001_21').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_22_1569346001_21').hide();
                $('#vhsjs_view_22_1569346001_21').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="22_1569346001_21" id="21_1569346001_21" style="display: none"><div class="arrow-slidedown"><blockquote>Training deep neural networks on large scientific data is a challenging task that requires enormous compute power, especially if no pre-trained models exist to initialize the process. We present a novel tournament method to train traditional as well as generative adversarial networks built on LBANN, a scalable deep learning framework optimized for HPC systems. LBANN combines multiple levels of parallelism and exploits some of the worlds largest supercomputers.
We demonstrate our framework by creating a complex predictive model based on multi-variate data from high-energy-density physics containing hundreds of millions of images and hundreds of millions of scalar values derived from tens of millions of simulations of inertial confinement fusion. Our approach combines an HPC workflow and extends LBANN with optimized data ingestion and the new tournament-style training algorithm to produce a scalable neural network architecture using a CORAL-class supercomputer. Experimental results show that 64 trainers (1024 GPUs) achieve a speedup of 70.2 over a single trainer (16 GPUs) baseline, and an effective 109% parallel efficiency.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap253"></a><div class="slot-title">Quantifying the Impact of Memory Errors in Deep Learning <a href="calendar/submissions/sess114--pap253.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Zhao Zhang, Lei Huang, Ruizhu Huang, and Weijia Xu (TACC) and Daniel S. Katz (University of Illinois)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_24_1569346001_21" onclick="$('#vhsjs_view_24_1569346001_21').hide();
                $('#vhsjs_hide_24_1569346001_21').show();
                $('#23_1569346001_21').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_24_1569346001_21" onclick="$('#23_1569346001_21').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_24_1569346001_21').hide();
                $('#vhsjs_view_24_1569346001_21').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="24_1569346001_21" id="23_1569346001_21" style="display: none"><div class="arrow-slidedown"><blockquote>The use of deep learning (DL) on HPC resources has become common as scientists  explore and exploit DL methods to solve domain problems.
On the other hand, in the coming exascale computing era, a high error rate is expected to be problematic for most HPC applications.
The impact of errors on DL applications, especially DL training, remains unclear given their stochastic nature. 
In this paper, we focus understanding DL training applications on HPC in the presence of silent data corruption.
Specifically, we design and perform a quantification study with three representative applications by manually injecting silent data corruption errors (SDCs) across the design space and compare training results with the error-free baseline.
The results show only 0.61--1.76% of SDCs cause training failures, and taking the SDC rate in modern hardware into account, the actual chance of a failure is one in thousands to millions of executions.
With this quantitatively measured impact, computing centers can make rational design decisions based on their application portfolio, the acceptable failure rate, and financial constraints; for example, they might determine their confidence in the correctness of training results performed processors without error correction code (ECC) RAM.
We also discover that over 75-90% of the SDCs that cause catastrophic errors can be easily detected by a training loss in the next iteration.
Thus we propose this error-aware software solution to correct catastrophic errors, as it has significantly lower time and space overhead compared to algorithm-based fault-tolerance (ABFT) and ECC.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">2:00pm-3:30pm</span> <a href="calendar/sessions/sess122.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Regal<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Workflows</div><div class="session-chair">Chair: Sameer Shende (University of Oregon; ParaTools, Inc.)<br /></div><div class="slot-entry"><a name="pap187"></a><div class="slot-title">NORNS: Extending Slurm to Support Data-Driven Workflows through Asynchronous Data Staging <a href="calendar/submissions/sess122--pap187.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Alberto Miranda (Barcelona Supercomputing Center); Adrian Jackson (EPCC, The University of Edinburgh); Tommaso Tocci (Barcelona Supercomputing Center); Iakovos Panourgias (EPCC, The University of Edinburgh); and Ramon Nou (Barcelona Supercomputing Center)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_26_1569346001_22" onclick="$('#vhsjs_view_26_1569346001_22').hide();
                $('#vhsjs_hide_26_1569346001_22').show();
                $('#25_1569346001_22').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_26_1569346001_22" onclick="$('#25_1569346001_22').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_26_1569346001_22').hide();
                $('#vhsjs_view_26_1569346001_22').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="26_1569346001_22" id="25_1569346001_22" style="display: none"><div class="arrow-slidedown"><blockquote>As HPC systems move into the Exascale era, parallel file systems are struggling to keep up with the I/O requirements from data-intensive problems. While the inclusion of burst buffers has helped to alleviate this by improving I/O performance, it has also increased the complexity of the I/O hierarchy by adding additional storage layers each with its own semantics. This forces users to explicitly manage data movement between the different storage layers, which, coupled with the lack of interfaces to communicate data dependencies between jobs in a data-driven workflow, prevents resource schedulers from optimizing these transfers to benefit the cluster's overall performance. This paper proposes several extensions to job schedulers, prototyped using the Slurm scheduling system, to enable users to appropriately express the data dependencies between the different phases in their processing workflows. It also introduces a new service for asynchronous data staging called NORNS that coordinates with the job scheduler to orchestrate data transfers to achieve better resource utilization. Our evaluation shows that a workflow-aware Slurm exploits node-local storage more effectively, reducing the filesystem I/O contention and improving job running times.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap224"></a><div class="slot-title">Leveraging Machine Learning for Anticipatory Data Delivery in Extreme Scale In-situ Workflows <a href="calendar/submissions/sess122--pap224.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Pradeep Subedi, Philip E. Davis, and Manish Parashar (Rutgers University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_28_1569346001_22" onclick="$('#vhsjs_view_28_1569346001_22').hide();
                $('#vhsjs_hide_28_1569346001_22').show();
                $('#27_1569346001_22').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_28_1569346001_22" onclick="$('#27_1569346001_22').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_28_1569346001_22').hide();
                $('#vhsjs_view_28_1569346001_22').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="28_1569346001_22" id="27_1569346001_22" style="display: none"><div class="arrow-slidedown"><blockquote>Extreme scale scientific workflows are composed of multiple applications that exchange data at runtime. Several data-related challenges are limiting the potential impact of such workflows. While data staging and in-situ models of execution have emerged as approaches to address data-related costs at extreme scales, increasing data volumes and complex data exchange patterns impact the effectiveness of such approaches. In this paper, we design and implement DESTINY, which is an autonomic data delivery mechanism for staging-based in-situ workflows. DESTINY dynamically learns the data access patterns of scientific workflow applications and leverages these patterns to decrease data access costs. Specifically, DESTINY uses machine learning techniques to anticipate future data accesses, proactively packages and delivers the data necessary to satisfy these requests as close to the consumer as possible and, when data staging processes and consumer processes are colocated, removes the need for inter-process communication by making these data available to the consumer as shared-memory objects. When consumer processes reside on nodes other than staging nodes, the data is packaged and stored in a format the client will likely access in future. This amortizes expensive data discovery and assembly operations typically associated with data staging. We experimentally evaluate the performance and scalability of DESTINY on leadership class platforms using synthetic applications and the S3D combustion workflow. We demonstrate that DESTINY is scalable and can achieve a reduction of up to 75% in read response time as compared to in-memory staging service for production scientific workflows.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap188"></a><div class="slot-title">Harmony: An Approach for Geo-distributed Processing of Big-Data Applications <a href="calendar/submissions/sess122--pap188.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Han Zhang (National University of Singapore), Lavanya Ramapantulu (International Institute of Information Technology), and Yong Meng Teo (National University of Singapore)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_30_1569346001_23" onclick="$('#vhsjs_view_30_1569346001_23').hide();
                $('#vhsjs_hide_30_1569346001_23').show();
                $('#29_1569346001_23').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_30_1569346001_23" onclick="$('#29_1569346001_23').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_30_1569346001_23').hide();
                $('#vhsjs_view_30_1569346001_23').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="30_1569346001_23" id="29_1569346001_23" style="display: none"><div class="arrow-slidedown"><blockquote>Big-data application processing is increasingly geo-distributed, a paradigm shift from the traditional cluster-based processing frameworks. As the communication time for data movement across geo-distributed data centers is not a design criterion for traditional cluster-based processing approaches, there are research gaps in the algorithms used for staging and scheduling big-data applications for geo-distributed clusters. We address these gaps by proposing Harmony, an approach consisting of both staging and scheduling strategies to minimize an application's total execution time.<br><br>	The staging strategy of Harmony exploits the intra-stage parallelism by having concurrent operators within a stage in contrast to the traditional Apache spark which uses fine-grained stages, thus reducing the computation time within each stage. Secondly, the scheduling strategy of Harmony reduces data transfers between geo-distributed data centers by exploiting data locality and thus reducing communication time and total execution time. The proposed approach Harmony achieves a speedup of two times with respect to geo-distributed Apache Spark. In addition, Harmony achieves a speedup of 1.6 times and 2.1 times when compared with the state-of-the-art framework Iridium for geo-distributed analytics over five locations with uniform and non-uniform network link bandwidths respectively.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 3:30pm-4:00pm"></a><div class="section-title">Tuesday 3:30pm-4:00pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">3:30pm-4:00pm</span> <a href="calendar/sessions/sess132.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Wurlitzer<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 4:00pm-5:00pm"></a><div class="section-title">Tuesday 4:00pm-5:00pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">4:00pm-5:00pm</span> <a href="calendar/sessions/sess126.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Machine Learning</div><div class="session-chair">Chair: Ann Gentile (Sandia National Laboratories)<br /></div><div class="slot-entry"><a name="pap155"></a><div class="slot-title">HarpGBDT: Optimizing Gradient Boosting Decision Tree for Parallel Efficiency <a href="calendar/submissions/sess126--pap155.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Bo Peng and Judy Qiu (Indiana University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_32_1569346001_24" onclick="$('#vhsjs_view_32_1569346001_24').hide();
                $('#vhsjs_hide_32_1569346001_24').show();
                $('#31_1569346001_24').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_32_1569346001_24" onclick="$('#31_1569346001_24').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_32_1569346001_24').hide();
                $('#vhsjs_view_32_1569346001_24').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="32_1569346001_24" id="31_1569346001_24" style="display: none"><div class="arrow-slidedown"><blockquote>Gradient Boosting Decision Tree (GBDT) is a widely used machine learning algorithm, whose 
training involves both irregular computation and random memory access and is challenging for system optimizations. In this paper, we conduct a comprehensive performance analysis of two state-of-the-art systems, XGBoost and LightGBM. They represent two typical parallel implementations for GBDT; one is data parallel and the other one is parallel over features. Substantial thread synchronization overhead, as well as the inefficiency of random memory access, is identified. We propose HarpGBDT, a new GBDT system designed from the perspective of parallel efficiency optimization. Firstly, we adopt a new tree growth method that selects the top K candidates of tree nodes to enable the use of more levels of parallelism without sacrificing the algorithm's accuracy. Secondly, we organize the training data and model data in blocks and propose a block-wise approach as a general model that enables the exploration of various parallelism options. Thirdly, we propose a mixed mode to utilize the advantages of a different mode of parallelism in different phases of training. By changing the configuration of the block size and parallel mode, HarpGBDT is able to attain better parallel efficiency. By extensive experiments on four datasets with different statistical characteristics on the Intel(R) Xeon(R) E5-2699 server, HarpGBDT on average performs 8x faster than XGBoost and 2.6x faster than LightGBM.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap144"></a><div class="slot-title">Training Google Neural Machine Translation on an Intel CPU Cluster <a href="calendar/submissions/sess126--pap144.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Dhiraj Kalamkar, Kunal Banerjee, Sudarshan Srinivasan, Srinivas Sridharan, Evangelos Georganas, Mikhail Smorkalov, Cong Xu, and Alexander Heinecke (Intel)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_34_1569346001_24" onclick="$('#vhsjs_view_34_1569346001_24').hide();
                $('#vhsjs_hide_34_1569346001_24').show();
                $('#33_1569346001_24').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_34_1569346001_24" onclick="$('#33_1569346001_24').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_34_1569346001_24').hide();
                $('#vhsjs_view_34_1569346001_24').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="34_1569346001_24" id="33_1569346001_24" style="display: none"><div class="arrow-slidedown"><blockquote>Google's neural machine translation (GNMT) is state-of-the-art recurrent neural network (RNN/LSTM) based language translation application. It is computationally more demanding than well-studied convolutional neural networks (CNNs). Also, in contrast to CNNs, RNNs heavily mix compute and memory bound layers which requires careful tuning on a latency machine to optimally use fast on-die memories for best single processor performance. Additionally, due to massive compute demand, it is essential to distribute the entire workload among several processors and even compute nodes. To the best of our knowledge, this is the first work which attempts to scale this application on an Intel CPU cluster. Our CPU-based GNMT optimization, the first of its kind, achieves this by the following steps: (i) we choose a monolithic long short-term memory (LSTM) cell implementation from LIBXSMM library (specifically tuned for CPUs) and integrate it into TensorFlow, (ii) we modify GNMT code to use fused time step LSTM op for the encoding stage, (iii) we combine Horovod and Intel MLSL scaling libraries for improved performance on multiple nodes, and (iv) we extend the bucketing logic for grouping similar length sentences together to multiple nodes for achieving load balance across multiple ranks. In summary, we demonstrate that due to these changes we are able to outperform Google's stock CPU-based GNMT implementation by ~2x on single node and potentially enable more than 25x speedup using 16 node CPU cluster.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">4:00pm-5:00pm</span> <a href="calendar/sessions/sess121.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Regal<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Clustering</div><div class="session-chair">Chair: Ron Brightwell (Sandia National Laboratories)<br /></div><div class="slot-entry"><a name="pap195"></a><div class="slot-title">MuDBSCAN: An Exact Scalable DBSCAN Algorithm for Big Data Exploiting Spatial Locality</div><div class="slot-authors">Aditya Sarma, Poonam Goyal, Sonal Kumari, Anand Wani, Jagat Sesh Challa, Saiyedul Islam, and Navneet Goyal (Birla Institute of Technology & Science, Pilani)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_36_1569346001_25" onclick="$('#vhsjs_view_36_1569346001_25').hide();
                $('#vhsjs_hide_36_1569346001_25').show();
                $('#35_1569346001_25').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_36_1569346001_25" onclick="$('#35_1569346001_25').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_36_1569346001_25').hide();
                $('#vhsjs_view_36_1569346001_25').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="36_1569346001_25" id="35_1569346001_25" style="display: none"><div class="arrow-slidedown"><blockquote>DBSCAN is one of the most popular and effective clustering algorithms that is capable of identifying arbitrary-shaped clusters and noise efficiently. However, its super-linear complexity makes it infeasible for applications involving clustering of Big Data. A major portion of the computation time of DBSCAN is taken up by the neighborhood queries, which becomes a bottleneck to its performance. We address this issue in our proposed micro-cluster based DBSCAN algorithm, MuDBSCAN, which identifies core-points even without performing neighbourhood queries and becomes instrumental in reducing the run-time of the algorithm. It also significantly reduces the computation time per neighbourhood query while producing exact DBSCAN clusters. Moreover, the micro-cluster based solution makes it scalable for high dimensional data. We also propose a highly scalable distributed implementation of MuDBSCAN, MuDBSCAN-D, to exploit a commodity cluster infrastructure. Experimental results demonstrate tremendous improvements in performance of our proposed algorithms as compared to their respective state-of-the-art solutions for various standard datasets. MuDBSCAN-D is an exact parallel solution for DBSCAN which is capable of processing massive amounts of data efficiently (1 billion data points in 41 minutes on a 32 node cluster), while producing a clustering that is same as that of traditional DBSCAN.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 5:00pm-5:30pm"></a><div class="section-title">Tuesday 5:00pm-5:30pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">5:00pm-5:30pm</span> <a href="calendar/sessions/sess127.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Poster</span><br /><div class="session-title">Poster Blitz</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Tuesday 5:30pm-8:00pm"></a><div class="section-title">Tuesday 5:30pm-8:00pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Tuesday</span> <span class="session-time">5:30pm-8:00pm</span> <a href="calendar/sessions/sess128.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Wurlitzer<br /></span><span class="session-event-type">Poster</span><br /><div class="session-title">Poster Session and Reception</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 7:00am-8:45am"></a><div class="section-title">Wednesday 7:00am-8:45am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">7:00am-8:45am</span> <a href="calendar/sessions/sess139.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Foyer<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Registration and Badge Pickup</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 8:45am-9:00am"></a><div class="section-title">Wednesday 8:45am-9:00am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">8:45am-9:00am</span> <a href="calendar/sessions/sess118.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Plenary</span><br /><div class="session-title">Announcements</div><div class="session-chair">Chair: Ron Brightwell (Sandia National Laboratories); Patrick G. Bridges (University of New Mexico)<br /></div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 9:00am-10:00am"></a><div class="section-title">Wednesday 9:00am-10:00am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">9:00am-10:00am</span> <a href="calendar/sessions/sess102.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Keynote</span><br /><div class="session-title">Keynote 2</div><div class="session-chair">Chair: Patrick McCormick (Los Alamos National Laboratory)<br /></div><div class="slot-entry"><a name="pec101"></a><div class="slot-title">Big Data Spatiotemporal Analytics - Trends, Characteristics and Applications (Sangmi Lee Pallickara)</div><div class="slot-authors">Sangmi Pallickara (Colorado State University)</div><div class="auth-pics-section"></div><div><a class="clickable no-decoration" id="vhsjs_view_40_1569346001_27" onclick="$('#vhsjs_view_40_1569346001_27').hide();
                $('#vhsjs_hide_40_1569346001_27').show();
                $('#39_1569346001_27').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Biography</span></a><a class="clickable no-decoration" id="vhsjs_hide_40_1569346001_27" onclick="$('#39_1569346001_27').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_40_1569346001_27').hide();
                $('#vhsjs_view_40_1569346001_27').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Biography</span></a><div data-display-control="40_1569346001_27" id="39_1569346001_27" style="display: none"><div class="arrow-slidedown"><div><div class="biography-sect">Sangmi Pallickara (Colorado State University)<blockquote>Sangmi Lee Pallickara is an Associate Professor of Computer Science and a Cochran Family Professor at Colorado State University. Her research interests are in the area of Big Data for the sciences with an emphasis on issues related to predictive analytics, storage, retrievals, and metadata management.  She has published over 60 peer-reviewed articles. She serves on the editorial board of the Journal of Big Data. Her research has been funded through grants from the National Science Foundation, the Advanced Research Projects Agency-Energy (Department of Energy), the Department of Homeland Security, the Environmental Defense Fund, Google, Amazon and Hewlett Packard. She is a recipient of the CAREER award from the U.S. National Science Foundation and the IEEE TCSC Award for Excellence in Scalable Computing (Mid-Career Researcher).</blockquote></div></div></div></div></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_38_1569346001_27" onclick="$('#vhsjs_view_38_1569346001_27').hide();
                $('#vhsjs_hide_38_1569346001_27').show();
                $('#37_1569346001_27').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_38_1569346001_27" onclick="$('#37_1569346001_27').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_38_1569346001_27').hide();
                $('#vhsjs_view_38_1569346001_27').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="38_1569346001_27" id="37_1569346001_27" style="display: none"><div class="arrow-slidedown"><blockquote>We are living in the big data era. Scientists and businesses analyze large-scale datasets for insights leading to better decisions and strategic moves. With emergence of geo sensors and IoT, we are faced with voluminous geotagged data. Indeed, more than 80% of large datasets are estimated to be geospatial. In this talk, I discuss the unique characteristics and challenges in large-scale geospatial analytics while focusing on the distinctive patterns in data collection, access, and computation. I will also discuss our methods for data retrieval and analytics derived to cope with these challenges at scale. Finally, I will discuss a couple of scientific applications leveraging our methodology in geo-science domains including ecology and agriculture.<br><br>------<br><br>Sangmi Lee Pallickara is an Associate Professor of Computer Science and a Cochran Family Professor at Colorado State University. Her research interests are in the area of Big Data for the sciences with an emphasis on issues related to predictive analytics, storage, retrievals, and metadata management.  She has published over 60 peer-reviewed articles. She serves on the editorial board of the Journal of Big Data. Her research has been funded through grants from the National Science Foundation, the Advanced Research Projects Agency-Energy (Department of Energy), the Department of Homeland Security, the Environmental Defense Fund, Google, Amazon and Hewlett Packard. She is a recipient of the CAREER award from the U.S. National Science Foundation and the IEEE TCSC Award for Excellence in Scalable Computing (Mid-Career Researcher).</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 10:00am-10:30am"></a><div class="section-title">Wednesday 10:00am-10:30am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">10:00am-10:30am</span> <a href="calendar/sessions/sess105.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Top 3 Papers of Cluster 2019 (2/3)</div><div class="session-chair">Chair: Patrick McCormick (Los Alamos National Laboratory)<br /></div><div class="slot-entry"><a name="pap194"></a><div class="slot-title">Algorithm-Based Fault Tolerance for Parallel Stencil Computations</div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Aurlien Cavelan and Florina M. Ciorba (University of Basel, Swizterland)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_42_1569346001_28" onclick="$('#vhsjs_view_42_1569346001_28').hide();
                $('#vhsjs_hide_42_1569346001_28').show();
                $('#41_1569346001_28').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_42_1569346001_28" onclick="$('#41_1569346001_28').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_42_1569346001_28').hide();
                $('#vhsjs_view_42_1569346001_28').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="42_1569346001_28" id="41_1569346001_28" style="display: none"><div class="arrow-slidedown"><blockquote>The increase in HPC systems size and complexity, together with increasing on-chip transistor density, power limitations, and number of components, render modern HPC systems subject to soft errors. Silent data corruptions (SDCs) are typically caused by such soft errors in the form of bit-flips in the memory subsystem and hinder the correctness of scientific applications. This work addresses the problem of protecting a class of iterative computational kernels, called stencils, against SDCs when executing on parallel HPC systems. Existing SDC detection and correction methods are in general either inaccurate, inefficient, or targeting specific application classes that do not include stencils. This work proposes a novel algorithm-based fault tolerance (ABFT) method to protect scientific applications that contain arbitrary stencil computations against SDCs. The ABFT method can be applied both online and offline to accurately detect and correct SDCs in 2D and 3D parallel stencil computations. We present a formal model for the proposed method including theorems and proofs for the computation of the associated checksums as well as error detection and correction. We experimentally evaluate the use of the proposed ABFT method on a real 3D stencil-based application (HotSpot3D) via a fault-injection, detection, and correction campaign. Results show that the proposed ABFT method achieves less than 8% overhead compared to the performance of the unprotected stencil application. Moreover, it accurately detects and corrects SDCs. While the offline ABFT version corrects errors more accurately, it may incur a small additional overhead than its online counterpart.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 10:30am-11:00am"></a><div class="section-title">Wednesday 10:30am-11:00am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">10:30am-11:00am</span> <a href="calendar/sessions/sess133.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Wurlitzer<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 11:00am-12:30pm"></a><div class="section-title">Wednesday 11:00am-12:30pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">11:00am-12:30pm</span> <a href="calendar/sessions/sess109.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Message Passing</div><div class="session-chair">Chair: Scott Levy (Sandia National Laboratories)<br /></div><div class="slot-entry"><a name="pap221"></a><div class="slot-title">MPI Sessions: Evaluation of an Implementation in Open MPI <a href="calendar/submissions/sess109--pap221.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Nathan Hjelm (Google Inc.); Howard Pritchard and Samuel K. Gutirrez (Los Alamos National Laboratory); Daniel J. Holmes (EPCC, The University of Edinburgh); Ralph Castain (Intel); and Anthony Skjellum (University of Tennessee at Chattanooga)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_44_1569346001_29" onclick="$('#vhsjs_view_44_1569346001_29').hide();
                $('#vhsjs_hide_44_1569346001_29').show();
                $('#43_1569346001_29').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_44_1569346001_29" onclick="$('#43_1569346001_29').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_44_1569346001_29').hide();
                $('#vhsjs_view_44_1569346001_29').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="44_1569346001_29" id="43_1569346001_29" style="display: none"><div class="arrow-slidedown"><blockquote>The recently proposed MPI Sessions extensions to the MPI standard present a new paradigm for applications to use with MPI.  MPI Sessions has the potential to address several limitations of MPI's current specification: MPI cannot be initialized within an MPI process from different application components without a priori knowledge or coordination; MPI cannot be initialized more than once; and, MPI cannot be reinitialized after MPI finalization.  MPI Sessions also offers the possibility for more flexible ways for individual components of an application to express the capabilities they require from MPI at a finer granularity than is presently possible.<br><br>At this time, MPI Sessions has reached sufficient maturity for implementation and evaluation, which are the focuses of this paper.  This paper presents a prototype implementation of MPI Sessions, discusses certain of its performance characteristics, and describes its successful use in a large-scale production MPI application.  Overall, MPI Sessions is shown to be implementable, integrable with key infrastructure, and effective, but with certain overheads involving the initialization of MPI as well as communicator construction.  Small impacts on message-passing latency and throughput are noted.  Open MPI was used as the implementation vehicle, but results here are also relevant to other middleware stacks.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap168"></a><div class="slot-title">Give MPI Threading a Fair Chance: A Study of Multithreaded MPI Designs <a href="calendar/submissions/sess109--pap168.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Thananon Patinyasakdikul, David Eberius, and George Bosilca (University of Tennessee) and Nathan Hjelm (University of New Mexico)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_46_1569346001_29" onclick="$('#vhsjs_view_46_1569346001_29').hide();
                $('#vhsjs_hide_46_1569346001_29').show();
                $('#45_1569346001_29').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_46_1569346001_29" onclick="$('#45_1569346001_29').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_46_1569346001_29').hide();
                $('#vhsjs_view_46_1569346001_29').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="46_1569346001_29" id="45_1569346001_29" style="display: none"><div class="arrow-slidedown"><blockquote>The Message Passing Interface (MPI) has been one of the most prominent programming paradigms in highperformance computing (HPC) for the past decade. Lately, with changes in modern hardware leading to a drastic increase in the number of processor cores, developers of parallel applications are moving toward more integrated parallel programming paradigms, where MPI is used along with other, possibly node-level, programming paradigms, or MPI+X. MPI+threads emerged as one of the favorite choices in HPC community, according to a survey of the HPC community. However, threading support in MPI comes with many compromises to the overall performance delivered, and, therefore, its adoption is compromised.<br><br>This paper studies in depth the MPI multi-threaded implementation design in one of the leading MPI implementations, Open MPI, and expose some of the shortcomings of the current design. We propose, implement, and evaluate a new design of the internal handling of communication progress which allows for a significant boost in multi-threading performance, increasing the viability of MPI in the MPI+X programming paradigm.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap273"></a><div class="slot-title">Fast and Faithful Performance Prediction of MPI Applications: the HPL Case Study <a href="calendar/submissions/sess109--pap273.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Tom Cornebize (Universit Grenoble Alpes, French Institute for Research in Computer Science and Automation (INRIA)); Arnaud Legrand (National Center for Scientific Research (CNRS), French Institute for Research in Computer Science and Automation (INRIA)); and Franz Christian Heinrich (French Institute for Research in Computer Science and Automation (INRIA))</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_48_1569346001_29" onclick="$('#vhsjs_view_48_1569346001_29').hide();
                $('#vhsjs_hide_48_1569346001_29').show();
                $('#47_1569346001_29').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_48_1569346001_29" onclick="$('#47_1569346001_29').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_48_1569346001_29').hide();
                $('#vhsjs_view_48_1569346001_29').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="48_1569346001_29" id="47_1569346001_29" style="display: none"><div class="arrow-slidedown"><blockquote>Finely tuning MPI applications (number of processes, granularity, collective
operation algorithms, topology and process placement) is critical to obtain good performance on
supercomputers.  With a rising cost of modern supercomputers, running parallel
applications at scale solely to optimize their performance is extremely
expensive. Having inexpensive but faithful predictions of expected performance
could be a great help for researchers and system administrators.
The methodology we propose captures the complexity of adaptive applications by
emulating the MPI code while skipping insignificant parts. We demonstrate its
capability with High Performance Linpack (HPL), the benchmark used to rank
supercomputers in the TOP500 and which requires a careful tuning.  We explain
(1) how we both extended the SimGrid's SMPI simulator and slightly modified the
open-source version of HPL to allow a fast emulation on a single commodity
server at the scale of a supercomputer and (2) how to model the different
components (network, BLAS, ...) of the system.  We show that a careful
modeling of both spatial and temporal node variability allows us to obtain
predictions within a few percents of real experiments (see Figure 1).</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">11:00am-12:30pm</span> <a href="calendar/sessions/sess116.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Regal<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Data Centers and Clouds</div><div class="session-chair">Chair: Kevin Pedretti (Sandia National Laboratories)<br /></div><div class="slot-entry"><a name="pap252"></a><div class="slot-title">MBECN: Enabling ECN with Micro-burst Traffic in Multi-queue Data Center <a href="calendar/submissions/sess116--pap252.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Kexi Kang, Jinghui Zhang, Jiahui Jin, Dian Shen, and Junzhou Luo (Southeast University); Wenxin Li (Hong Kong University of Science and Technology); and Zhiang Wu (Nanjing University of Finance and Economics)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_50_1569346001_3" onclick="$('#vhsjs_view_50_1569346001_3').hide();
                $('#vhsjs_hide_50_1569346001_3').show();
                $('#49_1569346001_3').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_50_1569346001_3" onclick="$('#49_1569346001_3').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_50_1569346001_3').hide();
                $('#vhsjs_view_50_1569346001_3').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="50_1569346001_3" id="49_1569346001_3" style="display: none"><div class="arrow-slidedown"><blockquote>Modern multi-queue data centers often use the standard Explicit Congestion Notification (ECN) scheme to achieve high network performance. However, one substantial drawback of this approach is that micro-burst traffic can cause the instantaneous queue length to exceed the ECNs threshold, resulting in numerous mismarkings. After enduring too many mismarkings, senders may overreact, leading to severe throughput loss. As a solution to this dilemma, we propose our own adaptation the Micro-burst ECN (MBECN) scheme-to mitigate mismarking. MBECN finds a more appropriate threshold baseline for each queue to absorb micro-bursts, based on steady-state analysis and an ideal generalized processor sharing (GPS) model. By adopting a queue-occupation-based dynamically adjusting algorithm, MBECN effectively handles packet backlog without hurting latency. Through testbed experiments, we find that MBECN improves throughput by ~20% and reduces flow completion time (FCT) by ~40%. Using large scale simulations, we find that throughput can be improved by 1.5~2.4x with DCTCP and 1.26~1.35x with ECN*. We also measure network delay and find that latency only increases by 7.36%.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap269"></a><div class="slot-title">Large-Scale Analysis of the Docker Hub Dataset <a href="calendar/submissions/sess116--pap269.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Nannan Zhao (Virginia Tech); Vasily Tarasov (IBM ResearchAlmaden); Hadeel Albahar (Virginia Tech); Ali Anwar, Lukas Rupprecht, Dimitrios Skourtis, and Amit S. Warke (IBM ResearchAlmaden); Mohamed Mohamed (Apple); and Ali R. Butt (Virginia Tech)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_52_1569346001_3" onclick="$('#vhsjs_view_52_1569346001_3').hide();
                $('#vhsjs_hide_52_1569346001_3').show();
                $('#51_1569346001_3').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_52_1569346001_3" onclick="$('#51_1569346001_3').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_52_1569346001_3').hide();
                $('#vhsjs_view_52_1569346001_3').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="52_1569346001_3" id="51_1569346001_3" style="display: none"><div class="arrow-slidedown"><blockquote>Docker containers have become a prominent solution for supporting modern enterprise applications due to the highly desirable features of isolation, low overhead, and efficient packaging of the execution environment. Containers are created from images which are shared between users via a Docker registry. The amount of data Docker registries store is massive; for example, Docker Hub, a popular public registry, stores at least half a million public images. In this paper, we analyze over 167 TB of uncompressed Docker Hub images, characterize them using multiple metrics and evaluate the potential of file-level deduplication in Docker Hub. Our analysis helps to make conscious decisions when designing storage for containers in general and Docker registries in particular. For example, only 3% of the files in images are unique, which means file-level deduplication has a great potential to save storage space for the registry. Our findings can motivate and help improve the design of data reduction, caching, and pulling optimizations for registries.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap191"></a><div class="slot-title">DP_Greedy: A Two-Phase Caching Algorithm for Mobile Cloud Services <a href="calendar/submissions/sess116--pap191.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Dong Huang, Xiaopeng Fan, and Yang Wang (Shenzhen Institutes of Advanced Technology); Shuibing He (Zhejiang University); and Chengzhong Xu (University of Macau)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_54_1569346001_31" onclick="$('#vhsjs_view_54_1569346001_31').hide();
                $('#vhsjs_hide_54_1569346001_31').show();
                $('#53_1569346001_31').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_54_1569346001_31" onclick="$('#53_1569346001_31').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_54_1569346001_31').hide();
                $('#vhsjs_view_54_1569346001_31').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="54_1569346001_31" id="53_1569346001_31" style="display: none"><div class="arrow-slidedown"><blockquote>In this paper, we study the data caching problem in mobile cloud environment where multiple correlated data items could be packed and migrated to serve a predefined sequence of requests. By leveraging the spatial and temporal trajectory of requests, we propose a two-phase caching algorithm. We first investigate the correlation between data items to determine whether or not two data items could be packed to transfer, and then combine the algorithm proposed in \cite{wang2017data} and a greedy strategy to design a two-phase algorithm, named \emph{DP\_Greedy}, for effectively caching these shared data items to serve a predefined sequence of requests. Under homogeneous cost model, we prove the proposed algorithm is at most $2/\alpha$ times worse than the optimal one in terms of the total service cost, where $\alpha$ is the discount factor we defined, and also show that the algorithm can achieve this results within $O(mn^2)$ time and $O(mn)$ space complexity for $m$ caches to serve a $n$-length sequence. We evaluate our algorithm by effectively implementing it and comparing it with the non-packing case, the result show the proposed DP\_Greedy algorithm not only presents excellent performances but is also more in line with the actual situation.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 12:30pm-2:00pm"></a><div class="section-title">Wednesday 12:30pm-2:00pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">12:30pm-2:00pm</span> <a href="calendar/sessions/sess135.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Other<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Lunch (on your own)</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 2:00pm-3:30pm"></a><div class="section-title">Wednesday 2:00pm-3:30pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">2:00pm-3:30pm</span> <a href="calendar/sessions/sess123.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Cluster Communication</div><div class="session-chair">Chair: Felix Wolf (TU Darmstadt)<br /></div><div class="slot-entry"><a name="pap145"></a><div class="slot-title">X-RDMA: Effective RDMA Middleware in Large-scale Production Environments <a href="calendar/submissions/sess123--pap145.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Teng Ma (Tsinghua University, Alibaba); Tao Ma, Zhuo Song, Jingxuan Li, and Huaixin Chang (Alibaba); Kang Chen (Tsinghua University); Hai Jiang (Arkansas State University); and Yongwei Wu (Tsinghua University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_56_1569346001_32" onclick="$('#vhsjs_view_56_1569346001_32').hide();
                $('#vhsjs_hide_56_1569346001_32').show();
                $('#55_1569346001_32').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_56_1569346001_32" onclick="$('#55_1569346001_32').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_56_1569346001_32').hide();
                $('#vhsjs_view_56_1569346001_32').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="56_1569346001_32" id="55_1569346001_32" style="display: none"><div class="arrow-slidedown"><blockquote>X-RDMA is a communication middleware deployed and heavily used in Alibabas large-scale cluster hosting cloud storage and database systems. Unlike recent research projects which purely focus on squeezing out the raw hardware performance, it puts emphasis on robustness, scalability, and maintainability of large-scale production clusters. X-RDMA integrates necessary features, not available in current RDMA ecosystem, to release the developers from complex and imperfect details. X-RDMA simplifies the programming model, extends RDMA protocols for application awareness, and proposes mechanisms for resource management with thousands of connections per machine. It also reduces the work for administration and performance tuning with built-in tracing, tuning and monitoring tools.<br><br>X-RDMA has been deployed in several large-scale clusters with over 4000 servers in Alibaba cloud since 2016. It can save at least 70% development and maintenance time over RDMA, effectively improve performance and reduce network jitter especially when production servers are under pressure. It also helped locate over 30 issues in different layers of productions with over 5000 connections for each server on average.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap197"></a><div class="slot-title">Propagation and Decay of Injected One-Off Delays on Clusters: A Case Study <a href="calendar/submissions/sess123--pap197.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Ayesha Afzal, Georg Hager, and Gerhard Wellein (Friedrich-Alexander University Erlangen-Nrnberg)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_58_1569346001_32" onclick="$('#vhsjs_view_58_1569346001_32').hide();
                $('#vhsjs_hide_58_1569346001_32').show();
                $('#57_1569346001_32').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_58_1569346001_32" onclick="$('#57_1569346001_32').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_58_1569346001_32').hide();
                $('#vhsjs_view_58_1569346001_32').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="58_1569346001_32" id="57_1569346001_32" style="display: none"><div class="arrow-slidedown"><blockquote>Analytic, first-principles performance modeling of distributed-memory
applications is difficult due to a wide spectrum of random
disturbances caused by the application and the system.  These
disturbances (commonly called "noise") run contrary to the assumptions
about regularity that one usually employs when constructing simple
analytic models.  Despite numerous efforts to quantify, categorize,
and reduce such effects, a comprehensive quantitative understanding of
their performance impact is not available, especially for
long, one-off delays of execution periods
that have global consequences for the parallel application.
In this work, we
investigate various traces collected from synthetic
benchmarks that mimic real applications on simulated and real message-passing
systems in order to pinpoint the mechanisms behind delay propagation.
We analyze the dependence of the propagation speed of "idle
waves," i.e., propagating phases of inactivity,
emanating from injected delays
with respect to the execution and communication properties of the
application, study how such delays decay under increased noise levels,
and how they interact with each other.
We also show how fine-grained noise can make a system immune
against the adverse effects of propagating idle waves.
Our results contribute to a better understanding of the
collective phenomena that manifest themselves in distributed-memory
parallel applications.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap278"></a><div class="slot-title">An Empirical Study of Cryptographic Libraries for MPI Communications <a href="calendar/submissions/sess123--pap278.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Abu Naser, Mohsen Gavahi, Cong Wu, Viet Tung Hoang, Zhi Wang, and Xin Yuan (Florida State University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_60_1569346001_33" onclick="$('#vhsjs_view_60_1569346001_33').hide();
                $('#vhsjs_hide_60_1569346001_33').show();
                $('#59_1569346001_33').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_60_1569346001_33" onclick="$('#59_1569346001_33').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_60_1569346001_33').hide();
                $('#vhsjs_view_60_1569346001_33').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="60_1569346001_33" id="59_1569346001_33" style="display: none"><div class="arrow-slidedown"><blockquote>As High Performance Computing (HPC) applications with data security requirements are increasingly moving to
execute in the public cloud, there is a demand that the cloud
infrastructure for HPC should support privacy and integrity.
Incorporating privacy and integrity mechanisms in the communication
 infrastructure of todays public cloud is challenging
because recent advances in the networking infrastructure in
data centers have shifted the communication bottleneck from the
network links to the network end points and because encryption
is computationally intensive.<br><br>In this work, we consider incorporating encryption to support
privacy and integrity in the Message Passing Interface (MPI)
library, which is widely used in HPC applications. We empirically
study four contemporary cryptographic libraries, OpenSSL,
BoringSSL, Libsodium, and CryptoPP using micro-benchmarks
and NAS parallel benchmarks to evaluate their overheads for encrypting
 MPI messages on two different networking technologies,
10Gbps Ethernet and 40Gbps InfiniBand. The results indicate
that (1) the performance differs drastically across cryptographic
libraries, and (2) effectively supporting privacy and integrity
in MPI communications on high speed data center networks is
challengingeven with the most efficient cryptographic library,
encryption can still introduce very significant overheads in some
scenarios such as a single MPI communication operation on
InfiniBand, but (3) the overall overhead may not be prohibitive
for practical uses since there can be multiple concurrent communications.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">2:00pm-3:30pm</span> <a href="calendar/sessions/sess125.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Regal<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Efficient Storage</div><div class="session-chair">Chair: Kathryn Mohror (LLNL)<br /></div><div class="slot-entry"><a name="pap293"></a><div class="slot-title">RE-Store: Reliable and Efficient KV-Store with Erasure Coding and Replication <a href="calendar/submissions/sess125--pap293.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Yuzhe Li, Jiang Zhou, and Weiping Wang (Institute of Information Engineering, Chinese Academy of Sciences) and Yong Chen (Texas Tech University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_62_1569346001_34" onclick="$('#vhsjs_view_62_1569346001_34').hide();
                $('#vhsjs_hide_62_1569346001_34').show();
                $('#61_1569346001_34').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_62_1569346001_34" onclick="$('#61_1569346001_34').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_62_1569346001_34').hide();
                $('#vhsjs_view_62_1569346001_34').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="62_1569346001_34" id="61_1569346001_34" style="display: none"><div class="arrow-slidedown"><blockquote>In-memory key/value store (KV-store) is a key building block for numerous applications running on a cluster. With the increase of cluster scale, efficiency and availability have become two critical demanding features. Traditional replication provides redundancy but is inefficient due to its high storage cost. Erasure coding can provide data reliability with significantly low storage requirements but primarily used for long-term archival data due to the limitation of write performance. Recent studies attempt to combine these two techniques, e.g. using replication
for frequently-updated metadata and using erasure coding for large, read-only data. In this study, we propose RE-Store, an in-memory key/value system with a novel, hybrid scheme of replication and erasure coding to achieve both efficiency and reliability. RE-Store introduces replication into erasure coding by making one copy for each encoded data and replacing partial parity with replicas for storage-efficiency. When failures occur, it uses replicas to ensure data availability, avoiding the inefficiency of erasure coding during repair. A fast online recovery is achieved
for fault tolerance at different failure scenarios, with little performance degradation. We have implemented RE-Store on a real key/value system and conducted extensive evaluations
to validate its design and to study its performance, efficiency, and reliability. Experimental results show that RE-Store has a similar performance with erasure coding and replication under
normal operations, yet saves 18% to 34% memory compared to replication when tolerating 2 to 4 failures.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap173"></a><div class="slot-title">Compact Filter Structures for Fast Data Partitioning <a href="calendar/submissions/sess125--pap173.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Qing Zheng, Charles Cranor, Ankush Jain, Gregory Ganger, Garth Gibson, and George Amvrosiadis (Carnegie Mellon University) and Bradley Settlemyer and Gary Grider (Los Alamos National Lab)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_64_1569346001_34" onclick="$('#vhsjs_view_64_1569346001_34').hide();
                $('#vhsjs_hide_64_1569346001_34').show();
                $('#63_1569346001_34').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_64_1569346001_34" onclick="$('#63_1569346001_34').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_64_1569346001_34').hide();
                $('#vhsjs_view_64_1569346001_34').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="64_1569346001_34" id="63_1569346001_34" style="display: none"><div class="arrow-slidedown"><blockquote>We are approaching a point in time when it will be infeasible to catalog and query data after it has been generated. This trend has fueled research on in-situ data processing (i.e. operating on data as it is streamed to storage). One important example of this approach is in-situ data indexing. Prior work has shown the feasibility of indexing at scale as a two-step process. First, one partitions data by key across the CPU cores of a parallel job. Then each core indexes its subset as data is persisted. Online partitioning requires transferring data over the network so that it can be indexed and stored by the core responsible for the data. This approach is becoming increasingly costly as new computing platforms emphasize parallelism instead of individual core performance that is crucial for communication libraries and systems software in general. In addition to indexing, scalable online data partitioning is also useful in other contexts such as load balancing and efficient compression.<br><br>We present FilterKV, an efficient data management scheme for fast online data partitioning of key-value (KV) pairs. FilterKV reduces the total amount of data sent over the network and to storage. We achieve this by: (a) partitioning pointers to KV pairs instead of the KV pairs themselves and (b) using a compact format to represent and store KV pointers. Results from LANL show that FilterKV can reduce total write slowdown (including partitioning overhead) by up to 3x across 4096 CPU cores.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap140"></a><div class="slot-title">Building Reliable High-Performance Storage Systems: An Empirical and Analytical Study <a href="calendar/submissions/sess125--pap140.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Zhi Qiao (University of North Texas; USRC, LANL); Song Fu (University of North Texas); and Hsing-Bung Chen and Bradley Settlemyer (Los Alamos National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_66_1569346001_35" onclick="$('#vhsjs_view_66_1569346001_35').hide();
                $('#vhsjs_hide_66_1569346001_35').show();
                $('#65_1569346001_35').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_66_1569346001_35" onclick="$('#65_1569346001_35').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_66_1569346001_35').hide();
                $('#vhsjs_view_66_1569346001_35').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="66_1569346001_35" id="65_1569346001_35" style="display: none"><div class="arrow-slidedown"><blockquote>Due to the vast storage needs of high performance computing (HPC), the scale and complexity of storage systems in HPC data centers  continue growing. Disk failures have become the norm. With the ever-increasing disk capacity, RAID recovery based on disk rebuild becomes more and more expensive, which causes significant performance degradation and even unavailability of storage systems. Declustered redundant array of independent disks shuffle data and parity blocks among all drives in a RAID group, which aims to accelerate  RAID reconstruction and improve  performance. With the popularity of ZFS file system and software RAID used in  production systems, in this paper, we extensively evaluate and analyze declustered RAID with regard to the RAID I/O performance and recovery time on an high performance storage platform at Los Alamos National Laboratory. Our empirical study reveals that the speedup of declustered RAID over traditional RAID is sub-linear to the parallelism of recovery I/O. Furthermore, we formally model and analyze the reliability of declustered RAID using the mean-time-to-data-loss  and discover that the improved recovery performance leads to a higher storage reliability compared with the traditional RAID.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 3:30pm-4:00pm"></a><div class="section-title">Wednesday 3:30pm-4:00pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">3:30pm-4:00pm</span> <a href="calendar/sessions/sess134.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Wurlitzer<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 4:00pm-5:15pm"></a><div class="section-title">Wednesday 4:00pm-5:15pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">4:00pm-5:15pm</span> <a href="calendar/sessions/sess111.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Regal<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Compression</div><div class="session-chair">Chair: Christian Engelmann (Oak Ridge National Laboratory)<br /></div><div class="slot-entry"><a name="pap206"></a><div class="slot-title">Analyzing the Impact of Lossy Compressor Variability on Checkpointing Scientific Simulations <a href="calendar/submissions/sess111--pap206.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Pavlo D. Triantafyllides, Tasmia Reza, and Jon C. Calhoun (Clemson University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_68_1569346001_36" onclick="$('#vhsjs_view_68_1569346001_36').hide();
                $('#vhsjs_hide_68_1569346001_36').show();
                $('#67_1569346001_36').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_68_1569346001_36" onclick="$('#67_1569346001_36').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_68_1569346001_36').hide();
                $('#vhsjs_view_68_1569346001_36').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="68_1569346001_36" id="67_1569346001_36" style="display: none"><div class="arrow-slidedown"><blockquote>Lossy compression algorithms are effective tools to reduce the size of high-performance computing data sets. As established lossy compressors such as SZ and ZFP evolve, they seek to improve the compression/decompression bandwidth and the compression ratio. Algorithm improvements may alter the spatial distribution of errors in the compressed data even when using the same error bound and error bound type. If HPC applications are to compute on lossy compressed data, application users require an understanding of how the performance and spatial distribution of error changes. We explore how spatial distributions of error, compression/decompression bandwidth, and compression ratio change for HPC data sets from the applications PlasComCM and Nek5000 between various versions of SZ and ZFP. In addition, we explore how the spatial distribution of error impacts application correctness when restarting from lossy compressed checkpoints. We verify that known approaches to selecting error tolerances for lossy compressed checkpointing are robust to compressor selection and in the face of changes in the distribution of error.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap160"></a><div class="slot-title">Improving Performance of Data Dumping with Lossy Compression for Scientific Simulation <a href="calendar/submissions/sess111--pap160.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Xin Liang (UC, Riverside); Sheng Di (Argonne National Laboratory); Dingwen Tao (the University of Alabama); Sihuan Li (UC, Riverside); Bogdan Nicolae (Argonne National Laboratory); Zizhong Chen (UC, Riverside); and Franck Cappello (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_70_1569346001_36" onclick="$('#vhsjs_view_70_1569346001_36').hide();
                $('#vhsjs_hide_70_1569346001_36').show();
                $('#69_1569346001_36').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_70_1569346001_36" onclick="$('#69_1569346001_36').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_70_1569346001_36').hide();
                $('#vhsjs_view_70_1569346001_36').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="70_1569346001_36" id="69_1569346001_36" style="display: none"><div class="arrow-slidedown"><blockquote>Because of the ever-increasing  data being produced by today's high performance computing (HPC) scientific simulations, I/O performance is becoming a significant bottleneck for their executions. An efficient error-controlled lossy compressor is a promising solution to significantly reduce data writing time for scientific simulations running on supercomputers. In this paper, we explore how to optimize the data dumping performance for scientific simulation by leveraging error-bounded lossy compression techniques. The contributions of the paper are threefold. (1) We propose a novel I/O performance profiling model that can effectively represent the I/O performance with different execution scales and data sizes, and optimize the  estimation accuracy of data dumping performance using least square method. (2) We develop an adaptive lossy compression framework that can select the bestfit compressor (between two leading lossy compressors SZ and ZFP) with optimized parameter settings with respect to overall data dumping performance. (3) We evaluate our adaptive lossy compression framework with up to 32k cores on a supercomputer facilitated with fast I/O systems and using real-world scientific simulation datasets. Experiments show that our solution can mostly always lead the data dumping performance to the optimal level with very accurate selection of the bestfit lossy compressor and settings. The data dumping performance can be improved by up to 27% at different scales.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap259"></a><div class="slot-title">Efficient Distributed Graph Analytics using Triply Compressed Sparse Format <a href="calendar/submissions/sess111--pap259.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Mohammad Hasanzadeh Mofrad and Rami Melhem (University of Pittsburgh) and Yousuf Ahmad and Mohammad Hammoud (Carnegie Mellon University in Qatar)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_72_1569346001_37" onclick="$('#vhsjs_view_72_1569346001_37').hide();
                $('#vhsjs_hide_72_1569346001_37').show();
                $('#71_1569346001_37').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_72_1569346001_37" onclick="$('#71_1569346001_37').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_72_1569346001_37').hide();
                $('#vhsjs_view_72_1569346001_37').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="72_1569346001_37" id="71_1569346001_37" style="display: none"><div class="arrow-slidedown"><blockquote>This paper presents Triply Compressed Sparse Column (TCSC), a novel compression technique designed specifically for matrix-vector operations where the matrix as well as the input and output vectors are sparse. We refer to these operations as SpMSpV$^2$. TCSC compresses the nonzero columns and rows of a highly sparse matrix representing a large real-world graph. During this compression, it encodes the sparsity patterns of the input and output vectors within the compressed representation of the sparse matrix itself. Consequently, it aligns the compressed indices of the input and output vectors with those of the compressed matrix columns and rows, thus eliminating the need for extra indirections when SpMSpV$^2$ operations access the vectors. This results in fewer cache misses, greater space efficiency and faster execution times. We evaluate TCSC's performance and show that it is more space and time efficient compared to CSC and DCSC, with up to $11 \times$ speedup. We integrate TCSC into GraphTap, our suggested linear algebra-based distributed graph analytics system. We compare GraphTap against GraphPad and LA3, two state-of-the-art linear algebra-based distributed graph analytics systems, using different dataset scales and numbers of processes. GraphTap is up to $7\times$ faster than these systems due to TCSC and the resulting communication efficiency.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 4:00pm-5:30pm"></a><div class="section-title">Wednesday 4:00pm-5:30pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">4:00pm-5:28pm</span> <a href="calendar/sessions/sess119.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Resource Allocation</div><div class="session-chair">Chair: Eishi Arima (The University of Tokyo)<br /></div><div class="slot-entry"><a name="pap146"></a><div class="slot-title">SMQoS: Improving Utilization and Power Efficiency with QoS Awareness on GPUs <a href="calendar/submissions/sess119--pap146.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Qingxiao Sun, Yi Liu, Hailong Yang, Zhongzhi Luan, and Depei Qian (Beihang University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_74_1569346001_38" onclick="$('#vhsjs_view_74_1569346001_38').hide();
                $('#vhsjs_hide_74_1569346001_38').show();
                $('#73_1569346001_38').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_74_1569346001_38" onclick="$('#73_1569346001_38').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_74_1569346001_38').hide();
                $('#vhsjs_view_74_1569346001_38').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="74_1569346001_38" id="73_1569346001_38" style="display: none"><div class="arrow-slidedown"><blockquote>Meeting the Quality of Service (QoS) requirement under task consolidation on the GPU is extremely challenging. Previous work mostly relies on static task or resource scheduling and cannot handle the QoS violation during runtime. In addition, the existing work fails to exploit the computing characteristics of batch tasks, and thus wastes the opportunities to reduce power consumption while improving GPU utilization. To address the above problems, we propose a new runtime mechanism SMQoS that can dynamically adjust the resource allocation during runtime to satisfy the QoS of latency-sensitive tasks and determine the optimal resource allocation for batch tasks to improve GPU utilization and power efficiency. The experimental results show that with SMQoS, 2.27% and 7.58% more task co-runnings reach the 95% QoS target than Spart and Rollover respectively. In addition, SMQoS achieves 23.9% and 32.3% higher throughput, and reduces the power consumption by 25.7% and 10.1%, compared to Spart and Rollover respectively.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap163"></a><div class="slot-title">Mitigating Inter-Job Interference via Process-Level Quality-of-Service <a href="calendar/submissions/sess119--pap163.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Lee Savoie and David Lowenthal (University of Arizona), Bronis de Supinski and Kathryn Mohror (Lawrence Livermore National Laboratory), and Nikhil Jain (Nvidia)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_76_1569346001_38" onclick="$('#vhsjs_view_76_1569346001_38').hide();
                $('#vhsjs_hide_76_1569346001_38').show();
                $('#75_1569346001_38').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_76_1569346001_38" onclick="$('#75_1569346001_38').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_76_1569346001_38').hide();
                $('#vhsjs_view_76_1569346001_38').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="76_1569346001_38" id="75_1569346001_38" style="display: none"><div class="arrow-slidedown"><blockquote>Jobs on most high-performance computing (HPC) systems share the network with other concurrently executing jobs. This sharing creates contention that can severely degrade performance. We investigate the use of Quality of Service (QoS) mechanisms to reduce the negative impacts of network contention. Our results show that careful use of QoS reduces the impact of contention for specific jobs, resulting in up to a 27% performance improvement. In some cases the impact of contention is completely eliminated. These improvements are achieved with limited negative impact to other jobs; any job that experiences performance loss typically degrades less than 5%, often much less. Our approach can help ensure that HPC machines maintain high throughput as per-node compute power continues to increase faster than network bandwidth.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap119"></a><div class="slot-title">Kube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters <a href="calendar/submissions/sess119--pap119.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Prashanth Thinakaran and Jashwant Raj Gunasekaran (Penn State), Bikash Sharma (Facebook), and Mahmut Kandemir and Chita Das (Penn State)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_78_1569346001_39" onclick="$('#vhsjs_view_78_1569346001_39').hide();
                $('#vhsjs_hide_78_1569346001_39').show();
                $('#77_1569346001_39').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_78_1569346001_39" onclick="$('#77_1569346001_39').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_78_1569346001_39').hide();
                $('#vhsjs_view_78_1569346001_39').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="78_1569346001_39" id="77_1569346001_39" style="display: none"><div class="arrow-slidedown"><blockquote>Compute heterogeneity is increasingly gaining prominence in modern datacenters due to the addition of accelerators like GPUs and FPGAs. We observe that datacenter schedulers are agnostic of these emerging accelerators, especially their resource utilization footprints, and thus, not well equipped to dynamically provision them based on the application needs. We observe that the state-of-the-art datacenter schedulers fail to provide fine-grained resource guarantees for latency-sensitive tasks that are GPU-bound. Specifically for GPUs, this results in resource fragmentation and interference leading to poor utilization of allocated resources. Furthermore, GPUs exhibit highly linear energy efficiency with respect to utilization and hence proactive management of these resources is essential to keep the operational costs low while ensuring the end-to-end Quality of Service (QoS).<br><br>We build Knots, a GPU-aware resource orchestration layer and integrate it with the Kubernetes container orchestrator to build Kube-Knots. Kube-Knots can dynamically harvest spare compute cycles through dynamic container orchestration enabling co-location of latency-critical and batch workloads together while improving the overall resource utilization. We design and evaluate two GPU-based schedulers for datacenter-scale workloads on a ten node GPU cluster. Our proposed Correlation Based Prediction (CBP) and Peak Prediction (PP) schemes together improve both average and 99th percentile cluster-wide GPU utilization by up to 80%. This leads to 33% cluster-wide energy savings on an average for three different workloads compared to state-of-the-art GPU-agnostic schedulers. Further, the PP scheduler guarantees the end-to-end QoS for latency-critical GPU-bound queries by reducing QoS violations by up to 53% when compared to GPU-agnostic policies.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap123"></a><div class="slot-title">Scheduling Independent Stochastic Tasks on Heterogeneous Cloud Platforms <a href="calendar/submissions/sess119--pap123.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Yiqin Gao (ENS Lyon); Louis-Claude Canon (Univ. Franche Comt); Yves Robert (ENS Lyon, Univ. Tenn. Knoxville); and Frdric Vivien (Inria)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_80_1569346001_39" onclick="$('#vhsjs_view_80_1569346001_39').hide();
                $('#vhsjs_hide_80_1569346001_39').show();
                $('#79_1569346001_39').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_80_1569346001_39" onclick="$('#79_1569346001_39').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_80_1569346001_39').hide();
                $('#vhsjs_view_80_1569346001_39').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="80_1569346001_39" id="79_1569346001_39" style="display: none"><div class="arrow-slidedown"><blockquote>This work introduces scheduling strategies to  maximize the
expected number of independent tasks that can be executed on a cloud platform with budget and deadline constraints. The cloud platform is composed of several types of virtual machines (VMs), where each type has a unit execution cost that depends upon its characteristics. 
The amount of budget spent during the execution of a task on a given VM  is the product of its execution length by the unit execution cost of that VM. The execution lengths of  tasks 
follow a variety of standard probability distributions (exponential, uniform, half-normal, lognormal, gamma, inverse-gamma and Weibull) whose mean and 
standard deviation both depend upon the VM type. Finally, there is a global available budget
and a deadline constraint,
and the goal is to successfully execute as many tasks as possible before the deadline is reached or the budget is exhausted (whichever comes first).
On each VM,
the scheduler can decide at any instant to interrupt the execution of a (long) running task 
and to launch a new one,
but the budget already spent for the interrupted task is lost. 
The main
questions are which\VMs to enroll, and whether and when to
interrupt tasks that have been executing for some time.
We assess the complexity of the problem by showing its NP-completeness and providing a 2-approximation for the asymptotic case where budget and deadline both tend to infinity.
Then we introduce several heuristics and compare their performance by  running 
an extensive set of simulations.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Wednesday 6:30pm-10:00pm"></a><div class="section-title">Wednesday 6:30pm-10:00pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Wednesday</span> <span class="session-time">6:30pm-10:00pm</span> <a href="calendar/sessions/sess129.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Roxy<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Conference Banquet</div><div class="slot-entry"><a name="pec104"></a><div class="slot-title">Conference Dinner</div><div class="slot-authors"></div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_82_1569346001_4" onclick="$('#vhsjs_view_82_1569346001_4').hide();
                $('#vhsjs_hide_82_1569346001_4').show();
                $('#81_1569346001_4').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_82_1569346001_4" onclick="$('#81_1569346001_4').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_82_1569346001_4').hide();
                $('#vhsjs_view_82_1569346001_4').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="82_1569346001_4" id="81_1569346001_4" style="display: none"><div class="arrow-slidedown"><blockquote>Dinner for attendees at the conference hotel (Sheraton Albuquerque Uptown).</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Thursday 7:00am-8:45am"></a><div class="section-title">Thursday 7:00am-8:45am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">7:00am-8:45am</span> <a href="calendar/sessions/sess140.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Foyer<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Registration and Badge Pickup</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Thursday 8:45am-9:00am"></a><div class="section-title">Thursday 8:45am-9:00am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">8:45am-9:00am</span> <a href="calendar/sessions/sess108.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Plenary</span><br /><div class="session-title">Cluster 2020 Presentation</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Thursday 9:00am-10:00am"></a><div class="section-title">Thursday 9:00am-10:00am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">9:00am-10:00am</span> <a href="calendar/sessions/sess103.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Keynote</span><br /><div class="session-title">Keynote 3</div><div class="session-chair">Chair: Patrick G. Bridges (University of New Mexico)<br /></div><div class="slot-entry"><a name="pec102"></a><div class="slot-title">Irene Qualters Keynote</div><div class="slot-authors">Irene Qualters (Los Alamos National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Thursday 10:00am-10:30am"></a><div class="section-title">Thursday 10:00am-10:30am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">10:00am-10:30am</span> <a href="calendar/sessions/sess106.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Top 3 Papers of Cluster 2019 (3/3)</div><div class="session-chair">Chair: Patrick G. Bridges (University of New Mexico)<br /></div><div class="slot-entry"><a name="pap218"></a><div class="slot-title">STASH : Fast Hierarchical Aggregation Queries for Effective Visual Spatiotemporal Explorations</div><div><span class="BP award">Best Paper</span></div><div class="slot-authors">Saptashwa Mitra, Paahuni Khandelwal, Shrideep Pallickara, and Sangmi Lee Pallickara (Colorado State University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_84_1569346001_42" onclick="$('#vhsjs_view_84_1569346001_42').hide();
                $('#vhsjs_hide_84_1569346001_42').show();
                $('#83_1569346001_42').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_84_1569346001_42" onclick="$('#83_1569346001_42').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_84_1569346001_42').hide();
                $('#vhsjs_view_84_1569346001_42').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="84_1569346001_42" id="83_1569346001_42" style="display: none"><div class="arrow-slidedown"><blockquote>The proliferation of spatiotemporal datasets has lead to a demand for scalable real-time analytics over these datasets to help scientists make inferences and inform decision-making. However, the data is voluminous and combined with incessant user queries adversely impacts the latency of the visualization queries over such datasets stored over large clusters.<br><br>In this paper, we introduce STASH, a distributed in-memory cache for hierarchical aggregation query evaluation.  STASH is a middleware which can be loaded on top of a distributed spatiotemporal file system.  Users perform queries from a front-end lightweight visualization interface and the evaluations occur over the back-end storage system housing the raw data over which summarization and subsequent visualization are performed. STASH facilitates fast exploratory analytics by caching relevant past query results based on their frequency and freshness to assist similar, future queries and avoid expensive disk I/O and network usage, thus reducing their latency. Additionally, STASH also handles any hotspot that might result from a spike in user requests due to the spatial and temporal locality of their access patterns.<br><br>Our empirical benchmarks show that a STASH-enabled system reduces query latency of a basic system by over 5 folds and brings it down to interactive speed even for large country-sized spatial queries. We have contrasted STASH with existing cache-enabled analytics engines, such as ElasticSearch, and it shows that our STASH-enabled system reduces the aggregation query latency up to ~71%. STASH also alleviates skewed workloads through its dynamic replication scheme and improves throughput by ~40% in hotspot scenarios.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Thursday 10:30am-11:00am"></a><div class="section-title">Thursday 10:30am-11:00am</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">10:30am-11:00am</span> <a href="calendar/sessions/sess137.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Wurlitzer<br /></span><span class="session-event-type">Other</span><br /><div class="session-title">Break</div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Thursday 11:00am-12:30pm"></a><div class="section-title">Thursday 11:00am-12:30pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">11:00am-12:30pm</span> <a href="calendar/sessions/sess115.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Tools and Optimization</div><div class="session-chair">Chair: Bronis R. de Supinski (Lawrence Livermore National Laboratory)<br /></div><div class="slot-entry"><a name="pap164"></a><div class="slot-title">DiffTrace: Efficient Whole-Program Trace Analysis and Diffing for Debugging <a href="calendar/submissions/sess115--pap164.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Saeed Taheri and Ian Briggs (University of Utah), Martin Burtscher (Texas State University), and Ganesh Gopalakrishnan (University of Utah)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_86_1569346001_43" onclick="$('#vhsjs_view_86_1569346001_43').hide();
                $('#vhsjs_hide_86_1569346001_43').show();
                $('#85_1569346001_43').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_86_1569346001_43" onclick="$('#85_1569346001_43').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_86_1569346001_43').hide();
                $('#vhsjs_view_86_1569346001_43').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="86_1569346001_43" id="85_1569346001_43" style="display: none"><div class="arrow-slidedown"><blockquote>We present a tool called DiffTrace that approaches
debugging via whole program tracing and diffing of typical and
erroneous traces. After collecting these traces, a user-configurable
front-end filters out irrelevant function calls and then summarizes
loops in the retained function calls based on state-of-the-art
loop extraction algorithms. Information about these loops is
inserted into concept lattices, which we use to compute salient
dissimilarities to narrow down bugs. DiffTrace is a clean start
that addresses debugging features missing in existing approaches.
Our experiments on an MPI/OpenMP program called ILCS and
initial measurements on LULESH, a DOE miniapp, demonstrate
the advantages of the proposed debugging approach.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap268"></a><div class="slot-title">FSMonitor: Scalable File System Monitoring for Arbitrary Storage Systems <a href="calendar/submissions/sess115--pap268.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Arnab K. Paul (Virginia Tech); Ryan Chard (Argonne National Laboratory); Kyle Chard and Steven Tuecke (University of Chicago); Ali R. Butt (Virginia Tech); and Ian Foster (Argonne National Laboratory, University of Chicago)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_88_1569346001_43" onclick="$('#vhsjs_view_88_1569346001_43').hide();
                $('#vhsjs_hide_88_1569346001_43').show();
                $('#87_1569346001_43').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_88_1569346001_43" onclick="$('#87_1569346001_43').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_88_1569346001_43').hide();
                $('#vhsjs_view_88_1569346001_43').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="88_1569346001_43" id="87_1569346001_43" style="display: none"><div class="arrow-slidedown"><blockquote>Data automation, monitoring, and management tools are reliant on being able to detect, report, and respond to file system events. Various data event reporting tools exist for specific operating systems and storage devices, such as inotify for Linux, kqueue for BSD, and FSEvents for macOS. However, these tools are not designed to monitor distributed file systems. Indeed, many cannot scale to monitor many thousands of directories, or simply cannot be applied to distributed file systems. Moreover, each tool implements a custom API and event representation, making the development of generalized and portable event-based applications challenging. As file systems grow in size and become increasingly diverse, there is a need for scalable monitoring solutions that can be applied to a wide range of both distributed and local systems. We present here a generic and scalable file system monitor and event reporting tool, FSMonitor, that provides a file-system-independent event representation and event capture interface. FSMonitor uses a modular Data Storage Interface (DSI) architecture to enable the selection and application of appropriate event monitoring tools to detect and report events from a target file system, and implements efficient and fault-tolerant mechanisms that can detect and report events even on large file systems. We describe and evaluate DSIs for common UNIX, macOS, and Windows storage systems, and for the Lustre distributed file system. Our experiments on a 897 TB Lustre file system show that FSMonitor can capture and process almost 38000 events per second.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap258"></a><div class="slot-title">On the Benefits of Anticipating Load Imbalance for Performance Optimization of Parallel Applications <a href="calendar/submissions/sess115--pap258.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Anthony Boulmier (University of Geneva), Franck Raynaud (Unversity of Geneva), Nabil Abdennadher (HES-SO), and Bastien Chopard (Unversity of Geneva)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_90_1569346001_44" onclick="$('#vhsjs_view_90_1569346001_44').hide();
                $('#vhsjs_hide_90_1569346001_44').show();
                $('#89_1569346001_44').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_90_1569346001_44" onclick="$('#89_1569346001_44').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_90_1569346001_44').hide();
                $('#vhsjs_view_90_1569346001_44').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="90_1569346001_44" id="89_1569346001_44" style="display: none"><div class="arrow-slidedown"><blockquote>In parallel iterative applications, computational efficiency is essential for addressing large problems. Load imbalance is one of the major performance degradation factors of parallel applications. Therefore, distributing, cleverly, and as evenly as possible, the workload among processing elements (PE) maximizes application performance. So far, the standard load balancing method consists in distributing the workload evenly between PEs and, when load imbalance appears, redistributing the extra load from overloaded PEs to underloaded PEs. However, this does not anticipate the load imbalance growth that may continue during the next iterations. In this paper, we present a first step toward a novel philosophy of load balancing that unloads the PEs that will be overloaded in the near future to let the application rebalance itself via its own dynamics. 
Herein, we present a formal definition of our new approach using a simple mathematical model and discuss its advantages compared to the standard load balancing method. In addition to the theoretical study, we apply our method to an application that reproduces the computation of a fluid model with non-uniform erosion. The performance validates the benefit of anticipating load imbalance. We observed up to 16% performance improvement compared to the standard load balancing method.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">11:00am-12:30pm</span> <a href="calendar/sessions/sess124.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Regal<br /></span><span class="session-event-type">Paper</span><br /><div class="session-title">Applications</div><div class="session-chair">Chair: Ryan Grant (Sandia National Labs, University of New Mexico)<br /></div><div class="slot-entry"><a name="pap222"></a><div class="slot-title">Multi-physics simulations of particle tracking in arterial geometries with a scalable moving window algorithm <a href="calendar/submissions/sess124--pap222.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Gregory J. Herschlag (Duke University), John Gounley (Oak Ridge National Laboratory), Sayan Roychowdhury (Duke University), Erik W. Draeger (Lawrence Livermore National Laboratory), and Amanda Randles (Duke University)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_92_1569346001_44" onclick="$('#vhsjs_view_92_1569346001_44').hide();
                $('#vhsjs_hide_92_1569346001_44').show();
                $('#91_1569346001_44').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_92_1569346001_44" onclick="$('#91_1569346001_44').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_92_1569346001_44').hide();
                $('#vhsjs_view_92_1569346001_44').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="92_1569346001_44" id="91_1569346001_44" style="display: none"><div class="arrow-slidedown"><blockquote>In arterial systems, cancer cell trajectories determine metastatic cancer locations; similarly, particle trajectories determine drug delivery distribution. Predicting trajectories is challenging, as the dynamics are affected by local interactions with red blood cells, complex hemodynamic flow structure, and downstream factors such as stenoses or blockages. Direct simulation is not possible, as even a single simulation of a large arterial domain with explicit red blood cells is currently intractable on even the largest supercomputers. To overcome this limitation, we present a multi-physics adaptive window algorithm, in which individual red blood cells are explicitly modeled in a small region of interest moving through a coupled arterial fluid domain. We describe the coupling between the window and fluid domains, including automatic insertion and deletion of explicit cells and dynamic tracking of cells of interest by the window. We show that this algorithm scales efficiently on heterogeneous architectures and enables us to perform large, highly-resolved particle-tracking simulations that would otherwise be intractable.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap127"></a><div class="slot-title">Fast and Scalable Implementations of Influence Maximization Algorithms <a href="calendar/submissions/sess124--pap127.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Marco Minutoli (Pacific Northwest National Laboratory, Washington State University); Mahantesh Halappanavar (Pacific Northwest National Laboratory); Ananth Kalyanaraman (Washington State University); and Arun Sathanur, Ryan Mcclure, and Jason McDermott (Pacific Northwest National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_94_1569346001_45" onclick="$('#vhsjs_view_94_1569346001_45').hide();
                $('#vhsjs_hide_94_1569346001_45').show();
                $('#93_1569346001_45').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_94_1569346001_45" onclick="$('#93_1569346001_45').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_94_1569346001_45').hide();
                $('#vhsjs_view_94_1569346001_45').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="94_1569346001_45" id="93_1569346001_45" style="display: none"><div class="arrow-slidedown"><blockquote>The Influence Maximization problem has been extensively studied in the past
  decade because of its practical applications in finding the key influencers in
  social networks.  Due to the hardness of the underlying problem, existing
  algorithms have tried to trade off practical efficiency with approximation
  guarantees.  However, approximate solutions take several hours of compute time
  on modest sized real world inputs. On the other hand, there is a lack of
  effective parallel and distributed algorithms to solve this problem. In this
  paper, we present efficient parallel algorithms for multithreaded and
  distributed systems to solve the influence maximization with approximation
  guarantee. Our algorithms extend state-of-the-art sequential approach based on
  computing reverse reachability sets.  We present a detailed experimental
  evaluation, and analyze their performance and their sensitivity to input
  parameters, using real world inputs.  Our experimental results demonstrate
  significant speedup on parallel architectures. We further show a speedup of up
  to 586X relative to the state-of-the-art sequential baseline using
  1024 nodes of a supercomputer at far greater accuracy and twice the seed set
  size.  To the best of our knowledge, this is the first effort in parallelizing
  the influence maximization operation at scale.</blockquote></div></div></div></div><div class="slot-urls"></div></div><div class="slot-entry"><a name="pap147"></a><div class="slot-title">Scalable, High-Order Continuity Across Block Boundaries of   Functional Approximations Computed in Parallel <a href="calendar/submissions/sess124--pap147.ics" target="_blank" title="Import this presentation into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> </div><div class="slot-authors">Iulian Grindeanu, Tom Peterka, Vijay Mahadevan, and Youssef S. Nashed (Argonne National Laboratory)</div><div class="auth-pics-section"></div><div class="slot-abstract"><div><a class="clickable no-decoration" id="vhsjs_view_96_1569346001_45" onclick="$('#vhsjs_view_96_1569346001_45').hide();
                $('#vhsjs_hide_96_1569346001_45').show();
                $('#95_1569346001_45').slideDown(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                    
                });"><i class="fa fa-caret-right"></i> <span class="hover_link">Abstract</span></a><a class="clickable no-decoration" id="vhsjs_hide_96_1569346001_45" onclick="$('#95_1569346001_45').hide(function() {
                    if (typeof Masonry === 'function') {
                        $('.use_masonry').masonry();
                    };
                });
                $('#vhsjs_hide_96_1569346001_45').hide();
                $('#vhsjs_view_96_1569346001_45').show();" style="display: none"><i class="fa fa-caret-down"></i> <span class="hover_link">Abstract</span></a><div data-display-control="96_1569346001_45" id="95_1569346001_45" style="display: none"><div class="arrow-slidedown"><blockquote>We investigate the representation of discrete scientific data with a $C^k$-continuous functional
 model, where $C^k$ denotes k-th-order continuity, in a distributed-memory parallel setting.
 The MFA  Multivariate Functional Approximation  model is a piecewise-continuous functional
approximation based on multivariate high-dimensional B-splines. When computing
 an MFA approximation in parallel over multiple blocks in a spatial domain
decomposition, the interior of each block will be $C^k$-continuous,
k being the B-spline polynomial degree, but discontinuities exist across
neighboring block boundaries. We present an efficient and scalable solution
to ensure $C^k$ continuity across blocks, by blending neighboring approximations.
We show that after decomposing the domain in structured, overlapping
blocks and approximating blocks independently to high degrees of accuracy,
 we can extend the local solution to the global domain by using compact,
multidimensional smooth-step functions. We prove that this approach,
which can be viewed as an extended partition of unity approximation method,
is highly scalable on modern architectures.</blockquote></div></div></div></div><div class="slot-urls"></div></div></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div><div><div class="centered"><a name="Thursday 12:30pm-12:45pm"></a><div class="section-title">Thursday 12:30pm-12:45pm</div></div><div class="section-entry"><div class="session-entry"><span class="session-date">Thursday</span> <span class="session-time">12:30pm-12:45pm</span> <a href="calendar/sessions/sess136.ics" target="_blank" title="Import this session into your desktop calendar program."><img height="15" src="includes/images/ical_icon2.gif" style="border: none;" width="15" /></a> <br /><span class="room-name">Ambassador/Registry<br /></span><span class="session-event-type">Plenary</span><br /><div class="session-title">Closing Remarks</div><div class="session-chair">Chair: Ron Brightwell (Sandia National Laboratories); Patrick G. Bridges (University of New Mexico)<br /></div><span class="slot-entry"></span></div></div><div class="centered"><div class="top-link"><a href="#top">Return to Top</a></div></div><hr /></div></td></tr></table></div></div><div class="created-date righted">Created 2019-9-24 12:26</div></body></html>
